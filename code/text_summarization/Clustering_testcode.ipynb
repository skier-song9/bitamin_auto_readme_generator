{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048f32c2-a1d3-48e8-bb32-9c60e2d61fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import glob,os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bce51905-8fde-42f0-8c3d-bdcc2eae970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/openai_api_key.json\", 'r') as f:\n",
    "    api_key = json.load(f)\n",
    "api_key = api_key['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0952a7-829c-49fd-84e3-86e0fbc2a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "feca9637-9360-457f-8135-75067a8cb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT():\n",
    "    __classname__ = \"OpenAI\"\n",
    "    api_key = ''\n",
    "    client = None    \n",
    "    def __init__(self, api_filepath):\n",
    "        with open(api_filepath,'r') as f:\n",
    "            ak = json.load(f)\n",
    "        self.api_key = ak['OPENAI_API_KEY']\n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "        \n",
    "    def get_chat_completion(self, msg, model='gpt-4o-mini', **kargs):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model = model,\n",
    "            messages = msg\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def get_embedding(self, sentence, model=\"text-embedding-3-small\"):\n",
    "       '''\n",
    "       - pricing : text-embedding-3-small = $0.02/1M tokens\n",
    "           텍스트가 많은 pdf는 대략 5,000 tokens -> pdf 200개에 0.02 달러(25~30원).\n",
    "       text : 한 문장\n",
    "       return : 한 문장에 대한 embedding (output dimension = 1536)\n",
    "       '''\n",
    "       return self.client.embeddings.create(input = sentence, model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a20672cb-fafa-4e0a-8936-47a472ec7a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT(api_filepath=\"../assets/openai_api_key.json\")\n",
    "print(str(gpt.__classname__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800b0cb1-9a3f-4db0-8c06-a9fff3ec6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_by_pages(filepath):\n",
    "    ### 문서를 페이지별로 구분하여 list에 저장\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    text = []\n",
    "    page_text = []\n",
    "    page_pattern = re.compile('<p.\\d*>')\n",
    "    for d in data:\n",
    "        # d = d.replace('\\n', ' <lf> ') # lf = line feed\n",
    "        d = d.replace('\\n', ' \\n') \n",
    "        if page_pattern.match(d):\n",
    "            if len(page_text)>0:\n",
    "                text.append(' '.join(page_text))\n",
    "            page_text = [] # 페이지 텍스트 초기화\n",
    "        page_text.append(d)\n",
    "    text.append(''.join(page_text)) # 마지막에는 수동으로 추가\n",
    "    return text\n",
    "\n",
    "def get_chat_completion(msg, model='gpt-4o-mini',**kargs):\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = msg\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def erase_page_tag(plist):\n",
    "    page_pattern = re.compile('<p.\\d*>')\n",
    "    plist = [page_pattern.sub('',x) for x in plist]\n",
    "    return plist\n",
    "\n",
    "def extract_text_between_tag(text, tag):\n",
    "    # Create a regex pattern for the specified tag\n",
    "    pattern = f'<{tag}>(.*?)</{tag}>'\n",
    "    # Use re.findall to extract all occurrences between the specified tags\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23276f3b-ebbf-449e-a6a1-287101310a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "sample1 = text_by_pages('./sample1.txt')\n",
    "print(len(sample1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6835c0f2-6117-4c7e-b449-c49b2d481444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<subject>웹툰 추천 시스템</subject>\n",
      "<team>강나영, 김나현, 엄성원, 이철민</team>\n",
      "<index>프로젝트 소개, 데이터 수집 및 전처리, 모델 선택 및 학습, 웹툰 추천 시스템 구현, 결론 및 향후 과제</index>\n"
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"\"\"triple quotes 사이에 있는 문서를 참조하여 아래 3가지 작업을 수행하여라. 문서는 pdf 파일을 텍스트로 변환한 것이다. 문서는 <lf> 태그로 구분된 passage들로 구성되어 있다. passage들은 pdf에서 서로 다른 textbox이다. 단, 반복적으로 등장하는 passage는 무의미한 textbox이므로 답변에 포함시키지 말아라.\n",
    "- 작업1: 문서의 \"주제\"에 해당하는 paasage를 선택하여라.\n",
    "- 작업2: 사람의 이름을 추출하여 쉼표(,)로 연결하여라.\n",
    "- 작업3: 문서의 목차(index)를 추출하여 쉼표(,)로 연결하여라.\n",
    "\n",
    "3가지 작업 내용에 대해 다음과 같은 형식에 맞춰 답변하여라. 만약 작업의 결과가 명확하지 않다면 해당 결과에는 None을 출력하여라.\n",
    "<subject>작업1의 결과</subject>\n",
    "<team>작업2의 결과</team>\n",
    "<index>작업3의 결과</index>\n",
    "     \"\"\"\n",
    "    },\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": f'\"\"\"{\" \".join(sample1[:5]) + \" \".join(sample1[-3:])}\"\"\"'\n",
    "    }\n",
    "]\n",
    "answer1 = get_chat_completion(messages)\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab9de77-ef6c-4ddc-a454-7325d5fb3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 별루...\n",
    "# messages=[\n",
    "#     {\"role\": \"system\", \n",
    "#      \"content\": \"\"\"\n",
    "#      You will be provided with a pair of passages delimited with <lf> tags. Passages represent the content of the project. Answer each given questions in a specified formats.\n",
    "     \n",
    "#      Question 1 - What is the main subject of project? Answer in <subject> tag.\n",
    "#      Question 2 - List the names of the people who participated in the project in <team> tag.\n",
    "#      Question 3 - What is the table of contents of this project? Answer in <index> tag.\n",
    "\n",
    "#      Answer in Korean.\n",
    "#      \"\"\"\n",
    "#     },\n",
    "#     {\"role\": \"user\", \n",
    "#      \"content\": f'\"\"\"{\" \".join(erase_page_tag(sample1[:5]))}\"\"\"'\n",
    "#     }\n",
    "# ]\n",
    "# answer = get_chat_completion(messages)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722176a-5284-4101-bf3f-a72bd3521ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20381b87-830a-4c0b-ba92-6b2210484b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73243672-1659-4ecc-a94a-dc819b8d206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "sample2 = text_by_pages('./sample2.txt')\n",
    "print(len(sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f5e22b2-cd08-40cf-b1ca-a2a03cb6849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<subject>Netflix Stock Price Prediction with News Topic & Sentiment</subject>  \n",
      "<team>송규헌, 권도영, 이태경, 김서윤, 한진솔</team>  \n",
      "<index>INTRODUCTION, DATA PREPROCESSING, MODELING, CONCLUSIONS AND LIMITATION</index>\n"
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"\"\"triple quotes 사이에 있는 문서를 참조하여 아래 3가지 작업을 수행하여라. 문서는 pdf 파일을 텍스트로 변환한 것이다. 문서는 <lf> 태그로 구분된 passage들로 구성되어 있다. passage들은 pdf에서 서로 다른 textbox이다.ㅍ단, 반복적으로 등장하는 passage는 무의미한 textbox이므로 답변에 포함시키지 말아라.\n",
    "- 작업1: 문서의 \"주제\"에 해당하는 paasage를 선택하여라.\n",
    "- 작업2: 사람의 이름을 추출하여 쉼표(,)로 연결하여라.\n",
    "- 작업3: 문서의 목차(index)를 추출하여 쉼표(,)로 연결하여라.\n",
    "\n",
    "3가지 작업 내용에 대해 다음과 같은 형식에 맞춰 답변하여라. 만약 작업의 결과가 명확하지 않다면 해당 결과에는 None을 출력하여라.\n",
    "<subject>작업1의 결과</subject>\n",
    "<team>작업2의 결과</team>\n",
    "<index>작업3의 결과</index>\n",
    "     \"\"\"\n",
    "    },\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": f'\"\"\"{\" \".join(sample2[:5]) + \" \".join(sample2[-3:])}\"\"\"'\n",
    "    }\n",
    "]\n",
    "answer2 = get_chat_completion(messages)\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2859f-1ee1-4719-a4d5-b0e050277260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301bc50-ef0f-4a45-bbda-77a3dc9314f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "efeb32a7-c1eb-4bfc-988e-c13f8306fca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "sample3 = text_by_pages('./sample3.txt')\n",
    "print(len(sample3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37053a2d-ab30-49a7-8941-e42d15fef598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<subject>비타민 11기 겨울 컨퍼런스 LLM 기반 거짓말 탐지기 : 피의자 신문 언어적 접근</subject>\n",
      "<team>조민호, 박소연, 박준형, 박세준</team>\n",
      "<index>서비스 배경 및 기획, 모델 구축 과정, 결론 및 제언</index>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "\t{\n",
    "\t\"role\" : \"system\",\n",
    "\t\"content\" : \"\"\"triple quotes 사이에 있는 문서를 참조하여 아래 3가지 작업을 수행하여라. 문서는 pdf 파일을 텍스트로 변환한 것이다. <p.숫자>는 문서의 페이지 번호를 의미한다. 문서는 <lf> 태그로 구분된 passage들로 구성되어 있다. passage들은 pdf에서 서로 다른 textbox이다. 단, 반복적으로 등장하는 passage는 무의미한 textbox이므로 답변에 포함시키지 말아라.\n",
    "- 작업1: 문서의 \"주제\"에 해당하는 paasage를 선택하여라.\n",
    "- 작업2: 사람의 이름을 추출하여 쉼표(,)로 연결하여라.\n",
    "- 작업3: 문서의 목차(table of contents)를 추출하여 쉼표(,)로 연결하여라.\n",
    "\n",
    "3가지 작업 내용에 대해 다음과 같은 형식에 맞춰 답변하여라. 만약 작업의 결과가 명확하지 않다면 해당 결과에는 None을 출력하여라.\n",
    "<subject>작업1의 결과</subject>\n",
    "<team>작업2의 결과</team>\n",
    "<index>작업3의 결과</index>\n",
    "     \"\"\"\n",
    "   },\n",
    "\t{\n",
    "\t\"role\" : \"user\",\n",
    "\t\"content\" : f'\"\"\"{\" \".join(sample3[:5]) + \" \".join(sample3[-3:])}\"\"\"'\n",
    "\t}\n",
    "]\n",
    "answer3 = get_chat_completion(messages)\n",
    "print(answer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45207b-826d-404e-967b-7347a23b1b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f6cf384-b590-420e-a3fa-cd862340c05a",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "1. Embedding Model, Clustering Algorithm 별로 군집화를 진행\n",
    "2. SS, CHI 지표를 측정하여 가장 pdf별로 가장 좋은 embedding model, clustering algo 조합 결과를 채택\n",
    "3. 직접 매긴 target cluster와 비교 -> ARI, HS 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d0d1ca9-3cf6-4602-aed8-61103c14aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def pca_best_component(X):\n",
    "    '''\n",
    "    최적의 PCA component값을 찾는 함수\n",
    "    '''\n",
    "    pca_optimize = PCA()\n",
    "    pca_optimize.fit(X)\n",
    "    \n",
    "    # 누적 설명된 분산 비율 계산\n",
    "    cumulative_variance = np.cumsum(pca_optimize.explained_variance_ratio_)\n",
    "    \n",
    "    # 99% 이상 설명력을 갖는 주성분 개수 계산\n",
    "    n_components = np.argmax(cumulative_variance >= 0.99) + 1\n",
    "    \n",
    "    # 설명된 분산 비율 시각화\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(cumulative_variance, marker='o', linestyle='--', color='b')\n",
    "    # plt.xlabel('Number of Components')\n",
    "    # plt.ylabel('Cumulative Explained Variance')\n",
    "    # plt.title('Cumulative Explained Variance by Number of Components')\n",
    "    # plt.axvline(x=n_components, color='r', linestyle=':', label=f'{n_components} components (95% explained variance)')\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    # plt.show()\n",
    "    # print(f\"99% 이상의 설명력을 갖기 위해 {n_components}개의 주성분으로 나누어야 합니다.\")\n",
    "\n",
    "    return n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b63fd91b-221a-4629-a65d-f30164a604b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kneed\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# 클러스터 개수 변화에 따른 SSE 확인, Elbow Point 찾기\n",
    "def SSE_graph(K, data):\n",
    "    SSE = []\n",
    "    k = 1\n",
    "    while 1 <= k <= K:\n",
    "        k_means = KMeans(n_clusters = k)     # 클러스터 개수가 k개인 모델 생성\n",
    "        k_means.fit(data)\n",
    "        SSE.append(k_means.inertia_)     # inertia : sum of squared distances of samples to their closest cluster center\n",
    "        k += 1\n",
    "     \n",
    "    plt.plot(range(1, K+1), SSE, 'o')     # K값에 따른 SSE 표시\n",
    "    plt.plot(range(1, K+1), SSE, '--')     # SSE값을 연결하는 직선 그리기\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('SSE')\n",
    "    kn = KneeLocator(range(1, K+1), SSE, curve = 'convex', direction = 'decreasing')    \n",
    "    return SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8e4e3b0-8767-418b-b861-c30bdc54d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 여러개의 클러스터링 갯수를 List로 입력 받아 각각의 실루엣 계수를 면적으로 \n",
    "### 시각화한 함수 작성\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "\n",
    "def visualize_silhouette(max_cluster, X_features):     \n",
    "    '''\n",
    "    K-means, Spectral clustering에서 최적의 K값을 찾는 함수\n",
    "    '''\n",
    "    # cluster의 최솟값은 3으로 고정하고 clusterlist를 만든다.\n",
    "    min_cluster = 4\n",
    "    cluster_lists = [x for x in range(min_cluster,max_cluster)]\n",
    "    \n",
    "    # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함\n",
    "    n_cols = len(cluster_lists)\n",
    "    \n",
    "    # plt.subplots()으로 리스트에 기재된 클러스터링 수만큼의 sub figures를 가지는 axs 생성 \n",
    "    # fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols)\n",
    "\n",
    "    results = []\n",
    "    # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화\n",
    "    for ind, n_cluster in enumerate(cluster_lists):\n",
    "        \n",
    "        # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산. \n",
    "        clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0)\n",
    "        cluster_labels = clusterer.fit_predict(X_features)\n",
    "        \n",
    "        sil_avg = silhouette_score(X_features, cluster_labels)\n",
    "        sil_values = silhouette_samples(X_features, cluster_labels)\n",
    "        \n",
    "        # y_lower = 10\n",
    "        # axs[ind].set_title('Number of Cluster : '+ str(n_cluster)+'\\n' \\\n",
    "        #                   'Silhouette Score :' + str(round(sil_avg,3)) )\n",
    "        # axs[ind].set_xlabel(\"The silhouette coefficient values\")\n",
    "        # axs[ind].set_ylabel(\"Cluster label\")\n",
    "        # axs[ind].set_xlim([-0.1, 1])\n",
    "        # axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10])\n",
    "        # axs[ind].set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        # axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        \n",
    "        # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현. \n",
    "        # for i in range(n_cluster):\n",
    "        #     ith_cluster_sil_values = sil_values[cluster_labels==i]\n",
    "        #     ith_cluster_sil_values.sort()\n",
    "            \n",
    "        #     size_cluster_i = ith_cluster_sil_values.shape[0]\n",
    "        #     y_upper = y_lower + size_cluster_i\n",
    "            \n",
    "        #     color = cm.nipy_spectral(float(i) / n_cluster)\n",
    "        #     axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, \\\n",
    "        #                         facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        #     axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        #     y_lower = y_upper + 10\n",
    "            \n",
    "            \n",
    "        # axs[ind].axvline(x=sil_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        results.append([sil_avg, np.var([x if x>=0 else 0 for x in sil_values])])\n",
    "\n",
    "    ### 결과를 토대로 최적의 K 값을 도출\n",
    "    # 1. 실루엣 계수의 평균과 분산에 대해 MinMax 정규화\n",
    "    results_df = pd.DataFrame(data=results,columns=['avg','var']) \n",
    "    mms = MinMaxScaler()\n",
    "    results_df[['avg_norm', 'var_norm']] = mms.fit_transform(results_df[['avg', 'var']])\n",
    "    # 2. 표준화된 var을 1에서 빼서 \"분산은 작은 값이 좋음\"을 반영\n",
    "    results_df['var_norm'] = 1 - results_df['var_norm']\n",
    "    # 3. 가중 평균 계산 (평균에 대한 가중치: 0.65, 분산에 대한 가중치: 0.35 -> 가중치에 대한 근거는 경험적 판단.)\n",
    "    results_df['score'] = results_df['avg_norm'] * 0.65 + results_df['var_norm'] * 0.35    \n",
    "    sorted_results = results_df.sort_values('score',ascending=False)\n",
    "    optimal_k = sorted_results.index[0]+min_cluster\n",
    "    return optimal_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f1269-973e-4dab-8a4f-7116a9f9362f",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "- BERT 기반의 LaBSE 모델을 사용하여 문장 임베딩 (max sequence length = 256)\n",
    "- ConvBERT 모델 : text feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79795818-692a-4f44-975b-4ad071417747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers\n",
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "904017d3-a21b-497a-bcb4-1eb4f59b8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sentence Transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "se_model = SentenceTransformer('sentence-transformers/LaBSE') # BERT 기반 문장 임베딩 모델\n",
    "se_model.__classname__ = \"LaBSE\"\n",
    "\n",
    "# 2. ConvBERT \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "convbert_tokenizer = AutoTokenizer.from_pretrained(\"YituTech/conv-bert-base\")\n",
    "convbert_model = AutoModel.from_pretrained(\"YituTech/conv-bert-base\")\n",
    "convbert_model.__classname__ = \"ConvBERT\"\n",
    "\n",
    "# 3. Canine\n",
    "from transformers import CanineTokenizer, CanineModel\n",
    "canine_tokenizer = CanineTokenizer.from_pretrained('google/canine-c')\n",
    "canine_model = CanineModel.from_pretrained('google/canine-c')\n",
    "canine_model.__classname__ = \"Canine-C\"\n",
    "\n",
    "# 4. OpenAI text-embedding-ada-002\n",
    "# !pip install -U tiktoken\n",
    "# import tiktoken\n",
    "# openai_model = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "# from transformers import GPT2Tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('Xenova/text-embedding-ada-002')\n",
    "\n",
    "# 5. GIST-Embedding v0\n",
    "gist_model = SentenceTransformer(\"avsolatorio/GIST-Embedding-v0\", \n",
    "                            revision=None)\n",
    "gist_model.__classname__ = \"GIST-Embedding-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8806534a-53a1-448f-9fd6-b08bd838618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   '''\n",
    "   - pricing : text-embedding-3-small = $0.02/1M tokens\n",
    "       텍스트가 많은 pdf는 대략 5,000 tokens -> pdf 200개에 0.02 달러.\n",
    "   text : 한 문장\n",
    "   return : 한 문장에 대한 embedding (output dimension = 1536)\n",
    "   '''\n",
    "   return client.embeddings.create(input = text, model=model).data[0].embedding\n",
    "tmpt = [s.replace('\\n',' ') for s in erase_page_tag(sample2)]\n",
    "ees = []\n",
    "for tt in tmpt:\n",
    "    tmp = get_embedding(tt)\n",
    "    ees.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ec5929ec-be3a-43d6-99d5-cc0ea06e4386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 1536)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ees).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e24bd07f-2264-48ca-87bc-80912e3acd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'sentence':[se_model, gist_model, gpt],\n",
    "    'token':[(convbert_tokenizer,convbert_model),(canine_tokenizer,canine_model)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "815b0e70-d25c-4699-8ade-368bef0c1583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAI'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.__classname__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "df3b116f-4de7-43ad-9d9c-1480644f65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering,KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import gc\n",
    "\n",
    "def clustering(text, n_clusters, model_dict, random_state):\n",
    "    '''\n",
    "    text : pdf 문서를 page별로 split하고, <p>태그와 \\n 문자열을 없앤 텍스트 리스트 전달.\n",
    "    n_clusters : 클러스터의 개수를 지정 -> extract_text_between_tag(text,'index')로 추출한 인덱스의 개수 + 2\n",
    "\n",
    "    return\n",
    "        - total_metrics : 모델별, 클러스터링 알고리즘별 클러스터링 결과를 SS, CHI로 평가지표로 측정한 결과에 대한 데이터프레임\n",
    "        - total_dict : model_name을 key값으로 text, cluster label, embedding 컬럼으로 하는 데이터프레임을 value로 갖는 딕셔너리\n",
    "    '''\n",
    "    total_metrics = pd.DataFrame(columns=['model','kmeans_ss','spectral_ss','kmeans_chi','spectral_chi'])\n",
    "    total_dict = {}\n",
    "    max_cluster = len(text)-1\n",
    "    # sentence 단위로 임베딩하는 모델의 경우\n",
    "    for model in model_dict['sentence']:\n",
    "        model_name = model.__classname__\n",
    "        if model_name == 'Encoding':\n",
    "            text_embeddings = [model.encode(t) for t in text]\n",
    "        elif model_name in set(['LaBSE','GIST-Embedding-v0']):\n",
    "            text_embeddings = model.encode(text, convert_to_tensor=True)\n",
    "            text_embeddings = text_embeddings.cpu().detach()\n",
    "        elif model_name == 'OpenAI':\n",
    "            text_embeddings = []\n",
    "            for t in text:\n",
    "                embedding = model.get_embedding(t)\n",
    "                text_embeddings.append(embedding)\n",
    "            text_embeddings = np.array(text_embeddings)\n",
    "        else:\n",
    "            text_embeddings = model.encode(text)\n",
    "        ### 차원 축소 : PCA + t-SNE\n",
    "        # PCA n_component 구하기\n",
    "        optimal_component = pca_best_component(text_embeddings)\n",
    "        # PCA\n",
    "        text_embeddings = PCA(n_components=optimal_component, random_state=random_state).fit_transform(text_embeddings)\n",
    "        # t-SNE (3차원으로 축소하는 것으로 고정)\n",
    "        # perplexity는 일반적으로 데이터 개수의 3분의 1 이하\n",
    "        general_perplexity = int(text_embeddings.shape[0]/3)\n",
    "        # 최솟값은 5, 최댓값은 50으로 제한\n",
    "        if general_perplexity<5:\n",
    "            general_perplexity = 5\n",
    "        elif general_perplexity>50:\n",
    "            general_perplexity=50\n",
    "        text_embeddings = TSNE(n_components=3, perplexity=general_perplexity,\n",
    "                               random_state=random_state).fit_transform(text_embeddings)\n",
    "        # 최적의 k값 찾기 [x] >> pdf의 인덱스 개수에 맞춰 n_clusters를 설정\n",
    "        # optimal_k = visualize_silhouette(max_cluster=max_cluster,X_features=text_embeddings)\n",
    "        # kmeans와 spectral clustering 진행\n",
    "        kmeans = KMeans(n_clusters=n_clusters, \n",
    "                init='k-means++', # centroid들을 서로 최대한 멀리 배치하는 initialisation 방식\n",
    "                max_iter = 500,\n",
    "                random_state = random_state)\n",
    "        spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors',\n",
    "                             random_state=random_state)\n",
    "        kmeans.fit(text_embeddings)\n",
    "        spectral.fit(text_embeddings)\n",
    "        # 평가지표 계산\n",
    "        new_row = pd.DataFrame(data=[[\n",
    "            model_name, # model name\n",
    "            silhouette_score(text_embeddings, kmeans.labels_), # kmeans ss\n",
    "            silhouette_score(text_embeddings, spectral.labels_), # spectral ss\n",
    "            calinski_harabasz_score(text_embeddings, kmeans.labels_), # kmeans chi\n",
    "            calinski_harabasz_score(text_embeddings, spectral.labels_) # spectral chi\n",
    "        ]], columns = total_metrics.columns)\n",
    "\n",
    "        # Save results\n",
    "        total_metrics = pd.concat([total_metrics, new_row],axis=0,ignore_index=True)\n",
    "        temp_df = pd.DataFrame(data=[\n",
    "            text, kmeans.labels_, spectral.labels_\n",
    "        ]).transpose()\n",
    "        temp_df.columns = ['text','kmeans','spectral']\n",
    "        total_dict[model_name] = pd.concat([\n",
    "            temp_df,pd.DataFrame(text_embeddings)\n",
    "        ],axis=1)\n",
    "\n",
    "        # garbage collection > 별 의미 없는 듯\n",
    "        # gc.collect()\n",
    "\n",
    "    # token 단위로 임베딩하는 모델의 경우\n",
    "    for tokenizer, model in model_dict['token']:\n",
    "        model_name = model.__classname__\n",
    "        if model_name == 'Canine-C':\n",
    "            token = tokenizer(erase_page_tag(text), padding='longest', truncation=True, return_tensors='pt')\n",
    "        else: # ConvBERT\n",
    "            token = tokenizer(erase_page_tag(text), padding='longest',return_tensors='pt')\n",
    "        text_embeddings = model(**token).last_hidden_state.detach()\n",
    "        flattened = text_embeddings.view(text_embeddings.shape[0],-1)\n",
    "        ### 차원 축소 : PCA + t-SNE\n",
    "        # PCA n_component 구하기\n",
    "        optimal_component = pca_best_component(flattened)\n",
    "        # PCA\n",
    "        text_embeddings = PCA(n_components=optimal_component, random_state=random_state).fit_transform(flattened)\n",
    "        # t-SNE (3차원으로 축소하는 것으로 고정)\n",
    "        # perplexity는 일반적으로 데이터 개수의 3분의 1 이하\n",
    "        general_perplexity = int(text_embeddings.shape[0]/3)\n",
    "        # 최솟값은 5, 최댓값은 50으로 제한\n",
    "        if general_perplexity<5:\n",
    "            general_perplexity = 5\n",
    "        elif general_perplexity>50:\n",
    "            general_perplexity=50\n",
    "        text_embeddings = TSNE(n_components=3, perplexity=general_perplexity,\n",
    "                               random_state=random_state).fit_transform(text_embeddings)\n",
    "\n",
    "        # 최적의 k값 찾기 [x] >> pdf의 인덱스 개수에 맞춰 n_clusters를 설정\n",
    "        # optimal_k = visualize_silhouette(max_cluster=max_cluster,X_features=text_embeddings)\n",
    "        # kmeans와 spectral clustering 진행\n",
    "        kmeans = KMeans(n_clusters=n_clusters, \n",
    "                init='k-means++', # centroid들을 서로 최대한 멀리 배치하는 initialisation 방식\n",
    "                max_iter = 500,\n",
    "                random_state = random_state)\n",
    "        spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors',\n",
    "                             random_state=random_state)\n",
    "        kmeans.fit(text_embeddings)\n",
    "        spectral.fit(text_embeddings)\n",
    "        # 평가지표 계산\n",
    "        new_row = pd.DataFrame(data=[[\n",
    "            model_name, # model name\n",
    "            silhouette_score(text_embeddings, kmeans.labels_), # kmeans ss\n",
    "            silhouette_score(text_embeddings, spectral.labels_), # spectral ss\n",
    "            calinski_harabasz_score(text_embeddings, kmeans.labels_), # kmeans chi\n",
    "            calinski_harabasz_score(text_embeddings, spectral.labels_) # spectral chi\n",
    "        ]], columns = total_metrics.columns)\n",
    "\n",
    "        # Save results\n",
    "        total_metrics = pd.concat([total_metrics, new_row],axis=0,ignore_index=True)\n",
    "        temp_df = pd.DataFrame(data=[\n",
    "            text, kmeans.labels_, spectral.labels_\n",
    "        ]).transpose()\n",
    "        temp_df.columns = ['text','kmeans','spectral']\n",
    "        total_dict[model_name] = pd.concat([\n",
    "            temp_df,pd.DataFrame(text_embeddings)\n",
    "        ],axis=1)\n",
    "\n",
    "        # garbage collection\n",
    "        # gc.collect()\n",
    "\n",
    "    return total_metrics, total_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "78f6d6f2-bbd9-425a-86b5-0adc91a2e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_n_algo(metrics_df):\n",
    "    \"\"\"\n",
    "    metrics_df : clustering함수 결과로 얻은 total_metrics\n",
    "\n",
    "    return : total_metrics에서 가장 성능이 좋은 model과 클러스터링 알고리즘을 반환 \n",
    "    -> total_dict에서 해당 모델 이름과 클러스터링 알고리즘 이름으로 클러스터 결과를 찾을 수 있음.\n",
    "    \"\"\"\n",
    "    metrics_df['kmeans']=metrics_df['kmeans_ss']+metrics_df['kmeans_chi']\n",
    "    metrics_df['spectral']=metrics_df['spectral_ss']+metrics_df['spectral_chi']\n",
    "    \n",
    "    max_value = metrics_df[['spectral','kmeans']].max().max()\n",
    "    # 최대값을 가진 행과 열 찾기\n",
    "    max_value_row_col = metrics_df[['spectral', 'kmeans']].apply(lambda x: x == max_value).stack()\n",
    "    max_value_row_col = max_value_row_col[max_value_row_col].index[0]\n",
    "    model_name = metrics_df.loc[max_value_row_col[0],'model']\n",
    "    clustering_algo = max_value_row_col[1]\n",
    "    return model_name, clustering_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eb6a9b66-53b0-4896-9a3f-7ff7a3d9f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renew_cluster_byorder(dfs, best):\n",
    "    '''\n",
    "    dfs : clustering 함수 결과로 얻은 total_dict\n",
    "    best = (best_model, best_algo) 튜플\n",
    "        - best_model : best_model_n_algo 결과로 얻은 model_name\n",
    "        - best_algo : best_model_n_algo 결과로 얻은 clustering_algo\n",
    "\n",
    "    return df : 순서가 엉켜있는 cluster label을 재정렬한 결과를 반환\n",
    "    '''\n",
    "    best_model, best_algo = best\n",
    "    df = dfs[best_model][['text',best_algo]]\n",
    "    cnum = len(df[best_algo].unique())\n",
    "    checklist = []\n",
    "    for c in df[best_algo]:\n",
    "        if c not in checklist:\n",
    "            checklist.append(c)\n",
    "        if len(checklist) == cnum:\n",
    "            break\n",
    "    renew_c = list(range(cnum))\n",
    "    df['new_cluster'] = df[best_algo].apply(lambda x:renew_c[checklist.index(x)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756b6ed-cc56-4d69-af6a-0cf1f55a6c6e",
   "metadata": {},
   "source": [
    "### sample별 clustering 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5cc928dc-0d5c-4c0a-95e3-d41dd494c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214.34 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "### 첫 5페이지에서 요약한 <index> 정보를 통해 n_clusters 설정\n",
    "s1_nclusters = len(extract_text_between_tag(answer1,'index')[0].split(',')) + 2\n",
    "s2_nclusters = len(extract_text_between_tag(answer2,'index')[0].split(',')) + 2\n",
    "s3_nclusters = len(extract_text_between_tag(answer3,'index')[0].split(',')) + 2\n",
    "\n",
    "# 각 sample text의 page tag와 \\n를 없애준다.\n",
    "sample1_text = [s.replace('\\n',' ') for s in erase_page_tag(sample1)]\n",
    "sample2_text = [s.replace('\\n',' ') for s in erase_page_tag(sample2)]\n",
    "sample3_text = [s.replace('\\n',' ') for s in erase_page_tag(sample3)]\n",
    "\n",
    "s1_metrics, s1_dfs = clustering(text=sample1_text,\n",
    "                       n_clusters=s1_nclusters,\n",
    "                       model_dict=model_dict,\n",
    "                       random_state = RANDOM_STATE)\n",
    "s2_metrics, s2_dfs = clustering(text=sample2_text,\n",
    "                       n_clusters=s2_nclusters, \n",
    "                       model_dict=model_dict,\n",
    "                       random_state = RANDOM_STATE)\n",
    "s3_metrics, s3_dfs = clustering(text=sample3_text,\n",
    "                       n_clusters=s3_nclusters, \n",
    "                       model_dict=model_dict,\n",
    "                       random_state = RANDOM_STATE)\n",
    "print(f\"{time.time()-start_time:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "101f929e-9c4d-4f33-8a07-1b8df1f4c399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>kmeans_ss</th>\n",
       "      <th>spectral_ss</th>\n",
       "      <th>kmeans_chi</th>\n",
       "      <th>spectral_chi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LaBSE</td>\n",
       "      <td>0.143071</td>\n",
       "      <td>0.129194</td>\n",
       "      <td>6.920010</td>\n",
       "      <td>7.010319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GIST-Embedding-v0</td>\n",
       "      <td>0.177095</td>\n",
       "      <td>0.059558</td>\n",
       "      <td>8.247603</td>\n",
       "      <td>4.382099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenAI</td>\n",
       "      <td>0.152055</td>\n",
       "      <td>0.100633</td>\n",
       "      <td>6.379099</td>\n",
       "      <td>5.368058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ConvBERT</td>\n",
       "      <td>0.213544</td>\n",
       "      <td>0.042948</td>\n",
       "      <td>9.257685</td>\n",
       "      <td>4.993456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canine-C</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>0.182227</td>\n",
       "      <td>8.512961</td>\n",
       "      <td>6.178575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  kmeans_ss  spectral_ss  kmeans_chi  spectral_chi\n",
       "0              LaBSE   0.143071     0.129194    6.920010      7.010319\n",
       "1  GIST-Embedding-v0   0.177095     0.059558    8.247603      4.382099\n",
       "2             OpenAI   0.152055     0.100633    6.379099      5.368058\n",
       "3           ConvBERT   0.213544     0.042948    9.257685      4.993456\n",
       "4           Canine-C   0.179941     0.182227    8.512961      6.178575"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0d3123e-3751-41e9-881b-6c92e76df3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>kmeans_ss</th>\n",
       "      <th>spectral_ss</th>\n",
       "      <th>kmeans_chi</th>\n",
       "      <th>spectral_chi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LaBSE</td>\n",
       "      <td>0.225770</td>\n",
       "      <td>0.150443</td>\n",
       "      <td>24.336221</td>\n",
       "      <td>9.058081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GIST-Embedding-v0</td>\n",
       "      <td>0.218695</td>\n",
       "      <td>0.252737</td>\n",
       "      <td>16.262513</td>\n",
       "      <td>16.953961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenAI</td>\n",
       "      <td>0.231234</td>\n",
       "      <td>0.198090</td>\n",
       "      <td>17.189459</td>\n",
       "      <td>11.637532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ConvBERT</td>\n",
       "      <td>0.210228</td>\n",
       "      <td>0.193859</td>\n",
       "      <td>16.267961</td>\n",
       "      <td>15.038704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canine-C</td>\n",
       "      <td>0.204173</td>\n",
       "      <td>0.188609</td>\n",
       "      <td>15.769724</td>\n",
       "      <td>11.682071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  kmeans_ss  spectral_ss  kmeans_chi  spectral_chi\n",
       "0              LaBSE   0.225770     0.150443   24.336221      9.058081\n",
       "1  GIST-Embedding-v0   0.218695     0.252737   16.262513     16.953961\n",
       "2             OpenAI   0.231234     0.198090   17.189459     11.637532\n",
       "3           ConvBERT   0.210228     0.193859   16.267961     15.038704\n",
       "4           Canine-C   0.204173     0.188609   15.769724     11.682071"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8f6cd31d-d14d-45ee-957f-ff241c1e8c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>kmeans_ss</th>\n",
       "      <th>spectral_ss</th>\n",
       "      <th>kmeans_chi</th>\n",
       "      <th>spectral_chi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LaBSE</td>\n",
       "      <td>0.180692</td>\n",
       "      <td>0.227414</td>\n",
       "      <td>9.786701</td>\n",
       "      <td>11.935285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GIST-Embedding-v0</td>\n",
       "      <td>0.209102</td>\n",
       "      <td>0.183199</td>\n",
       "      <td>10.779423</td>\n",
       "      <td>9.693611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenAI</td>\n",
       "      <td>0.242993</td>\n",
       "      <td>0.254565</td>\n",
       "      <td>12.287375</td>\n",
       "      <td>12.959210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ConvBERT</td>\n",
       "      <td>0.304815</td>\n",
       "      <td>0.284957</td>\n",
       "      <td>16.971637</td>\n",
       "      <td>17.315836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canine-C</td>\n",
       "      <td>0.207217</td>\n",
       "      <td>0.260713</td>\n",
       "      <td>11.004939</td>\n",
       "      <td>13.529499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  kmeans_ss  spectral_ss  kmeans_chi  spectral_chi\n",
       "0              LaBSE   0.180692     0.227414    9.786701     11.935285\n",
       "1  GIST-Embedding-v0   0.209102     0.183199   10.779423      9.693611\n",
       "2             OpenAI   0.242993     0.254565   12.287375     12.959210\n",
       "3           ConvBERT   0.304815     0.284957   16.971637     17.315836\n",
       "4           Canine-C   0.207217     0.260713   11.004939     13.529499"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9a69fa16-df5e-4e8f-a1c5-060667e1b2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1 : ('ConvBERT', 'kmeans')\n",
      "sample2 : ('LaBSE', 'kmeans')\n",
      "sample3 : ('ConvBERT', 'spectral')\n"
     ]
    }
   ],
   "source": [
    "print(f\"sample1 :\",best_model_n_algo(s1_metrics))\n",
    "print(f\"sample2 :\",best_model_n_algo(s2_metrics))\n",
    "print(f\"sample3 :\",best_model_n_algo(s3_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2fc4a669-55d1-470d-8749-c10dd31e87c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 6 5\n"
     ]
    }
   ],
   "source": [
    "print(s1_nclusters,s2_nclusters,s3_nclusters)\n",
    "s1_renew = renew_cluster_byorder(dfs=s1_dfs, best=best_model_n_algo(metrics_df=s1_metrics))\n",
    "s2_renew = renew_cluster_byorder(dfs=s2_dfs, best=best_model_n_algo(metrics_df=s2_metrics))\n",
    "s3_renew = renew_cluster_byorder(dfs=s3_dfs, best=best_model_n_algo(metrics_df=s3_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "08ec834a-e91a-484a-aff9-476dc44df4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내가 직접 매긴 정답 clustering\n",
    "s1_renew['target'] = [0,0,1,2,2,2,3,3,3,3,3,3,3,3,3,3,3,4,4,5,5,6]\n",
    "s2_renew['target'] = [0,0,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5]\n",
    "s3_renew['target'] = [0,0,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5e91d346-e4fb-4c7e-8298-e10629e77d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1966148852306979 0.6195675237327858\n",
      "0.4258502217969905 0.5349039903379956\n",
      "0.002308940610126665 0.22930542090012493\n"
     ]
    }
   ],
   "source": [
    "# ARI, HS\n",
    "from sklearn.metrics import adjusted_rand_score, homogeneity_score\n",
    "s1_air = adjusted_rand_score(s1_renew['target'], s1_renew['new_cluster'])\n",
    "s1_hs = homogeneity_score(s1_renew['target'], s1_renew['new_cluster'])\n",
    "print(s1_air, s1_hs)\n",
    "\n",
    "s2_air = adjusted_rand_score(s2_renew['target'], s2_renew['new_cluster'])\n",
    "s2_hs = homogeneity_score(s2_renew['target'], s2_renew['new_cluster'])\n",
    "print(s2_air, s2_hs)\n",
    "\n",
    "s3_air = adjusted_rand_score(s3_renew['target'], s3_renew['new_cluster'])\n",
    "s3_hs = homogeneity_score(s3_renew['target'], s3_renew['new_cluster'])\n",
    "print(s3_air, s3_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "655147d5-3945-4eb1-87ed-8f426d3d8b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>kmeans</th>\n",
       "      <th>new_cluster</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BITAmin   24-1R 학기 프로젝트   독자와 웹툰 간 로그데이터를 활...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BITAmin   TABLE OF CONTENTS   목차 소개   01 프로...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01 프로젝트 소개   프로젝트 배경    정제된 '대량의 데이터'를 사용하여...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02 데이터 수집 및 전처리   BITAmin   데이터 소스 설명</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02 데이터 수집 및 전처리   BITAmin   데이터 전처리</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02 데이터 수집 및 전처리   BITAmin   데이터 전처리   피봇 테이...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>03 모델 선택 및 학습 - CF   BITAmin   CF(Collabora...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>03 모델 선택 및 학습 - CF   BITAmin   CF(Collabora...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>03 모델 선택 및 학습 - NCF   BITAmin   NCF(Neural ...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>03 모델 선택 및 학습 - NCF   BITAmin   NCF(Neural ...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>03 모델 선택 및 학습 - SVD   BITAmin   SVD(Singula...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>03 모델 선택 및 학습 - SVD   BITAmin   SVD(Singula...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>03 모델 선택 및 학습 - SVD   BITAmin   SVD(Singula...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>03 모델 선택 및 학습 - ALS   BITAmin   ALS(Alterna...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>03  최종 모델 선정   BITAmin   ALS(Alternative L...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>03  최종 모델 선정   BITAmin   ALS(Alternative L...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>03  최종 모델 선정   BITAmin   ALS(Alternative L...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>04 웹툰 추천 시스템 구현 - 기존 사용자    BITAmin   사용자가...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>04 웹툰 추천 시스템 구현 - 신규 사용자    BITAmin   좋아하는...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>05 결론 및 향후 과제    BITAmin   프로젝트 요약 및 의의   0...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>05 결론 및 향후 과제    BITAmin   한계점   이미 정제된 데이터...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BITAmin  24-1R 학기 프로젝트   감사합니다  RecSys Team ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text kmeans  new_cluster  \\\n",
       "0      BITAmin   24-1R 학기 프로젝트   독자와 웹툰 간 로그데이터를 활...      5            0   \n",
       "1      BITAmin   TABLE OF CONTENTS   목차 소개   01 프로...      1            1   \n",
       "2      01 프로젝트 소개   프로젝트 배경    정제된 '대량의 데이터'를 사용하여...      0            2   \n",
       "3          02 데이터 수집 및 전처리   BITAmin   데이터 소스 설명           5            0   \n",
       "4           02 데이터 수집 및 전처리   BITAmin   데이터 전처리            2            3   \n",
       "5      02 데이터 수집 및 전처리   BITAmin   데이터 전처리   피봇 테이...      5            0   \n",
       "6      03 모델 선택 및 학습 - CF   BITAmin   CF(Collabora...      3            4   \n",
       "7      03 모델 선택 및 학습 - CF   BITAmin   CF(Collabora...      6            5   \n",
       "8      03 모델 선택 및 학습 - NCF   BITAmin   NCF(Neural ...      6            5   \n",
       "9      03 모델 선택 및 학습 - NCF   BITAmin   NCF(Neural ...      6            5   \n",
       "10     03 모델 선택 및 학습 - SVD   BITAmin   SVD(Singula...      3            4   \n",
       "11     03 모델 선택 및 학습 - SVD   BITAmin   SVD(Singula...      6            5   \n",
       "12     03 모델 선택 및 학습 - SVD   BITAmin   SVD(Singula...      6            5   \n",
       "13     03 모델 선택 및 학습 - ALS   BITAmin   ALS(Alterna...      1            1   \n",
       "14      03  최종 모델 선정   BITAmin   ALS(Alternative L...      4            6   \n",
       "15      03  최종 모델 선정   BITAmin   ALS(Alternative L...      3            4   \n",
       "16      03  최종 모델 선정   BITAmin   ALS(Alternative L...      2            3   \n",
       "17      04 웹툰 추천 시스템 구현 - 기존 사용자    BITAmin   사용자가...      1            1   \n",
       "18      04 웹툰 추천 시스템 구현 - 신규 사용자    BITAmin   좋아하는...      1            1   \n",
       "19     05 결론 및 향후 과제    BITAmin   프로젝트 요약 및 의의   0...      0            2   \n",
       "20     05 결론 및 향후 과제    BITAmin   한계점   이미 정제된 데이터...      3            4   \n",
       "21    BITAmin  24-1R 학기 프로젝트   감사합니다  RecSys Team ...      2            3   \n",
       "\n",
       "    target  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  \n",
       "3        2  \n",
       "4        2  \n",
       "5        2  \n",
       "6        3  \n",
       "7        3  \n",
       "8        3  \n",
       "9        3  \n",
       "10       3  \n",
       "11       3  \n",
       "12       3  \n",
       "13       3  \n",
       "14       3  \n",
       "15       3  \n",
       "16       3  \n",
       "17       4  \n",
       "18       4  \n",
       "19       5  \n",
       "20       5  \n",
       "21       6  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_renew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "778e77f8-15a3-4159-a82b-ce90c1c85d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>kmeans</th>\n",
       "      <th>new_cluster</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p.1&gt; \\n 2024 BITAmin 겨울 연합프로젝트 시계열 1조 \\n Netf...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p.2&gt; \\n CONTENTS \\n 01. INTRODUCTION \\n 02. D...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p.3&gt; \\n 01. INTRODUCTION \\n  \\n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p.4&gt; \\n 01. INTRODUCTION \\n 1.1 Background of...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p.5&gt; \\n 01. INTRODUCTION \\n 1.1 Background of...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;p.6&gt; \\n 01. INTRODUCTION \\n 1.2 Brief Project...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;p.7&gt; \\n 01. INTRODUCTION \\n 1.3 Data collecti...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;p.8&gt; \\n 01. INTRODUCTION \\n 1.3 Data collecti...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;p.9&gt; \\n 01. INTRODUCTION \\n 1.3 Data collecti...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;p.10&gt; \\n 02. DATA PREPROCESSING \\n  \\n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;p.11&gt; \\n 02. DATA PREPROCESING \\n 2.1 Make de...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;p.12&gt; \\n 02. DATA PREPROCESING \\n 2.2 Add ind...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;p.13&gt; \\n 02. DATA PREPROCESING \\n 2.2 Add ind...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;p.14&gt; \\n 02. DATA PREPROCESING \\n 2.3 Peer An...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;p.15&gt; \\n 02. DATA PREPROCESING \\n 2.3 Peer An...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;p.16&gt; \\n 02. DATA PREPROCESING \\n 2.4 Remove ...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;p.17&gt; \\n 02. DATA PREPROCESING \\n 2.5 Make de...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;p.18&gt; \\n 02. DATA PREPROCESING \\n 2.6 Dataset...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;p.19&gt; \\n 02. DATA PREPROCESING \\n 2.6 Dataset...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;p.20&gt; \\n 03. MODELING \\n  \\n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;p.21&gt; \\n 03. MODELING \\n 3.1 Time Series  \\n ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;p.22&gt; \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;p.23&gt; \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;p.24&gt; \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;p.25&gt; \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;p.26&gt; \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;p.27&gt; \\n 04. CONCLUSION AND LIMITATION \\n  \\n</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;p.28&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;p.29&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;p.30&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;p.31&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;p.32&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;p.33&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;p.34&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;p.35&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;p.36&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;p.37&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;p.38&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;p.39&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;p.40&gt; \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;p.41&gt; \\n2024 BITAmin 겨울 연합프로젝트 시계열 1조 \\nTHANK...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text kmeans  new_cluster  \\\n",
       "0   <p.1> \\n 2024 BITAmin 겨울 연합프로젝트 시계열 1조 \\n Netf...      4            0   \n",
       "1   <p.2> \\n CONTENTS \\n 01. INTRODUCTION \\n 02. D...      1            1   \n",
       "2                    <p.3> \\n 01. INTRODUCTION \\n  \\n      1            1   \n",
       "3   <p.4> \\n 01. INTRODUCTION \\n 1.1 Background of...      5            2   \n",
       "4   <p.5> \\n 01. INTRODUCTION \\n 1.1 Background of...      2            3   \n",
       "5   <p.6> \\n 01. INTRODUCTION \\n 1.2 Brief Project...      5            2   \n",
       "6   <p.7> \\n 01. INTRODUCTION \\n 1.3 Data collecti...      4            0   \n",
       "7   <p.8> \\n 01. INTRODUCTION \\n 1.3 Data collecti...      2            3   \n",
       "8   <p.9> \\n 01. INTRODUCTION \\n 1.3 Data collecti...      4            0   \n",
       "9             <p.10> \\n 02. DATA PREPROCESSING \\n  \\n      1            1   \n",
       "10  <p.11> \\n 02. DATA PREPROCESING \\n 2.1 Make de...      3            4   \n",
       "11  <p.12> \\n 02. DATA PREPROCESING \\n 2.2 Add ind...      4            0   \n",
       "12  <p.13> \\n 02. DATA PREPROCESING \\n 2.2 Add ind...      3            4   \n",
       "13  <p.14> \\n 02. DATA PREPROCESING \\n 2.3 Peer An...      2            3   \n",
       "14  <p.15> \\n 02. DATA PREPROCESING \\n 2.3 Peer An...      2            3   \n",
       "15  <p.16> \\n 02. DATA PREPROCESING \\n 2.4 Remove ...      3            4   \n",
       "16  <p.17> \\n 02. DATA PREPROCESING \\n 2.5 Make de...      3            4   \n",
       "17  <p.18> \\n 02. DATA PREPROCESING \\n 2.6 Dataset...      2            3   \n",
       "18  <p.19> \\n 02. DATA PREPROCESING \\n 2.6 Dataset...      4            0   \n",
       "19                      <p.20> \\n 03. MODELING \\n  \\n      1            1   \n",
       "20  <p.21> \\n 03. MODELING \\n 3.1 Time Series  \\n ...      2            3   \n",
       "21  <p.22> \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...      4            0   \n",
       "22  <p.23> \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...      5            2   \n",
       "23  <p.24> \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...      4            0   \n",
       "24  <p.25> \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...      4            0   \n",
       "25  <p.26> \\n 03. MODELING \\n 3.2 Modeling  \\n Ste...      5            2   \n",
       "26     <p.27> \\n 04. CONCLUSION AND LIMITATION \\n  \\n      0            5   \n",
       "27  <p.28> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      0            5   \n",
       "28  <p.29> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      2            3   \n",
       "29  <p.30> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      5            2   \n",
       "30  <p.31> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      1            1   \n",
       "31  <p.32> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      1            1   \n",
       "32  <p.33> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      1            1   \n",
       "33  <p.34> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      5            2   \n",
       "34  <p.35> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      2            3   \n",
       "35  <p.36> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      3            4   \n",
       "36  <p.37> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      0            5   \n",
       "37  <p.38> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      4            0   \n",
       "38  <p.39> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      4            0   \n",
       "39  <p.40> \\n 04. CONCLUSIONS AND LIMITATIONS \\n 4...      3            4   \n",
       "40  <p.41> \\n2024 BITAmin 겨울 연합프로젝트 시계열 1조 \\nTHANK...      2            3   \n",
       "\n",
       "    target  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        2  \n",
       "10       2  \n",
       "11       2  \n",
       "12       2  \n",
       "13       2  \n",
       "14       2  \n",
       "15       2  \n",
       "16       2  \n",
       "17       2  \n",
       "18       2  \n",
       "19       3  \n",
       "20       3  \n",
       "21       3  \n",
       "22       3  \n",
       "23       3  \n",
       "24       3  \n",
       "25       3  \n",
       "26       4  \n",
       "27       4  \n",
       "28       4  \n",
       "29       4  \n",
       "30       4  \n",
       "31       4  \n",
       "32       4  \n",
       "33       4  \n",
       "34       4  \n",
       "35       4  \n",
       "36       4  \n",
       "37       4  \n",
       "38       4  \n",
       "39       4  \n",
       "40       5  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2_renew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "71fb3e0c-f7be-4618-b9fd-41678fce5185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spectral</th>\n",
       "      <th>new_cluster</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>비타민 11기 겨울 컨퍼런스   LLM 기반 거짓말 탐지기   : 피의자 신문...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서비스 배경 및 기획 | 모델 구축 과정 | 결론 및 제언   서비스 배경 및...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>서비스 배경 및 기획 ► 문제 상황   비타민 11기 겨울 컨퍼런스   거짓말...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>서비스 배경 및 기획 ► 문제 상황   비타민 11기 겨울 컨퍼런스   비언어...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>서비스 배경 및 기획 ► 문제 상황   비타민 11기 겨울 컨퍼런스   단순 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>서비스 배경 및 기획 ► 서비스 제안 및 사용 예시   비타민 11기 겨울 컨...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>서비스 배경 및 기획 ► 실무 파이프라인   비타민 11기 겨울 컨퍼런스   ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RAG Retrieval –Augmented Generation   사용자의 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PEFT Parameter Efficient Fine Tuning   적은 매...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>비타민 11기 겨울 컨퍼런스   서비스 배경 및 기획 | 모델 구축 과정 | ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>모델 구축 과정 ► STT 구현   비타민 11기 겨울 컨퍼런스</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>모델 구축 과정 ► STT 구현   비타민 11기 겨울 컨퍼런스   Speec...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   데이터형식...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   데이터가갖...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   Train...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   Synth...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   Synth...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   Synth...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스   ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스   ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스   ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스   ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>모델 구축 과정 ► RAG   비타민 11기 겨울 컨퍼런스   Finding ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>모델 구축 과정 ► RAG   비타민 11기 겨울 컨퍼런스   Final Ou...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>비타민 11기 겨울 컨퍼런스   서비스 배경 및 기획 | 모델 구축 과정 | ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>결론 및 제언 ► 프로젝트 의의   비타민 11기 겨울 컨퍼런스   프로젝트 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>결론 및 제언 ► 프로젝트 의의   비타민 11기 겨울 컨퍼런스   다른LLM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>End of Document  팀원 | 조민호 박소연 박준형 박세준       ...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text spectral  new_cluster  \\\n",
       "0      비타민 11기 겨울 컨퍼런스   LLM 기반 거짓말 탐지기   : 피의자 신문...        0            0   \n",
       "1      서비스 배경 및 기획 | 모델 구축 과정 | 결론 및 제언   서비스 배경 및...        4            1   \n",
       "2      서비스 배경 및 기획 ► 문제 상황   비타민 11기 겨울 컨퍼런스   거짓말...        4            1   \n",
       "3      서비스 배경 및 기획 ► 문제 상황   비타민 11기 겨울 컨퍼런스   비언어...        2            2   \n",
       "4      서비스 배경 및 기획 ► 문제 상황   비타민 11기 겨울 컨퍼런스   단순 ...        0            0   \n",
       "5      서비스 배경 및 기획 ► 서비스 제안 및 사용 예시   비타민 11기 겨울 컨...        1            3   \n",
       "6      서비스 배경 및 기획 ► 실무 파이프라인   비타민 11기 겨울 컨퍼런스   ...        0            0   \n",
       "7      RAG Retrieval –Augmented Generation   사용자의 ...        1            3   \n",
       "8      PEFT Parameter Efficient Fine Tuning   적은 매...        1            3   \n",
       "9      비타민 11기 겨울 컨퍼런스   서비스 배경 및 기획 | 모델 구축 과정 | ...        4            1   \n",
       "10           모델 구축 과정 ► STT 구현   비타민 11기 겨울 컨퍼런스             3            4   \n",
       "11     모델 구축 과정 ► STT 구현   비타민 11기 겨울 컨퍼런스   Speec...        2            2   \n",
       "12           모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스             3            4   \n",
       "13     모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   데이터형식...        0            0   \n",
       "14     모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   데이터가갖...        2            2   \n",
       "15     모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   Train...        3            4   \n",
       "16     모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   Synth...        2            2   \n",
       "17     모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   Synth...        2            2   \n",
       "18     모델 구축 과정 ► 데이터 구축   비타민 11기 겨울 컨퍼런스   Synth...        4            1   \n",
       "19      모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스             3            4   \n",
       "20     모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스   ...        1            3   \n",
       "21     모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스   ...        1            3   \n",
       "22     모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스   ...        2            2   \n",
       "23     모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스   ...        0            0   \n",
       "24      모델 구축 과정 ► Fine Tuning   비타민 11기 겨울 컨퍼런스             3            4   \n",
       "25     모델 구축 과정 ► RAG   비타민 11기 겨울 컨퍼런스   Finding ...        1            3   \n",
       "26     모델 구축 과정 ► RAG   비타민 11기 겨울 컨퍼런스   Final Ou...        1            3   \n",
       "27     비타민 11기 겨울 컨퍼런스   서비스 배경 및 기획 | 모델 구축 과정 | ...        4            1   \n",
       "28     결론 및 제언 ► 프로젝트 의의   비타민 11기 겨울 컨퍼런스   프로젝트 ...        1            3   \n",
       "29     결론 및 제언 ► 프로젝트 의의   비타민 11기 겨울 컨퍼런스   다른LLM...        0            0   \n",
       "30    End of Document  팀원 | 조민호 박소연 박준형 박세준       ...        3            4   \n",
       "\n",
       "    target  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        2  \n",
       "10       2  \n",
       "11       2  \n",
       "12       2  \n",
       "13       2  \n",
       "14       2  \n",
       "15       2  \n",
       "16       2  \n",
       "17       2  \n",
       "18       2  \n",
       "19       2  \n",
       "20       2  \n",
       "21       2  \n",
       "22       2  \n",
       "23       2  \n",
       "24       2  \n",
       "25       2  \n",
       "26       2  \n",
       "27       3  \n",
       "28       3  \n",
       "29       3  \n",
       "30       4  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_renew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4acb79b-20ce-483d-8c0e-d2b4abdb3d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<p.1> \\n BITAmin \\n 24-1R 학기 프로젝트 \\n 독자와 웹툰 간 로그데이터를 활용한 \\n 웹툰 추천 시스템 \\n 2024.06.05 \\n RecSys Team 2 | 강나영 김나현 엄성원 이철민 \\n  \\n<p.11> \\n 03 모델 선택 및 학습 - SVD \\n BITAmin \\n SVD(Singular Value Decomposition) 모델 \\n 개념 \\n 사용자-아이템 행렬을 세 개의 행렬로 분해하여  \\n 사용자와 아이템의 잠재적 특성 (latent factors)을 추출하는 방식. \\n 특징  \\n m x n 행렬로도 대각행렬을 통한 특이값 분해가 가능하기 때문에  \\n 유저, 아이템의 행과 열의 개수가 다른 추천모델에도 적합하여 \\n 잠재 요인을 이용하여 사용자가 아직 평가하지 않은 아이템에 대한 평점을 예측 \\n  \\n 장점: 차원 축소를 통해 계산 효율성 증가 및 노이즈 제거 \\n 단점: 큰 데이터셋에서 계산 비용이 높아질 수 있음, 실시간 업데이트 어려움 \\n  \\n<p.18>  \\n 04 웹툰 추천 시스템 구현 - 기존 사용자  \\n BITAmin \\n 사용자가 이전에 봤던 웹툰: \\n 올가미(17회) \\n 외모지상주의(16회) \\n 이상한 변호사 우영우(7회) \\n  \\n 타입 스토리 \\n 장르 판타지, 드라마 등 \\n  \\n 웹툰 추천 결과  \\n rmse: 0.10001713 \\n  \\n<p.19>  \\n 04 웹툰 추천 시스템 구현 - 신규 사용자  \\n BITAmin \\n 좋아하는 웹툰 3개 입력: \\n 이별 후 사내 결혼 \\n 순정말고 순종 \\n 다시 쓰는 연애사 \\n  \\n 타입 스토리 \\n 장르 로맨스 등 \\n  \\n 웹툰 추천 결과  \\n rmse: 0.10001718 \\n  \\n',\n",
       " 1: '<p.2> \\n BITAmin \\n TABLE OF CONTENTS \\n 목차 소개 \\n 01 프로젝트 소개 프로젝트 배경 프로젝트 목표  \\n 02 데이터 수집 및 전처리 데이터 소스 설명 데이터 전처리 \\n 03 모델 선택 및 학습 모델 비교 최종모델 선정 \\n 04 웹툰 추천 시스템 구현 기존 사용자 신규 사용자 \\n 05 결론 및 향후 과제 프로젝트 요약 및 의의 한계점 및 향후 과제 \\n  \\n<p.7> \\n 03 모델 선택 및 학습 - CF \\n BITAmin \\n CF(Collaborative Filtering) 모델 \\n 개념  \\n 어떤 아이템에 대해서 비슷한 취향을 가진 사용자들이 다른 아이템에 대해서도 비슷한 취향을 가지고 있을 것이라고 가정하는 추천을 하는 알고리즘. \\n 사용자 간 유사도를 측정하는 User-Base, 아이템 간 유사도를 측정하는 Item-Based로 나뉨. \\n 특징 \\n 데이터 크기가 작고 각 사용자에 대한 충분한 정보(구매나 평가)가 있는 경우에는 UBCF, 데이터가 크거나 각 사용자에 대한 충분한 정보가 없는 경우에는 IBCF가 적합 \\n 장점: 직관적인 결과, 상품의 정보 없이 추천 가능 \\n 단점: Cold Start Problem, 계산량이 많아 추천의 효율성 떨어짐 \\n  \\n<p.9> \\n 03 모델 선택 및 학습 - NCF \\n BITAmin \\n NCF(Neural Collaborative Filtering) 모델 \\n 개념 \\n 사용자와 아이템을 각각 embedding vector로 표현하고 \\n 사용자-아이템 간 interaction을 바탕으로 학습. \\n interaction 할 확률 학습하여 특정 사용자에 대해 확률 높은 순서로 아이템 추천 \\n 특징 \\n 신경망을 이용하여 사용자와 아이템 간의 복잡한 비선형 상호작용을 학습 \\n 장점: 추천 알고리즘 직관성, 사용성 \\n 단점: Long tail, Data Sparsity \\n  \\n<p.17>  \\n 03  최종 모델 선정 \\n BITAmin \\n ALS(Alternative Least Squares) 모델링 및 결과 출력 \\n  \\n',\n",
       " 2: \"<p.3> \\n 01 프로젝트 소개 \\n 프로젝트 배경  \\n 정제된 '대량의 데이터'를 사용하여 모델 선택 범위를 넓히고자 함 \\n 사용자의 데이터를 효과적으로 사용할 수 있는 주제 선정 \\n 직관적으로 추천 시스템의 작동을 확인할 수 있도록  \\n 가급적 국내 최신 데이터 활용 \\n 프로젝트 목표 \\n 사용자 데이터, 웹툰 데이터를 이용해 개인화된 추천 시스템 개발 \\n 사용자가 흥미를 느낄 만한 웹툰을 정확히 추천 \\n 사용자와 아이템의 interaction 데이터에 맞는 알고리즘 \\n 탐색 및 적용 \\n  \\n<p.4> \\n 02 데이터 수집 및 전처리 \\n BITAmin \\n 데이터 소스 설명 \\n  \\n<p.5> \\n 02 데이터 수집 및 전처리 \\n BITAmin \\n 데이터 전처리  \\n  \\n<p.14> \\n 03 모델 선택 및 학습 - ALS \\n BITAmin \\n ALS(Alternative Least Squares) 모델 \\n 개념 \\n 추천 시스템에서 주로 사용되는 행렬 분해(Matrix Factorization) 기법 중 하나이며 \\n 사용자-아이템 상호작용 행렬을 두 개의 저차원 행렬로 분해하여 잠재 요인을 학습하는 방식. \\n 특징 \\n 교대 최소 제곱법(Alternative Least Squares) \\n 손실 함수 최적화(Loss Function Optimization) \\n 정규화(Regularization) \\n 대규모 데이터셋에 적합 (Scalability) \\n  \\n\",\n",
       " 3: '<p.6> \\n 02 데이터 수집 및 전처리 \\n BITAmin \\n 데이터 전처리 \\n 피봇 테이블 형식의 데이터 생성 \\n  \\n<p.10> \\n 03 모델 선택 및 학습 - NCF \\n BITAmin \\n NCF(Neural Collaborative Filtering) 모델링 및 결과 출력 \\n  \\n<p.15>  \\n 03  최종 모델 선정 \\n BITAmin \\n ALS(Alternative Least Squares) 모델 선정 이유 \\n  \\n<p.16>  \\n 03  최종 모델 선정 \\n BITAmin \\n ALS(Alternative Least Squares) 모델 선정 이유 \\n Implicit feedback의 형태로 수집된 데이터의 문제점 \\n 사용자의 호불호를 정확하게 파악하기 어려움 \\n 1. 선호와 비선호를 나타내는 binary로 분류하는 방법 \\n 2. Implicit feedback을 대상으로 하는 MF의 목적함수 \\n  \\n 정리  \\n 1. rating 값 없음 -> implicit  feeback 형태의 데이터 \\n 2. 대규모 데이터에 적합 \\n 3. RMSE 수치:0.1 \\n  \\n',\n",
       " 4: '<p.8> \\n 03 모델 선택 및 학습 - CF \\n BITAmin \\n CF(Collaborative Filtering) 모델 \\n  \\n<p.12> \\n 03 모델 선택 및 학습 - SVD \\n BITAmin \\n SVD(Singular Value Decomposition) 모델링 및 결과 출력 \\n  \\n<p.13> \\n 03 모델 선택 및 학습 - SVD \\n BITAmin \\n SVD(Singular Value Decomposition) 모델링 및 결과 출력 \\n  \\n<p.22> \\nBITAmin \\n24-1R 학기 프로젝트  \\n감사합니다 \\nRecSys Team 2 | 김나영 김나현 엄성원 이철민',\n",
       " 5: '<p.20> \\n 05 결론 및 향후 과제  \\n BITAmin \\n 프로젝트 요약 및 의의 \\n 01 결론 \\n 02 결론 \\n 03 결론 \\n CF, NCF, SVD, ALS 등 여러 가지 추천 알고리즘 모델을 구축하고 비교 분석한 결과를 토대로 최적의 모델을 선택함으로써, 사용자에게 더 나은 추천을 제공할 수 있음 \\n Explicit feedback 없이 Implicit feedback을 사용하여 ALS 모델을 구축하고, 이를 통해 최상의 결과를 도출해냄 \\n 추천 시스템을 통해 사용자의 선호에 맞는 웹툰을 제공함으로써 사용자들의 만족도를 향상시키고 플랫폼 이용률을 증가시킬 수 있음 \\n  \\n',\n",
       " 6: '<p.21> \\n 05 결론 및 향후 과제  \\n BITAmin \\n 한계점 \\n 이미 정제된 데이터를 사용한 점 \\n 데이터 내 사용자와 아이템 간의 interaction 여부만 포함되어 있고, rating 값이 없어서 사용자의 선호도를 정확하게 파악하기 어려웠음 \\n 추천 시스템은 사용자의 이전 행동을 기반으로 작동하기 때문에, interaction 데이터가 없는 신규 사용자에 대한 추천에는 적합하지 않을 수 있음 (Cold Start Problem) \\n  \\n 향후과제  \\n explicit feedback(명시적데이터) 수집을 통한 모델 성능 향상 \\n 아이템 데이터 내 genre, description 컬럼을 활용하여 모델 개선 \\n  \\n'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_dict = dict(zip(list(range(s1_nclusters)),['' for _ in range(s1_nclusters)]))\n",
    "for idx in s1_renew.index:\n",
    "    new_cluster = s1_renew.iloc[idx]['new_cluster']\n",
    "    s1_dict[new_cluster] += s1_renew.iloc[idx]['text']\n",
    "s1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3411b2e7-fc99-46b1-958c-abdd9fed9938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프로젝트 소개', ' 데이터 수집 및 전처리', ' 모델 선택 및 학습', ' 웹툰 추천 시스템 구현', ' 결론 및 향후 과제']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text_between_tag(answer1,'index')[0].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d2a85983-b2d5-4ec8-90ac-63ce73831c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_pattern = re.compile(r'<p.\\d*>')\n",
    "last_key = len(s1_dict)-1\n",
    "summaries1 = []\n",
    "s1_index_list = extract_text_between_tag(answer1,'index')[0].split(',')\n",
    "i = 0\n",
    "for key in s1_dict:\n",
    "    if key in [0,last_key]:\n",
    "        continue\n",
    "    s1_index_item = s1_index_list[i]\n",
    "    i += 1\n",
    "    ctext = s1_dict[key]\n",
    "    ctext = re.sub(r'<p.\\d*>', ' ', ctext)\n",
    "    ctext = ctext.replace('\\n', '')\n",
    "\n",
    "    messages = [\n",
    "    \t{\n",
    "    \t\"role\" : \"system\",\n",
    "    \t\"content\" : f\"\"\"Your task is to 'Summarize' the given text in triple quote. Summarize the text shortly focusing on the key points related to '{s1_index_item}'.\n",
    "         Extract the Subtiles as you can, and summarize the content for each subtitles.\n",
    "         Answer in Korean! And Answer in following format.\n",
    "\n",
    "         <main>{s1_index_item}</main\n",
    "         <subtitle>Extract subtitle</subtitle>\n",
    "         <content>Summarization result</content>         \n",
    "         \"\"\"\n",
    "       },\n",
    "    \t{\n",
    "    \t\"role\" : \"user\",\n",
    "    \t\"content\" : f'\"\"\"{ctext}\"\"\"'\n",
    "    \t}\n",
    "    ]\n",
    "    \n",
    "    summary1 = get_chat_completion(messages)\n",
    "    summaries1.append(summary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eee82b90-69f3-47d4-a1d0-ac3e98ea576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<main>프로젝트 소개</main>\n",
      "<subtitle>프로젝트 배경</subtitle>\n",
      "<content>BITAmin 프로젝트는 사용자 맞춤형 추천 시스템을 구현하기 위해 시작되었으며, 웹툰 추천에 중점을 두고 있습니다.</content>         \n",
      "<subtitle>프로젝트 목표</subtitle>\n",
      "<content>이 프로젝트의 목표는 사용자와 아이템 간의 상호작용 데이터를 분석하여 효과적인 추천 알고리즘을 개발하는 것입니다.</content>\n",
      "<main> 데이터 수집 및 전처리</main>\n",
      "<subtitle>BITAmin 데이터 소스 설명</subtitle>\n",
      "<content>프로젝트는 사용자 및 웹툰 데이터를 활용하여 개인화된 추천 시스템을 개발하는 목적을 가지고 있다. 이를 위해 국내 최신 데이터를 사용하며, 데이터 수집은 BITAmin 플랫폼을 통해 이루어진다.</content>\n",
      "<subtitle>BITAmin 데이터 전처리</subtitle>\n",
      "<content>수집된 데이터는 적절히 전처리되어야 하며, 이는 추천 알고리즘의 효과성을 높이고 사용자에게 맞춤형 추천을 제공하기 위한 중요한 단계이다. 전처리 과정에서는 데이터의 정제 및 형식 통일성이 중요하다.</content>\n",
      "<main> 모델 선택 및 학습</main>\n",
      "<subtitle>NCF(Neural Collaborative Filtering)</subtitle>\n",
      "<content>BITAmin을 사용한 NCF 모델링을 통해 결과를 출력하였다.</content>\n",
      "<subtitle>최종 모델 선정</subtitle>\n",
      "<content>ALS(Alternative Least Squares)를 최종 모델로 선정한 이유는 implicit feedback 데이터의 문제점을 해결하기 위해서이다. 사용자의 선호를 정확히 파악하기 어려운 점을 고려하여 binary 분류 방법과 implicit feedback을 위한 MF의 목적함수를 활용하였다. ALS 모델은 대규모 데이터에 적합하며 RMSE 수치가 0.1로 우수하다.</content>\n",
      "<main> 웹툰 추천 시스템 구현</main>\n",
      "<subtitle>모델 선택 및 학습 - CF</subtitle>\n",
      "<content>CF(협업 필터링) 모델을 사용하여 웹툰 추천 시스템을 구현하는 방법을 설명한다.</content>\n",
      "<subtitle>모델 선택 및 학습 - SVD</subtitle>\n",
      "<content>SVD(특이값 분해) 모델링을 통해 웹툰 추천 시스템의 효과를 분석하고 결과를 출력하는 과정에 대해 설명한다.</content>\n",
      "<subtitle>BITAmin 24-1R 학기 프로젝트</subtitle>\n",
      "<content>웹툰 추천 시스템 프로젝트를 통해 RecSys 팀의 노력과 기여를 감사하게 생각하며 프로젝트의 마무리를 알린다.</content>\n",
      "<main> 결론 및 향후 과제</main>\n",
      "<subtitle>결론</subtitle>\n",
      "<content>BITAmin 프로젝트는 여러 추천 알고리즘 모델(예: CF, NCF, SVD, ALS)을 비교 분석하여 최적의 모델을 선정함으로써 사용자에게 더 나은 추천을 제공할 수 있음을 보여줍니다. 또한, 임시적 피드백을 기반으로 한 ALS 모델을 통해 최상의 결과를 도출하고, 이를 통해 사용자 취향에 맞는 웹툰을 제공하여 만족도와 플랫폼 이용률을 향상시킬 수 있음을 강조합니다.</content>\n"
     ]
    }
   ],
   "source": [
    "for s in summaries1:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9e0563a1-e746-495f-824f-4706c84b46d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\SKH\\Github_Projects\\bitamin_auto_readme_generator\\data\\object_detection\\output\\ocr_samples_txt\n",
      "arima_text.txt\n",
      "asiancup_text.txt\n",
      "barbot_text.txt\n",
      "braintumor_text.txt\n",
      "hangang_text.txt\n",
      "interior_text.txt\n",
      "kospi_text.txt\n",
      "lier-detector_text.txt\n",
      "netflix_text.txt\n",
      "webtoon_text.txt\n"
     ]
    }
   ],
   "source": [
    "pdf = 'ocr_samples_txt'\n",
    "ROOT = 'bitamin_auto_readme_generator'\n",
    "\n",
    "root_absdir = os.getcwd().split(ROOT)[0]+ROOT\n",
    "# print(root_absdir)\n",
    "pdf_dir = os.path.join(root_absdir,'data','object_detection','output',pdf)\n",
    "print(pdf_dir)\n",
    "for f in os.listdir(pdf_dir):\n",
    "    print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
