{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "from langchain_openai import ChatOpenAI\n",
    "import transformers\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"file_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\object_detection\\\\output\\\\ocr_samples_txt\\\\webtoon_text.txt'\\n\\nwith open(file_path, 'r', encoding='utf-8') as file:\\n        text = file.read()\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''file_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\object_detection\\\\output\\\\ocr_samples_txt\\\\webtoon_text.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_filepath = \"../assets/openai_api_key.json\"\n",
    "with open(api_key_filepath, 'r') as f:\n",
    "    api_key = json.load(f)\n",
    "api_key = api_key['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_3 = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_4 = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1: 첫 5페이지에서 팀원, 제목, 목차 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_5pages(text):\n",
    "    end_marker = text.find('<p.06>')\n",
    "    if end_marker != -1:\n",
    "        text5 = text[:end_marker]\n",
    "    else:\n",
    "        text5 = text\n",
    "    return text5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(text):\n",
    "    prompt = f\"\"\"\n",
    "    Extract the following information from the provided text:\n",
    "    1. Name (excluding the team name, include all people listed)\n",
    "    2. Title (include the entire title as it appears in the text)\n",
    "    3. Main Topics (only main topics from the Table of Contents, excluding subtopics)\n",
    "\n",
    "    The Main Topics should be extracted from the section that follows headers such as 'TABLE OF CONTENTS', '목차 소개', or any similar variation. \n",
    "    Only extract the topics from the page where the 'TABLE OF CONTENTS' or equivalent header is located, and stop when you encounter the next page marker (e.g., '<p.1>', '<p.2>').\n",
    "    If the '|' character is present in the text, treat it as equivalent to a comma and replace it with a comma when outputting the main topics.\n",
    "    \n",
    "    Ensure that team member's name are not split into multiple lines. Name should be connected by commas if there are more than one.\n",
    "    Ensure that title is not cut off and is extracted in their entirety.\n",
    "    Ensure that main topics are not split into multiple lines. Main topics should be connected by commas if there are more than one.\n",
    "    Exclude any subtopics or secondary information while extracting main topics.\n",
    "\n",
    "    Ensure that the output **never** starts with <xml>\n",
    "    Do not include any additional notes or explanations in the output.\n",
    "\n",
    "    Text: {text}\n",
    "\n",
    "\n",
    "    Format the extracted information as follows:\n",
    "    <subject>title</subject>\n",
    "    <team>team members</team>\n",
    "    <index>main topics</index>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_4.invoke(prompt)\n",
    "    extracted_info = response.content.strip()\n",
    "    return extracted_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_topics(extracted_info):\n",
    "\n",
    "    main_topics_match = re.search(r'<index>(.*?)</index>', extracted_info, re.DOTALL)\n",
    "    if main_topics_match:\n",
    "        main_topics = main_topics_match.group(1).strip()\n",
    "        main_topics_list = [topic.strip() for topic in main_topics.split(',')]\n",
    "    else:\n",
    "        main_topics_list = []\n",
    "\n",
    "    return main_topics_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2: 전체 목차를 4등분한 후 각각의 변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_topics(main_topics_list):\n",
    "    num_topics = len(main_topics_list)\n",
    "    \n",
    "    base_size = num_topics // 4\n",
    "    remainder = num_topics % 4\n",
    "    \n",
    "    topics_list1 = []\n",
    "    topics_list2 = []\n",
    "    topics_list3 = []\n",
    "    topics_list4 = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    \n",
    "    sizes = [base_size + (1 if i < remainder else 0) for i in range(4)]\n",
    "    \n",
    "    topics_list1 = main_topics_list[start_idx:start_idx + sizes[0]]\n",
    "    start_idx += sizes[0]\n",
    "    \n",
    "    topics_list2 = main_topics_list[start_idx:start_idx + sizes[1]]\n",
    "    start_idx += sizes[1]\n",
    "    \n",
    "    topics_list3 = main_topics_list[start_idx:start_idx + sizes[2]]\n",
    "    start_idx += sizes[2]\n",
    "    \n",
    "    topics_list4 = main_topics_list[start_idx:start_idx + sizes[3]]\n",
    "    \n",
    "    return topics_list1, topics_list2, topics_list3, topics_list4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3: 전체 텍스트에서 각 파트에 따른 텍스트 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_by_topic(text, topics_list):\n",
    "    \"\"\"\n",
    "    Extract text from the provided text that corresponds to the specified topics in the list.\n",
    "    \"\"\"\n",
    "    topics_str = ', '.join(topics_list)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Extract the following sections from the provided text:\n",
    "    Topics: {topics_str}\n",
    "\n",
    "    Only include the text that corresponds to the specified topics. Do not include any unrelated sections or subtopics.\n",
    "    Ensure that all text related to these topics is included without any omissions.\n",
    "    Make sure to **include page markers** such as <p.01>, <p.02>, etc., in the extracted text without fail.\n",
    "    Do not omit any content, especially page markers, or any part of the text directly related to the topics.\n",
    "\n",
    "\n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_4.invoke(prompt)\n",
    "    extracted_info = response.content.strip()\n",
    "    extracted_info = extracted_info.replace(\"```\", \"\").strip(\"'\")\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "\n",
    "def main(text, topics_list1, topics_list2, topics_list3, topics_list4):\n",
    "    topic_text1 = extract_info_by_topic(text, topics_list1) if topics_list1 else \"no text\"\n",
    "    topic_text2 = extract_info_by_topic(text, topics_list2) if topics_list2 else \"no text\"\n",
    "    topic_text3 = extract_info_by_topic(text, topics_list3) if topics_list3 else \"no text\"\n",
    "    topic_text4 = extract_info_by_topic(text, topics_list4) if topics_list4 else \"no text\"\n",
    "    \n",
    "    return topic_text1, topic_text2, topic_text3, topic_text4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4: 각 파트별 요약 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, topics_list):\n",
    "    prompt = f\"\"\"\n",
    "    Do not change the language of the original input text. \n",
    "    Summarize in the same language as the input text.\n",
    "    The purpose of the summary is to create a comprehensive README for a GitHub project.\n",
    "    If multiple subtopics within the same main topic have the same name, combine their content into a single entry with all relevant details, and merge the page numbers into a single list. \n",
    "    Ensure that subtopics from different main topics are not merged.\n",
    "\n",
    "    Ensure that the summary accurately captures the key points and essential information of each topic.\n",
    "    Summarize in a way that is concise and informative, omitting repetitive mentions of the main topic after the first mention.\n",
    "    Focus on the key details and results relevant to the project, with particular emphasis on the final model outcomes. \n",
    "    Ensure that the final model results are described with a high level of accuracy and detail, including specific metrics, performance evaluations, and any significant findings.\n",
    "    For each subtopic, aim to provide a summary that is 1 to 2 sentences long. Focus on delivering a concise yet comprehensive overview, capturing the main points and essential details without unnecessary elaboration.\n",
    "    When summarizing, only include the **first page** where each subtopic begins in the \"pages used\" list. For example, if a subtopic spans from page 7 to page 10, only include page 7 in the summary.\n",
    "\n",
    "    Use the following main topic to guide the summary:\n",
    "    {topics_list}\n",
    "    \n",
    "    The summary should be in the given format without requiring any specific markdown style or additional formatting.\n",
    "    The summary should follow this format:\n",
    "    Main topic\n",
    "    - subtopic1: detailed contents (pages used: first page only)\n",
    "    - subtopic2: detailed contents (pages used: first page only)\n",
    "        \n",
    "    Here is an example of a well-structured summary:\n",
    "    프로젝트 소개\n",
    "    - 프로젝트 배경: 정제된 '대량의 데이터'를 사용하여 모델 선택 범위를 넓히고자 함. (3)\n",
    "    - 프로젝트 목표: 사용자 데이터, 웹툰 데이터를 이용해 개인화된 추천 시스템 개발. 사용자와 아이템의 interaction 데이터에 맞는 알고리즘 탐색 및 적용. (7)\n",
    "    \n",
    "    데이터 수집 및 전처리\n",
    "    - 데이터 소스 설명: 다양한 데이터 소스 사용. 데이터 전처리 과정 설명. (11)\n",
    "    - 데이터 처리: 피봇 테이블 형식의 데이터 생성, 데이터 정제 및 전처리. (13)\n",
    "\n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_4.invoke(prompt)\n",
    "    result = response.content.strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_summaries(topic_text1, topic_text2, topic_text3, topic_text4, topics_list1, topics_list2, topics_list3, topics_list4):\n",
    "\n",
    "    topic1_summ = summarize(topic_text1, topics_list1) if topic_text1 != \"no text\" else \"\"\n",
    "    topic2_summ = summarize(topic_text2, topics_list2) if topic_text2 != \"no text\" else \"\"\n",
    "    topic3_summ = summarize(topic_text3, topics_list3) if topic_text3 != \"no text\" else \"\"\n",
    "    topic4_summ = summarize(topic_text4, topics_list4) if topic_text4 != \"no text\" else \"\"\n",
    "\n",
    "    return topic1_summ, topic2_summ, topic3_summ, topic4_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP5: 각 파트별 요약본 merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_summ(*args):\n",
    "    return '\\n'.join(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP6: 마크다운 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text):\n",
    "    prompt = f\"\"\"\n",
    "    Tag each part of the text according to the following rules:\n",
    "    - The first line before any \"-\" should be tagged as <main>.\n",
    "    - Each line starting with \"-\" should have the part before \":\" tagged as <sub> and the part after \":\" tagged as <content>.\n",
    "    - Remove \"-\", \"=\", and \":\" characters from the text.\n",
    "    - After each <content> tag, add a <page> tag that includes the relevant page numbers extracted from the text (e.g., <p.1>, <p.2>).\n",
    "    - Ensure the tags are correctly closed and formatted.\n",
    "    Ensure that the output **never** starts with <xml> or markdown\n",
    "\n",
    "    Here is an example of how the tagged text should look:\n",
    "\n",
    "    <main>대한민국의 계절</main>\n",
    "    <sub>여름</sub> <content>요즘 한국의 여름은 매우 덥고 습한 편이다.</content> <page>1</page>\n",
    "    <sub>겨울</sub> <content>12월부터 온도가 급격히 떨어지며 건조해진다.</content> <page>4</page>\n",
    "\n",
    "    <main>대한민국의 휴일</main>\n",
    "    <sub>추석</sub> <content>추석은 비교적 휴일 기간이 긴 편이다.</content> <page>6</page>\n",
    "    <sub>설날</sub> <content>설날에는 떡국을 먹으며, 가족들과 인사를 나눈다.</content> <page>7</page>\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_4.invoke(prompt)\n",
    "    result = response.content.strip()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\arima_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\asiancup_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\barbot_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\blind_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\braintumor_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\cartoon_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\disease_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\energy_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\hangang_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\insideout_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\interior_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\kospi_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\lier-detector_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\netflix_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\restaurant_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\trading_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\var_text.txt'.\n",
      "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method4\\webtoon_text.txt'.\n"
     ]
    }
   ],
   "source": [
    "input_directory = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\object_detection\\\\output\\\\ocr_samples_txt'\n",
    "\n",
    "output_directory = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\text_summarization\\\\output\\\\method4'\n",
    "\n",
    "file_list = os.listdir(input_directory)\n",
    "\n",
    "for file_name in file_list:\n",
    "    if file_name.endswith('_text.txt'):  \n",
    "        file_path = os.path.join(input_directory, file_name)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        text5 = extract_5pages(text)\n",
    "        extracted_info = extract_info(text5)\n",
    "        main_topics_list = extract_main_topics(extracted_info)\n",
    "        topics_list1, topics_list2, topics_list3, topics_list4 = split_topics(main_topics_list)\n",
    "        topic_text1, topic_text2, topic_text3, topic_text4 = main(text, topics_list1, topics_list2, topics_list3, topics_list4)\n",
    "        topic1_summ, topic2_summ, topic3_summ, topic4_summ = generate_summaries(topic_text1, topic_text2, topic_text3, topic_text4, topics_list1, topics_list2, topics_list3, topics_list4)\n",
    "        combined_summ = concat_summ(topic1_summ, topic2_summ, topic3_summ, topic4_summ)\n",
    "        result = tag_text(combined_summ)\n",
    "        final_text = f\"{extracted_info}\\n\\n{result}\"\n",
    "\n",
    "\n",
    "        base_name = file_name.split('_text')[0]\n",
    "\n",
    "        output_file_path = os.path.join(output_directory, f'{base_name}_text.txt')\n",
    "\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(final_text)\n",
    "\n",
    "        print(f\"Summarized text has been saved to '{output_file_path}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
