{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "import os\n",
        "import re\n",
        "from langchain_openai import ChatOpenAI\n",
        "import transformers\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"file_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\object_detection\\\\output\\\\ocr_samples_txt\\\\webtoon_text.txt'\\n\\nwith open(file_path, 'r', encoding='utf-8') as file:\\n        text = file.read()\""
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''file_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\object_detection\\\\output\\\\ocr_samples_txt\\\\webtoon_text.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key_filepath = \"C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\비타민NLP_240701\\\\text_summarization\\\\openai_api_key.json\"\n",
        "with open(api_key_filepath, 'r') as f:\n",
        "    api_key = json.load(f)\n",
        "api_key = api_key['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_3 = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_4 = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STEP1: 첫 5페이지에서 팀원, 제목, 목차 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_info(text, marker='<p.6>'):\n",
        "    end_marker = text.find(marker)\n",
        "\n",
        "    if end_marker != -1:\n",
        "        text5 = text[:end_marker]\n",
        "    else:\n",
        "        text5 = text\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following information from the provided text:\n",
        "    1. Name (excluding the team name, include all people listed)\n",
        "    2. Title (include the entire title as it appears in the text)\n",
        "    3. Main Topics (only main topics from the Table of Contents, excluding subtopics)\n",
        "\n",
        "    The Main Topics should be extracted from the section that follows headers such as 'TABLE OF CONTENTS', '목차 소개', or any similar variation. \n",
        "    Only extract the topics from the page where the 'TABLE OF CONTENTS' or equivalent header is located, and stop when you encounter the next page marker (e.g., '<p.1>', '<p.2>').\n",
        "    If the '|' character is present in the text, treat it as equivalent to a comma and replace it with a comma when outputting the main topics.\n",
        "    \n",
        "    Ensure that team member's name are not split into multiple lines. Name should be connected by commas if there are more than one.\n",
        "    Ensure that title is not cut off and is extracted in their entirety.\n",
        "    Ensure that main topics are not split into multiple lines. Main topics should be connected by commas if there are more than one.\n",
        "    Exclude any subtopics or secondary information while extracting main topics.\n",
        "\n",
        "    Ensure that the output **never** starts with <xml>\n",
        "    Do not include any additional notes or explanations in the output.\n",
        "\n",
        "    Text: {text5}\n",
        "\n",
        "\n",
        "    Format the extracted information as follows:\n",
        "    <subject>title</subject>\n",
        "    <team>team members</team>\n",
        "    <index>main topics</index>\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_4.invoke(prompt)\n",
        "    extracted_info = response.content.strip()\n",
        "    return extracted_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_main_topics(extracted_info):\n",
        "\n",
        "    main_topics_match = re.search(r'<index>(.*?)</index>', extracted_info, re.DOTALL)\n",
        "    if main_topics_match:\n",
        "        main_topics = main_topics_match.group(1).strip()\n",
        "        main_topics_list = [topic.strip() for topic in main_topics.split(',')]\n",
        "    else:\n",
        "        main_topics_list = []\n",
        "\n",
        "    return main_topics_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STEP2: 전체 텍스트를 대주제 개수에 따라 구분"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def divide_text(extracted_info, main_topics_list, text):\n",
        "    prompt = f\"\"\"\n",
        "    The provided text contains specific sections that need to be excluded before dividing it according to the main topics.\n",
        "    Exclude the following sections from the text:\n",
        "    {extracted_info}\n",
        "\n",
        "    After exclusion, divide the provided text according to the main topics.\n",
        "\n",
        "    The text should be divided into exactly {len(main_topics_list)} sections, separated by \"===================================\".\n",
        "\n",
        "    Use the main topics provided below as the indices for division:\n",
        "    {main_topics_list}\n",
        "\n",
        "    The text should remain unmodified.\n",
        "    The text should only be partitioned according to the number of main topics.\n",
        "\n",
        "    Text: {text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_4.invoke(prompt)\n",
        "    divided_text = response.content.strip()\n",
        "    return divided_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STEP3: 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize(text, main_topics_list):\n",
        "    prompt = f\"\"\"\n",
        "    The provided text is divided into several partitions separated by \"===================================\".\n",
        "    Summarize each partition separately. \n",
        "    Do not change the language of the original input text. \n",
        "    Summarize in the same language as the input text.\n",
        "    The purpose of the summaries is to create a comprehensive README for a GitHub project.\n",
        "    If multiple subtopics within the same main topic have the same name, combine their content into a single entry with all relevant details, and merge the page numbers into a single list. \n",
        "    Ensure that subtopics from different main topics are not merged.\n",
        "\n",
        "    Ensure that each summary accurately captures the key points and essential information of its respective partition.\n",
        "    Summarize in a way that is concise and informative, omitting repetitive mentions of the main topic after the first mention in each partition.\n",
        "    Focus on the key details and results relevant to the project, with particular emphasis on the final model outcomes. \n",
        "    Ensure that the final model results are described with a high level of accuracy and detail, including specific metrics, performance evaluations, and any significant findings.\n",
        "    For each subtopic, aim to provide a summary that is 1 to 2 sentences long. Focus on delivering a concise yet comprehensive overview, capturing the main points and essential details without unnecessary elaboration.\n",
        "    When summarizing, only include the **first page** where each subtopic begins in the \"pages used\" list. For example, if a subtopic spans from page 7 to page 10, only include page 7 in the summary.\n",
        "\n",
        "    Do not remove or modify the \"===================================\" separators.\n",
        "    Use the following main topics to guide the summaries:\n",
        "    {main_topics_list}\n",
        "    \n",
        "    The summaries should follow this format:\n",
        "    ===================================\n",
        "    Main topic\n",
        "    - subtopic1: detailed contents (pages used: first page only)\n",
        "    - subtopic2: detailed contents (pages used: first page only)\n",
        "    ===================================\n",
        "    Main topic\n",
        "    - subtopic1: detailed contents (pages used: first page only)\n",
        "    - subtopic2: detailed contents (pages used: first page only)\n",
        "    ===================================\n",
        "\n",
        "    Here is an example of a well-structured summary:\n",
        "\n",
        "    ===================================\n",
        "    프로젝트 소개\n",
        "    - 프로젝트 배경: 정제된 '대량의 데이터'를 사용하여 모델 선택 범위를 넓히고자 함. (3)\n",
        "    - 프로젝트 목표: 사용자 데이터, 웹툰 데이터를 이용해 개인화된 추천 시스템 개발. 사용자와 아이템의 interaction 데이터에 맞는 알고리즘 탐색 및 적용. (7)\n",
        "    ===================================\n",
        "    데이터 수집 및 전처리\n",
        "    - 데이터 소스 설명: 다양한 데이터 소스 사용. 데이터 전처리 과정 설명. (11)\n",
        "    - 데이터 처리: 피봇 테이블 형식의 데이터 생성, 데이터 정제 및 전처리. (13)\n",
        "    ===================================\n",
        "\n",
        "    Text: {text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_4.invoke(prompt)\n",
        "    result = response.content.strip()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STEP4: 마크다운 형식으로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tag_text(text):\n",
        "    prompt = f\"\"\"\n",
        "    Tag each part of the text according to the following rules:\n",
        "    - The first line before any \"-\" should be tagged as <main>.\n",
        "    - Each line starting with \"-\" should have the part before \":\" tagged as <sub> and the part after \":\" tagged as <content>.\n",
        "    - Remove \"-\", \"=\", and \":\" characters from the text.\n",
        "    - After each <content> tag, add a <page> tag that includes the relevant page numbers extracted from the text (e.g., <p.1>, <p.2>).\n",
        "    - Ensure the tags are correctly closed and formatted.\n",
        "    Ensure that the output **never** starts with <xml> or markdown\n",
        "\n",
        "    Here is an example of how the tagged text should look:\n",
        "\n",
        "    <main>대한민국의 계절</main>\n",
        "    <sub>여름</sub> <content>요즘 한국의 여름은 매우 덥고 습한 편이다.</content> <page>1</page>\n",
        "    <sub>겨울</sub> <content>12월부터 온도가 급격히 떨어지며 건조해진다.</content> <page>4</page>\n",
        "\n",
        "    <main>대한민국의 휴일</main>\n",
        "    <sub>추석</sub> <content>추석은 비교적 휴일 기간이 긴 편이다.</content> <page>6</page>\n",
        "    <sub>설날</sub> <content>설날에는 떡국을 먹으며, 가족들과 인사를 나눈다.</content> <page>7</page>\n",
        "\n",
        "    Text:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "\n",
        "    #temperature 설정\n",
        "    response = llm_4.invoke(prompt)\n",
        "    result = response.content.strip()\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\arima_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\asiancup_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\barbot_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\blind_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\braintumor_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\cartoon_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\disease_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\energy_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\hangang_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\insideout_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\interior_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\kospi_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\lier-detector_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\netflix_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\restaurant_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\trading_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\var_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method3\\webtoon_text.txt'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "input_directory = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\object_detection\\\\output\\\\ocr_samples_txt'\n",
        "\n",
        "output_directory = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\text_summarization\\\\output\\\\method3'\n",
        "\n",
        "file_list = os.listdir(input_directory)\n",
        "\n",
        "for file_name in file_list:\n",
        "    if file_name.endswith('_text.txt'):  \n",
        "        file_path = os.path.join(input_directory, file_name)\n",
        "        \n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        extracted_info = extract_info(text)  \n",
        "        main_topics_list = extract_main_topics(extracted_info)\n",
        "        divided_text = divide_text(extracted_info, main_topics_list, text)\n",
        "        summarized_text = summarize(divided_text, main_topics_list)\n",
        "        result = tag_text(summarized_text)\n",
        "        final_text = f\"{extracted_info}\\n\\n{result}\"\n",
        "\n",
        "\n",
        "        base_name = file_name.split('_text')[0]\n",
        "\n",
        "        output_file_path = os.path.join(output_directory, f'{base_name}_text.txt')\n",
        "\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.write(final_text)\n",
        "\n",
        "        print(f\"Summarized text has been saved to '{output_file_path}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
