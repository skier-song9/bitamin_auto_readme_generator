{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7661fd00-51c5-47e4-93d4-2a2cdfb17618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import glob,os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eed3d9bb-0072-4785-bc17-eee15c616d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_pages(filepath, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    filepath : ocr 결과 txt 파일 경로 전달\n",
    "\n",
    "    return : page별로 구분된 하나의 리스트를 반환\n",
    "    \"\"\"\n",
    "    with open(filepath,'r',encoding=encoding) as f:\n",
    "        data = f.readlines()\n",
    "    text = []\n",
    "    page_text = []\n",
    "    for d in data:\n",
    "        if re.compile(PAGE_PATTERN).match(d):\n",
    "            if len(page_text)>0:\n",
    "                text.append(' '.join(page_text))\n",
    "            page_text = []\n",
    "        page_text.append(d)\n",
    "    text.append(''.join(page_text))\n",
    "    return text\n",
    "\n",
    "def erase_tag(text, tag):\n",
    "    \"\"\"\n",
    "    text : split_by_page로 얻은 텍스트 리스트 또는 텍스트\n",
    "    tag : 지우고 싶은 <tag>\n",
    "    \"\"\"\n",
    "    tag_pattern = re.compile(f'<{tag}>|</{tag}>')\n",
    "    if isinstance(text, list):\n",
    "        text_ = [tag_pattern.sub('',x) for x in text]\n",
    "        return text_\n",
    "    else:\n",
    "        text_ = tag_pattern.sub('',text)\n",
    "        return text_\n",
    "\n",
    "def extract_text_between_tag(text, tag):\n",
    "    \"\"\"\n",
    "    text : split_by_page로 얻은 텍스트 리스트\n",
    "    tag : <tag> 사이의 텍스트를 추출\n",
    "    \"\"\"\n",
    "    # Create a regex pattern for the specified tag\n",
    "    pattern = f'<{tag}>(.*?)</{tag}>'\n",
    "    # Use re.findall to extract all occurrences between the specified tags\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "def extract_numbers_from_string(text):\n",
    "    # \\d+ 는 하나 이상의 숫자에 매치됨\n",
    "    return re.findall(r'\\d+', text)\n",
    "\n",
    "def remove_xml_tags(text, tag):\n",
    "    \"\"\"\n",
    "    주어진 문자열에서 XML 태그와 그 사이의 내용을 제거합니다.\n",
    "    \n",
    "    :param text: 태그를 제거할 입력 문자열\n",
    "    :return: XML 태그가 제거된 문자열\n",
    "    \"\"\"\n",
    "    # <와 > 사이에 있는 내용을 제거합니다.\n",
    "    # 정규 표현식 패턴: <[^>]+>.*?</[^>]+>\n",
    "    return re.sub(rf'<{tag}>.*?</{tag}>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6bf2b9c1-af8e-44d0-a5b1-9757abd5277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT():\n",
    "    __classname__ = \"OpenAI\"\n",
    "    api_key = ''\n",
    "    client = None    \n",
    "    def __init__(self, api_filepath):\n",
    "        with open(api_filepath,'r') as f:\n",
    "            ak = json.load(f)\n",
    "        self.api_key = ak['OPENAI_API_KEY']\n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "        self.EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions very carefully. \n",
    "Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{document}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Evaluation Form : 'INTEGER SCORE ONLY'\n",
    "\n",
    "\"\"\"\n",
    "# - {metric_name}\n",
    "        self.RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "Relevance(1-5) - selection of important content from the source. \\\n",
    "The summary should include only important information from the source document. \\\n",
    "Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
    "\"\"\"\n",
    "        self.RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\"\"\"\n",
    "        self.COHERENCE_SCORE_CRITERIA = \"\"\"\n",
    "Coherence(1-5) - the collective quality of all sentences. \\\n",
    "We align this dimension with the DUC quality question of structure and coherence \\\n",
    "whereby \"the summary should be well-structured and well-organized. \\\n",
    "The summary should not just be a heap of related information, but should build from sentence to a\\\n",
    "coherent body of information about a topic.\"\n",
    "\"\"\"\n",
    "        self.COHERENCE_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main topic and key points.\n",
    "2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\n",
    "and if it presents them in a clear and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "        self.CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
    "Consistency(1-5) - the factual alignment between the summary and the summarized source. \\\n",
    "A factually consistent summary contains only statements that are entailed by the source document. \\\n",
    "Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
    "\"\"\"\n",
    "        self.CONSISTENCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main facts and details it presents.\n",
    "2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\n",
    "3. Assign a score for consistency based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "        self.FLUENCY_SCORE_CRITERIA = \"\"\"\n",
    "Fluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
    "1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n",
    "2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "3: Good. The summary has few or no errors and is easy to read and follow.\n",
    "\"\"\"\n",
    "        self.FLUENCY_SCORE_STEPS = \"\"\"\n",
    "Read the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 3.\n",
    "\"\"\"\n",
    "        \n",
    "    def get_chat_completion(self, msg, model='gpt-4o-mini', temperature = 0):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model = model,\n",
    "            messages = msg,\n",
    "            temperature = temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "        # return response\n",
    "\n",
    "    def get_embedding(self, sentence, model=\"text-embedding-3-small\"):\n",
    "       '''\n",
    "       - pricing : text-embedding-3-small = $0.02/1M tokens\n",
    "           텍스트가 많은 pdf는 대략 5,000 tokens -> pdf 200개에 0.02 달러(25~30원).\n",
    "       text : 한 문장\n",
    "       return : 한 문장에 대한 embedding (output dimension = 1536)\n",
    "       '''\n",
    "       return self.client.embeddings.create(input = sentence, model=model).data[0].embedding\n",
    "\n",
    "    def get_geval_score(\n",
    "        self, document: str, summary: str, model: str = 'gpt-4o-mini', n_sampling: int = 20\n",
    "    ):\n",
    "        '''\n",
    "        document : 원본 문서\n",
    "        summary : 요약 텍스트\n",
    "\n",
    "        return : 요약 텍스트에 대한 relevance, coherence, consistency, fluency G-EVAL 점수\n",
    "        '''\n",
    "        evaluation_metrics = {\n",
    "            \"Relevance\": (self.RELEVANCY_SCORE_CRITERIA, self.RELEVANCY_SCORE_STEPS),\n",
    "            \"Coherence\": (self.COHERENCE_SCORE_CRITERIA, self.COHERENCE_SCORE_STEPS),\n",
    "            \"Consistency\": (self.CONSISTENCY_SCORE_CRITERIA, self.CONSISTENCY_SCORE_STEPS),\n",
    "            \"Fluency\": (self.FLUENCY_SCORE_CRITERIA, self.FLUENCY_SCORE_STEPS)\n",
    "        }\n",
    "        scores = []\n",
    "        for evaluation_type, (criteria, steps) in evaluation_metrics.items():\n",
    "            prompt = self.EVALUATION_PROMPT_TEMPLATE.format(\n",
    "                criteria=criteria,\n",
    "                steps=steps,\n",
    "                metric_name=evaluation_type,\n",
    "                document=document,\n",
    "                summary=summary\n",
    "            )\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=1,\n",
    "                max_tokens=5,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                n = n_sampling\n",
    "            )\n",
    "            # 논문에서는 GPT-4의 parameter를 다음과 같이 설정 : n = 20, temperature = 1, top_p = 1\n",
    "            score = 0\n",
    "\n",
    "            # manually sampling\n",
    "            # for sample in range(n_sampling):\n",
    "            #     res = response.choices[0].message.content.strip()\n",
    "            #     print(res)\n",
    "            #     numlist = extract_numbers_from_string(res)\n",
    "            #     if len(numlist) == 0 : # 점수가 안 나오는 error인 경우\n",
    "            #         n_sampling -= 1 # 정규화를 위한 n_sampling 1 줄여주기\n",
    "            #         continue\n",
    "            #     score += int(numlist[0])\n",
    "\n",
    "            # when using n parameter\n",
    "            for res in response.choices:\n",
    "                res = res.message.content.strip()\n",
    "                print(res)\n",
    "                numlist = extract_numbers_from_string(res)\n",
    "                if len(numlist) == 0 : # 점수가 안 나오는 error인 경우\n",
    "                    n_sampling -= 1 # 정규화를 위한 n_sampling 1 줄여주기\n",
    "                    continue\n",
    "                score += int(numlist[0])\n",
    "            score = score / (n_sampling if n_sampling != 0 else 1)\n",
    "            scores.append(score)\n",
    "            # scores.append(int(response.choices[0].message.content.strip()))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b373d4e-9979-4249-99af-5d971ea86389",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(api_filepath='../assets/openai_api_key.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7986b507-9a77-42e1-b709-23dc7b31a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9uZIUkZfsm6U7BjedXpNTdxGNDgrx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Sure! Here’s a random integer between 1 and 5: **3**.', role='assistant', function_call=None, tool_calls=None, refusal=None)), Choice(finish_reason='stop', index=1, logprobs=None, message=ChatCompletionMessage(content='Sure! Here’s a random integer between 1 and 5: **3**.', role='assistant', function_call=None, tool_calls=None, refusal=None)), Choice(finish_reason='stop', index=2, logprobs=None, message=ChatCompletionMessage(content='Sure! Here’s a random integer between 1 and 5: **3**.', role='assistant', function_call=None, tool_calls=None, refusal=None)), Choice(finish_reason='stop', index=3, logprobs=None, message=ChatCompletionMessage(content='Sure! Here’s a random integer between 1 and 5: **3**.', role='assistant', function_call=None, tool_calls=None, refusal=None)), Choice(finish_reason='stop', index=4, logprobs=None, message=ChatCompletionMessage(content='Sure! Here’s a random integer between 1 and 5: **3**.', role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1723269078, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_507c9469a1', usage=CompletionUsage(completion_tokens=90, prompt_tokens=17, total_tokens=107))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.get_chat_completion(msg=[{'role':'user','content':'Answer random integer number between 1 to 5'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ef9e7a3e-800c-448e-bc94-145945120a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arima_text.txt\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "asiancup_text.txt\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "barbot_text.txt\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "blind_text.txt\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "braintumor_text.txt\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "disease_text.txt\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "energy_text.txt\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "hangang_text.txt\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "insideout_text.txt\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "interior_text.txt\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "kospi_text.txt\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "lier-detector_text.txt\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "Score: 4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "netflix_text.txt\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "restaurant_text.txt\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "webtoon_text.txt\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Load Documnets and summaries\n",
    "ROOT = 'bitamin_auto_readme_generator'\n",
    "root_absdir = os.getcwd().split(ROOT)[0]+ROOT\n",
    "\n",
    "doc_dir = os.path.join(root_absdir,'data','object_detection','output','ocr_samples_txt')\n",
    "doc_files = os.listdir(doc_dir)\n",
    "\n",
    "summ_dir = os.path.join(root_absdir,'data','text_summarization','output','cluster_n_summary_temp1')\n",
    "\n",
    "scores_df = pd.DataFrame(columns=['pdf','relevance','coherence','consistency','fluency','summ_length','doc_summ_length_ratio'])\n",
    "for filename in doc_files:\n",
    "    print(filename)\n",
    "    with open(os.path.join(doc_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        document = f.readlines()\n",
    "    if os.path.exists(os.path.join(summ_dir, filename)):\n",
    "        with open(os.path.join(summ_dir, filename), 'r', encoding = 'utf-8') as f:\n",
    "            summary = f.readlines()\n",
    "    else:\n",
    "        print(\"⚠️Summary doesn't exist\")\n",
    "        break\n",
    "\n",
    "    # preprocess\n",
    "    document = ''.join(document)\n",
    "    document = erase_tag(document, 'p.\\d*')\n",
    "    # document = document.replace('\\n','')\n",
    "    summary = ''.join(summary)\n",
    "    for tag in ['subject','team','index']:\n",
    "        summary = remove_xml_tags(summary, tag)\n",
    "    summary = erase_tag(summary,'[^>]+')    \n",
    "\n",
    "    # Evaluate\n",
    "    rel, coh, cons, flu =gpt.get_geval_score(document, summary, model='gpt-4o-mini')\n",
    "\n",
    "    new_row = pd.DataFrame(data=[[filename.replace('.txt',''),rel,coh,cons,flu,len(summary),round(len(summary)/len(document)*100,2)]],\n",
    "                          columns=scores_df.columns)\n",
    "    scores_df = pd.concat([scores_df, new_row], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9dc24c82-7fde-4b19-a6f8-83d6ea0606bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf</th>\n",
       "      <th>relevance</th>\n",
       "      <th>coherence</th>\n",
       "      <th>consistency</th>\n",
       "      <th>fluency</th>\n",
       "      <th>summ_length</th>\n",
       "      <th>doc_summ_length_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arima_text</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2182</td>\n",
       "      <td>73.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asiancup_text</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2090</td>\n",
       "      <td>70.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>barbot_text</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2767</td>\n",
       "      <td>47.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blind_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.15</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1276</td>\n",
       "      <td>51.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>braintumor_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1331</td>\n",
       "      <td>98.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disease_text</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1654</td>\n",
       "      <td>44.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>energy_text</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1407</td>\n",
       "      <td>19.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hangang_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.95</td>\n",
       "      <td>4.15</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1789</td>\n",
       "      <td>86.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insideout_text</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.40</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1530</td>\n",
       "      <td>33.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interior_text</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1235</td>\n",
       "      <td>69.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kospi_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1866</td>\n",
       "      <td>29.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lier-detector_text</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2252</td>\n",
       "      <td>21.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>netflix_text</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1427</td>\n",
       "      <td>16.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>restaurant_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1850</td>\n",
       "      <td>46.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>webtoon_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.25</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3524</td>\n",
       "      <td>76.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pdf  relevance  coherence  consistency  fluency summ_length  \\\n",
       "0          arima_text       4.20       4.00         4.10     3.00        2182   \n",
       "0       asiancup_text       4.05       4.05         4.10     2.95        2090   \n",
       "0         barbot_text       4.15       4.05         4.20     2.95        2767   \n",
       "0          blind_text       4.00       4.00         4.15     3.00        1276   \n",
       "0     braintumor_text       4.10       4.00         4.05     2.65        1331   \n",
       "0        disease_text       4.15       4.00         4.35     2.05        1654   \n",
       "0         energy_text       4.25       4.00         4.30     3.00        1407   \n",
       "0        hangang_text       4.10       3.95         4.15     2.65        1789   \n",
       "0      insideout_text       4.25       4.05         4.40     2.95        1530   \n",
       "0       interior_text       4.45       4.10         4.20     3.00        1235   \n",
       "0          kospi_text       4.00       4.00         4.05     2.10        1866   \n",
       "0  lier-detector_text       4.05       4.00         4.05     2.55        2252   \n",
       "0        netflix_text       4.25       4.00         4.35     3.00        1427   \n",
       "0     restaurant_text       4.00       4.00         4.00     2.90        1850   \n",
       "0        webtoon_text       4.10       4.00         4.25     2.90        3524   \n",
       "\n",
       "   doc_summ_length_ratio  \n",
       "0                  73.74  \n",
       "0                  70.82  \n",
       "0                  47.59  \n",
       "0                  51.79  \n",
       "0                  98.23  \n",
       "0                  44.21  \n",
       "0                  19.47  \n",
       "0                  86.63  \n",
       "0                  33.56  \n",
       "0                  69.23  \n",
       "0                  29.54  \n",
       "0                  21.38  \n",
       "0                  16.92  \n",
       "0                  46.66  \n",
       "0                  76.39  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90758f3-9197-4a2b-b2c8-fdf9cf5f1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documnets and summaries\n",
    "ROOT = 'bitamin_auto_readme_generator'\n",
    "root_absdir = os.getcwd().split(ROOT)[0]+ROOT\n",
    "\n",
    "doc_dir = os.path.join(root_absdir,'data','object_detection','output','ocr_samples_txt')\n",
    "doc_files = os.listdir(doc_dir)\n",
    "\n",
    "summ_dir = os.path.join(root_absdir,'data','text_summarization','output','cluster_n_summary_temp0')\n",
    "\n",
    "scores_df = pd.DataFrame(columns=['pdf','relevance','coherence','consistency','fluency','summ_length','doc_summ_length_ratio'])\n",
    "for filename in doc_files:\n",
    "    with open(os.path.join(doc_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        document = f.readlines()\n",
    "    if os.path.exists(os.path.join(summ_dir, filename)):\n",
    "        with open(os.path.join(summ_dir, filename), 'r', encoding = 'utf-8') as f:\n",
    "            summary = f.readlines()\n",
    "    else:\n",
    "        print(\"⚠️Summary doesn't exist\")\n",
    "        break\n",
    "\n",
    "    # preprocess\n",
    "    document = ''.join(document)\n",
    "    document = erase_tag(document, 'p.\\d*')\n",
    "    # document = document.replace('\\n','')\n",
    "    summary = ''.join(summary)\n",
    "    for tag in ['subject','team','index']:\n",
    "        summary = remove_xml_tags(summary, tag)\n",
    "    \n",
    "\n",
    "    # Evaluate\n",
    "    rel, coh, cons, flu =gpt.get_geval_score(document, summary, model='gpt-4o-mini')\n",
    "\n",
    "    new_row = pd.DataFrame(data=[[filename.replace('.txt',''),rel,coh,cons,flu,len(summary),round(len(summary)/len(document)*100,2)]],\n",
    "                          columns=scores_df.columns)\n",
    "    scores_df = pd.concat([scores_df, new_row], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deefb417-147c-4399-be55-63f45ef7a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de69e5-8cd7-45a5-8927-5236b7c595c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documnets and summaries\n",
    "ROOT = 'bitamin_auto_readme_generator'\n",
    "root_absdir = os.getcwd().split(ROOT)[0]+ROOT\n",
    "\n",
    "doc_dir = os.path.join(root_absdir,'data','object_detection','output','ocr_samples_txt')\n",
    "doc_files = os.listdir(doc_dir)\n",
    "\n",
    "summ_dir = os.path.join(root_absdir,'data','text_summarization','output','method3')\n",
    "summ_files = os.listdir(summ_dir)\n",
    "\n",
    "scores_df = pd.DataFrame(columns=['pdf','relevance','coherence','consistency','fluency','summ_length','doc_summ_length_ratio'])\n",
    "i = 0 \n",
    "for filename in doc_files:\n",
    "    if filename not in ['lier-detector_text.txt','netflix_text.txt','webtoon_text.txt']:\n",
    "        continue\n",
    "    with open(os.path.join(doc_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        document = f.readlines()\n",
    "\n",
    "    summ_file = summ_files[i]\n",
    "    i+=1\n",
    "    if os.path.exists(os.path.join(summ_dir, summ_file)):\n",
    "        with open(os.path.join(summ_dir, summ_file), 'r', encoding = 'utf-8') as f:\n",
    "            summary = f.readlines()\n",
    "    else:\n",
    "        print(\"⚠️Summary doesn't exist\")\n",
    "        break\n",
    "\n",
    "    # preprocess\n",
    "    document = ''.join(document)\n",
    "    document = erase_tag(document, 'p.\\d*')\n",
    "    # document = document.replace('\\n','')\n",
    "    summary = ''.join(summary)\n",
    "\n",
    "    # Evaluate\n",
    "    rel, coh, cons, flu =gpt.get_geval_score(document, summary, model='gpt-4o-mini')\n",
    "\n",
    "    new_row = pd.DataFrame(data=[[filename.replace('.txt',''),rel,coh,cons,flu,len(summary),round(len(summary)/len(document)*100,2)]],\n",
    "                          columns=scores_df.columns)\n",
    "    scores_df = pd.concat([scores_df, new_row], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89590e-07ca-46e8-96c4-b4df22672b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e585f4f-49d4-4b2c-9cbb-dae3e617edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documnets and summaries\n",
    "ROOT = 'bitamin_auto_readme_generator'\n",
    "root_absdir = os.getcwd().split(ROOT)[0]+ROOT\n",
    "\n",
    "doc_dir = os.path.join(root_absdir,'data','object_detection','output','ocr_samples_txt')\n",
    "doc_files = os.listdir(doc_dir)\n",
    "\n",
    "summ_dir = os.path.join(root_absdir,'data','text_summarization','output','method2')\n",
    "summ_files = os.listdir(summ_dir)\n",
    "\n",
    "scores_df = pd.DataFrame(columns=['pdf','relevance','coherence','consistency','fluency','summ_length','doc_summ_length_ratio'])\n",
    "i = 0 \n",
    "for filename in doc_files:\n",
    "    if filename not in ['lier-detector_text.txt','netflix_text.txt','webtoon_text.txt']:\n",
    "        continue\n",
    "    with open(os.path.join(doc_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        document = f.readlines()\n",
    "\n",
    "    summ_file = summ_files[i]\n",
    "    i+=1\n",
    "    if os.path.exists(os.path.join(summ_dir, summ_file)):\n",
    "        with open(os.path.join(summ_dir, summ_file), 'r', encoding = 'utf-8') as f:\n",
    "            summary = f.readlines()\n",
    "    else:\n",
    "        print(\"⚠️Summary doesn't exist\")\n",
    "        break\n",
    "\n",
    "    # preprocess\n",
    "    document = ''.join(document)\n",
    "    document = erase_tag(document, 'p.\\d*')\n",
    "    # document = document.replace('\\n','')\n",
    "    summary = ' '.join(summary)\n",
    "    summary = re.sub(r'<nan>.*?</nan>', '', summary)\n",
    "\n",
    "    # Evaluate\n",
    "    rel, coh, cons, flu =gpt.get_geval_score(document, summary, model='gpt-4o-mini')\n",
    "\n",
    "    new_row = pd.DataFrame(data=[[filename.replace('.txt',''),rel,coh,cons,flu,len(summary),round(len(summary)/len(document)*100,2)]],\n",
    "                          columns=scores_df.columns)\n",
    "    scores_df = pd.concat([scores_df, new_row], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0fb9c2-b5d2-420a-985d-a4dc47d43c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea010bc9-7327-4751-9ef1-d8f588535c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documnets and summaries\n",
    "ROOT = 'bitamin_auto_readme_generator'\n",
    "root_absdir = os.getcwd().split(ROOT)[0]+ROOT\n",
    "\n",
    "doc_dir = os.path.join(root_absdir,'data','object_detection','output','ocr_samples_txt')\n",
    "doc_files = os.listdir(doc_dir)\n",
    "\n",
    "summ_dir = os.path.join(root_absdir,'data','text_summarization','output','method1')\n",
    "summ_files = os.listdir(summ_dir)\n",
    "\n",
    "scores_df = pd.DataFrame(columns=['pdf','relevance','coherence','consistency','fluency','summ_length','doc_summ_length_ratio'])\n",
    "i = 0 \n",
    "for filename in doc_files:\n",
    "    if filename not in ['lier-detector_text.txt','netflix_text.txt','webtoon_text.txt']:\n",
    "        continue\n",
    "    with open(os.path.join(doc_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        document = f.readlines()\n",
    "\n",
    "    summ_file = summ_files[i]\n",
    "    i+=1\n",
    "    if os.path.exists(os.path.join(summ_dir, summ_file)):\n",
    "        with open(os.path.join(summ_dir, summ_file), 'r', encoding = 'utf-8') as f:\n",
    "            summary = f.readlines()\n",
    "    else:\n",
    "        print(\"⚠️Summary doesn't exist\")\n",
    "        break\n",
    "\n",
    "    # preprocess\n",
    "    document = ''.join(document)\n",
    "    document = erase_tag(document, 'p.\\d*')\n",
    "    # document = document.replace('\\n','')\n",
    "    summary = ''.join(summary)\n",
    "\n",
    "    # Evaluate\n",
    "    rel, coh, cons, flu =gpt.get_geval_score(document, summary, model='gpt-4o-mini')\n",
    "\n",
    "    new_row = pd.DataFrame(data=[[filename.replace('.txt',''),rel,coh,cons,flu,len(summary),round(len(summary)/len(document)*100,2)]],\n",
    "                          columns=scores_df.columns)\n",
    "    scores_df = pd.concat([scores_df, new_row], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8539ab7d-fc9a-4470-991f-e57a4208dc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf</th>\n",
       "      <th>relevance</th>\n",
       "      <th>coherence</th>\n",
       "      <th>consistency</th>\n",
       "      <th>fluency</th>\n",
       "      <th>summ/doc_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arima_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>2.55</td>\n",
       "      <td>70.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asiancup_text</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>2.10</td>\n",
       "      <td>58.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>barbot_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>31.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blind_text</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>2.75</td>\n",
       "      <td>45.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>braintumor_text</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>104.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cartoon_text</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.10</td>\n",
       "      <td>52.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>disease_text</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>2.05</td>\n",
       "      <td>44.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>energy_text</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>4.850000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>20.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hangang_text</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.157895</td>\n",
       "      <td>4.421053</td>\n",
       "      <td>2.00</td>\n",
       "      <td>86.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>insideout_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>2.60</td>\n",
       "      <td>26.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>interior_text</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>73.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kospi_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.550000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>27.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lier-detector_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>2.30</td>\n",
       "      <td>20.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>netflix_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>16.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>restaurant_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>2.15</td>\n",
       "      <td>48.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>trading_text</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>70.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>var_text</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.263158</td>\n",
       "      <td>4.473684</td>\n",
       "      <td>2.00</td>\n",
       "      <td>52.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>webtoon_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>2.80</td>\n",
       "      <td>75.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pdf  relevance  coherence  consistency  fluency  \\\n",
       "0           arima_text       4.00   4.150000     4.100000     2.55   \n",
       "1        asiancup_text       4.20   4.250000     4.650000     2.10   \n",
       "2          barbot_text       4.10   4.100000     4.200000     3.00   \n",
       "3           blind_text       4.05   4.100000     4.150000     2.75   \n",
       "4      braintumor_text       4.05   4.000000     4.750000     2.00   \n",
       "5         cartoon_text       4.05   4.000000     4.000000     2.10   \n",
       "6         disease_text       4.05   4.050000     4.400000     2.05   \n",
       "7          energy_text       4.50   4.150000     4.850000     3.00   \n",
       "8         hangang_text       4.30   4.157895     4.421053     2.00   \n",
       "9       insideout_text       4.00   4.000000     4.150000     2.60   \n",
       "10       interior_text       4.25   4.100000     4.600000     2.65   \n",
       "11          kospi_text       4.00   4.050000     4.550000     2.20   \n",
       "12  lier-detector_text       4.00   4.000000     4.350000     2.30   \n",
       "13        netflix_text       4.10   4.050000     4.000000     2.00   \n",
       "14     restaurant_text       4.10   4.000000     4.100000     2.15   \n",
       "15        trading_text       4.05   4.050000     4.150000     2.00   \n",
       "16            var_text       4.15   4.263158     4.473684     2.00   \n",
       "17        webtoon_text       4.00   4.050000     4.200000     2.80   \n",
       "\n",
       "    summ/doc_ratio  \n",
       "0            70.60  \n",
       "1            58.01  \n",
       "2            31.85  \n",
       "3            45.90  \n",
       "4           104.80  \n",
       "5            52.62  \n",
       "6            44.67  \n",
       "7            20.15  \n",
       "8            86.63  \n",
       "9            26.78  \n",
       "10           73.15  \n",
       "11           27.99  \n",
       "12           20.31  \n",
       "13           16.17  \n",
       "14           48.95  \n",
       "15           70.58  \n",
       "16           52.32  \n",
       "17           75.16  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../../data/text_summarization/output/g-evals/g-eval_cluster_n_summary_temp0_1754.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c671972-b7fb-4992-8eaa-637cd6db94ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf</th>\n",
       "      <th>relevance</th>\n",
       "      <th>coherence</th>\n",
       "      <th>consistency</th>\n",
       "      <th>fluency</th>\n",
       "      <th>summ/doc_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arima_text</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.250</td>\n",
       "      <td>3.00</td>\n",
       "      <td>86.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asiancup_text</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.550</td>\n",
       "      <td>2.00</td>\n",
       "      <td>53.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>barbot_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.400</td>\n",
       "      <td>3.00</td>\n",
       "      <td>45.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blind_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.850</td>\n",
       "      <td>2.95</td>\n",
       "      <td>57.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>braintumor_text</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.650</td>\n",
       "      <td>2.00</td>\n",
       "      <td>91.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cartoon_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.250</td>\n",
       "      <td>2.00</td>\n",
       "      <td>43.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>disease_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.100</td>\n",
       "      <td>2.00</td>\n",
       "      <td>39.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>energy_text</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.400</td>\n",
       "      <td>2.65</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hangang_text</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.500</td>\n",
       "      <td>2.05</td>\n",
       "      <td>101.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>insideout_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.450</td>\n",
       "      <td>2.95</td>\n",
       "      <td>32.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>interior_text</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.500</td>\n",
       "      <td>2.70</td>\n",
       "      <td>76.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kospi_text</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>4.700</td>\n",
       "      <td>2.30</td>\n",
       "      <td>29.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lier-detector_text</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.450</td>\n",
       "      <td>2.15</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>netflix_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.055556</td>\n",
       "      <td>4.125</td>\n",
       "      <td>2.25</td>\n",
       "      <td>14.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>restaurant_text</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.500</td>\n",
       "      <td>2.30</td>\n",
       "      <td>36.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>trading_text</td>\n",
       "      <td>3.95</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.550</td>\n",
       "      <td>2.00</td>\n",
       "      <td>69.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>var_text</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.500</td>\n",
       "      <td>2.00</td>\n",
       "      <td>45.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>webtoon_text</td>\n",
       "      <td>3.95</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.050</td>\n",
       "      <td>2.15</td>\n",
       "      <td>73.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pdf  relevance  coherence  consistency  fluency  \\\n",
       "0           arima_text       4.25   4.200000        4.250     3.00   \n",
       "1        asiancup_text       4.30   4.100000        4.550     2.00   \n",
       "2          barbot_text       4.10   4.000000        4.400     3.00   \n",
       "3           blind_text       4.10   4.200000        4.850     2.95   \n",
       "4      braintumor_text       4.25   4.000000        4.650     2.00   \n",
       "5         cartoon_text       4.00   4.000000        4.250     2.00   \n",
       "6         disease_text       4.00   4.050000        4.100     2.00   \n",
       "7          energy_text       4.05   4.050000        4.400     2.65   \n",
       "8         hangang_text       4.20   4.200000        4.500     2.05   \n",
       "9       insideout_text       4.10   4.100000        4.450     2.95   \n",
       "10       interior_text       4.25   4.200000        4.500     2.70   \n",
       "11          kospi_text       4.45   4.150000        4.700     2.30   \n",
       "12  lier-detector_text       4.05   4.000000        4.450     2.15   \n",
       "13        netflix_text       4.10   4.055556        4.125     2.25   \n",
       "14     restaurant_text       4.10   4.000000        4.500     2.30   \n",
       "15        trading_text       3.95   4.000000        4.550     2.00   \n",
       "16            var_text       4.00   4.050000        4.500     2.00   \n",
       "17        webtoon_text       3.95   4.050000        4.050     2.15   \n",
       "\n",
       "    summ/doc_ratio  \n",
       "0            86.72  \n",
       "1            53.44  \n",
       "2            45.99  \n",
       "3            57.59  \n",
       "4            91.29  \n",
       "5            43.75  \n",
       "6            39.72  \n",
       "7            18.68  \n",
       "8           101.26  \n",
       "9            32.44  \n",
       "10           76.57  \n",
       "11           29.23  \n",
       "12           18.68  \n",
       "13           14.15  \n",
       "14           36.54  \n",
       "15           69.61  \n",
       "16           45.23  \n",
       "17           73.23  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../../data/text_summarization/output/g-evals/g-eval_cluster_n_summary_temp1_1756.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8efa1-a0a2-4298-ba48-7641aeb56772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
