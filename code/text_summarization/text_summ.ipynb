{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pipeline\n",
        "!pip install openai\n",
        "!pip install openai langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5Ypah86C1u2",
        "outputId": "2f04e384-b9cf-4386-d2d9-2395c572cec1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipeline\n",
            "  Downloading pipeline-0.1.0-py3-none-any.whl.metadata (483 bytes)\n",
            "Downloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\n",
            "Installing collected packages: pipeline\n",
            "Successfully installed pipeline-0.1.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.37.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.37.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.1\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.37.1)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Collecting langchain-core<0.3.0,>=0.2.24 (from langchain-openai)\n",
            "  Downloading langchain_core-0.2.25-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.24->langchain-openai) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.24->langchain-openai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3.0,>=0.2.24->langchain-openai)\n",
            "  Downloading langsmith-0.1.94-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.24->langchain-openai) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.24->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.24->langchain-openai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.24->langchain-openai)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\n",
            "Downloading langchain_openai-0.1.19-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.25-py3-none-any.whl (377 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.6/377.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.94-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: orjson, jsonpointer, tiktoken, jsonpatch, langsmith, langchain-core, langchain-openai\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.25 langchain-openai-0.1.19 langsmith-0.1.94 orjson-3.10.6 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import openai\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "H-8f48Oxetyi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wkSUAR5vwVGH",
        "outputId": "5a6aa90c-b28a-43eb-acf8-56f008b63bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/sample2.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()"
      ],
      "metadata": {
        "id": "opxC6SYMisdl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method1"
      ],
      "metadata": {
        "id": "nuaeCx602VNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 5페이지만 입력하여 참여 팀원, 주제, 목차 내용 추출"
      ],
      "metadata": {
        "id": "ZnAUt75OiBvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 첫 5페이지의 텍스트를 text5 변수에 저장"
      ],
      "metadata": {
        "id": "sX6DQDnrjrmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_marker = text.find('<p.6>')\n",
        "\n",
        "if end_marker != -1:\n",
        "    text5 = text[:end_marker]\n",
        "else:\n",
        "    text5 = text"
      ],
      "metadata": {
        "id": "FfSysbcBi4xR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "1DBzn4u0jWiZ",
        "outputId": "4f2366ca-5ae2-4d36-960a-1e4e00548bfb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<p.1>\\n2024 BITAmin 겨울 연합프로젝트 시계열 1조\\nNetflix Stock Price Prediction with News Topic & Sentiment\\n시계열 1조\\n12기 송규헌\\n12기 권도영\\n12기 이태경\\n13기 김서윤\\n13기 한진솔\\n\\n<p.2>\\nCONTENTS\\n01. INTRODUCTION\\n02. DATA PREPROCESSING\\n03. MODELING\\n04. CONCLUSIONS AND LIMITATION\\n\\n<p.3>\\n01. INTRODUCTION\\n\\n<p.4>\\n01. INTRODUCTION\\n1.1 Background of topic selection\\n1.뉴스가 주가 변동에 미치는 영향 탐구\\n주가 예측에 뉴스를 활용할 수 있는지 탐구해보고자 함\\n주가를 예측하는 데 사용하는 데이터로 뉴스의 감성분석 및 토픽 모델링 결과를 사용하고자 함\\n뉴스 기사에서는 주로 한 기업에 대해 보도하고 있어 예측 대상은 한 개의 주식 종목으로 정함\\n뉴스 감성 분석/토픽 모델링 결과를 활용하여 주가를 예측하는 프로젝트는 많지 않아, 직접 뉴스 데이터를 활용하고자 함\\n\\n<p.5>\\n01. INTRODUCTION\\n1.1 Background of topic selection\\n2. 장기적인 추세를 고려할 수 있는 주가 예측 모델 구현\\n기존의 주가 예측 프로젝트는 주로 모델의 예측 일수나 시퀀스 길이를 1~5로 설정함\\n짧은 시간 동안의 주가 변동을 예측한 프로젝트와 달리 예측 일수와 시퀀스 길이를 늘려 단기적인 변동뿐만 아니라 장기적인 추세를 고려한 예측 결과를 얻고자 함\\n기존 프로젝트에서 주가 예측 그래프가 실제 주가 그래프를 단순 평행 이동한 형태로 나타나는 문제를 해결하고자 함\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 마크다운 형식으로 팀원, 주제, main topic 추출"
      ],
      "metadata": {
        "id": "yweVqcFOjzVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key_filepath = \"/content/openai_api_key.json\"\n",
        "with open(api_key_filepath, 'r') as f:\n",
        "    api_key = json.load(f)\n",
        "api_key = api_key['OPENAI_API_KEY']"
      ],
      "metadata": {
        "id": "XXCSOd6ldbz4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "bcVtzndaqbXY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ],
      "metadata": {
        "id": "SigZdx4tt_Gx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_information(text5):\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following information from the provided text:\n",
        "    1. Name (excluding the team name, include all people listed)\n",
        "    2. Title (include the entire title as it appears in the text)\n",
        "    3. Main Topics (only main topics from the Table of Contents, excluding subtopics)\n",
        "\n",
        "    The Main Topics should be extracted from the section that follows headers such as 'TABLE OF CONTENTS', '목차 소개', or any similar variation.\n",
        "    Ensure that main topics are not split into multiple lines. Main topics should be connected by commas if there are more than one.\n",
        "    Ensure that team member's name are not split into multiple lines. Name should be connected by commas if there are more than one.\n",
        "    Ensure that title is not cut off and is extracted in their entirety.\n",
        "    Exclude any subtopics or secondary information while extracting main topics.\n",
        "    Stop at the next page marker '<p.'.\n",
        "\n",
        "    Do not include any additional notes or explanations in the output.\n",
        "\n",
        "    Text: {text5}\n",
        "\n",
        "\n",
        "    Format the extracted information as follows:\n",
        "    <subject>title</subject>\n",
        "    <team>team members</team>\n",
        "    <index>main topics</index>\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    extracted_info = response.content.strip()\n",
        "    return extracted_info\n",
        "\n",
        "extracted_info = extract_information(text5)\n",
        "print(extracted_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_cdttzzt_nX",
        "outputId": "76d701e8-8447-4dcc-cae8-9a70407d34d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<subject>Netflix Stock Price Prediction with News Topic & Sentiment</subject>\n",
            "<team>12기 송규헌, 12기 권도영, 12기 이태경, 13기 김서윤, 13기 한진솔</team>\n",
            "<index>INTRODUCTION, DATA PREPROCESSING, MODELING, CONCLUSIONS AND LIMITATION</index>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_topics_match = re.search(r'<index>(.*?)</index>', extracted_info, re.DOTALL)\n",
        "if main_topics_match:\n",
        "    main_topics = main_topics_match.group(1).strip()\n",
        "else:\n",
        "    main_topics = \"\""
      ],
      "metadata": {
        "id": "De0eBjj0BmuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전체 텍스트에서 대주제, 소주제, 핵심내용 추출"
      ],
      "metadata": {
        "id": "giYOigJxtK85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5페이지에서 추출한 대주제를 사용하는 경우"
      ],
      "metadata": {
        "id": "otIpJaBgD17a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_detailed_information(text, main_topics):\n",
        "    prompt = f\"\"\"\n",
        "    Extract detailed information for each main topic and its subtopics from the provided text.\n",
        "\n",
        "    Format the extracted information as follows:\n",
        "    <main>main topic</main>\n",
        "    <sub>subtopic</sub>\n",
        "    <content>content</content>\n",
        "\n",
        "    Use the main topics provided below:\n",
        "    {main_topics}\n",
        "\n",
        "    Do not include any additional notes or explanations in the output.\n",
        "\n",
        "    Text: {text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    extracted_details = response.content.strip()\n",
        "    return extracted_details"
      ],
      "metadata": {
        "id": "_ifc44eL-oka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detailed_info = extract_detailed_information(text, main_topics)\n",
        "print(detailed_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuGr0wnJ-oiP",
        "outputId": "a9f05a4a-6f6c-4e7e-e04c-21a950a75af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<main>INTRODUCTION</main>\n",
            "<sub>Background of topic selection</sub>\n",
            "<content>뉴스가 주가 변동에 미치는 영향 탐구 주가 예측에 뉴스를 활용할 수 있는지 탐구해보고자 함 주가를 예측하는 데 사용하는 데이터로 뉴스의 감성분석 및 토픽 모델링 결과를 사용하고자 함 뉴스 기사에서는 주로 한 기업에 대해 보도하고 있어 예측 대상은 한 개의 주식 종목으로 정함 뉴스 감성 분석/토픽 모델링 결과를 활용하여 주가를 예측하는 프로젝트는 많지 않아, 직접 뉴스 데이터를 활용하고자 함</content>\n",
            "\n",
            "<sub>Brief Project Introduction</sub>\n",
            "<content>[목표] 뉴스 데이터를 활용한 주가 예측 모델의 최적화 및 효율적인 파라미터 선정 [주요 내용] 장기적인 추세 예측에 효과적인 모델 판별: LSTM vs. GRU vs. Transformer 실험을 통한 모델 파라미터 튜닝 뉴스 감성분석과 토픽 모델링의 주가 예측 유용성 검토 각 실험 결과 평가에 필요한 지표 선정 [개발 환경] Colab</content>\n",
            "\n",
            "<sub>Data collection</sub>\n",
            "<content>FinanceDataReader 라이브러리를 활용하여 NETFLIX 기업의 2018년 1월 2일 ~ 2023년 12월 29일 주가 데이터 수집 동일한 기간에 대한 핀터레스트, 메타플렛폼스, 스포티파이의 주가 데이터 수집</content>\n",
            "\n",
            "<main>DATA PREPROCESSING</main>\n",
            "<sub>Make derived variable</sub>\n",
            "<content>Roc(Range of Change) 변화율 : 1d_RoC, 5d_RoC MA(Moving Average) 이동 평균 : 5MA, 120MA 이전 시점을 포함하여 rolling하는 변수이므로 생성된 파생변수의 첫 시점은 NaN 상태. => 2018년 데이터를 추가로 수집해 파생 변수 생성에 활용</content>\n",
            "\n",
            "<sub>Add indecators</sub>\n",
            "<content>TA(Technical Analysis) Library : 금융 시계열 데이터 세트(시가, 마감, 고가, 저가, 거래량)에 대한 기술 분석 라이브러리.</content>\n",
            "\n",
            "<sub>Peer Analysis</sub>\n",
            "<content>PINS(핀터레스트), FB(메타플렛폼스), SPOT(스포티파이) Netflix와 유사한 사업을 영위하고 있는 기업의 해당 기간 종가를 feature로 추가. 자료출처 : 한국투자증권</content>\n",
            "\n",
            "<main>MODELING</main>\n",
            "<sub>Time Series</sub>\n",
            "<content>시간에 따른 데이터 패턴을 분석하여 미래 값을 예측 Sequence Length (w): 한 번에 모델에 입력되는 연속된 데이터의 수 (input data) Predict Size (k): 모델이 예측할 미래 데이터의 길이 (output data) => 고정된WindowSize: w+k</content>\n",
            "\n",
            "<sub>Modeling</sub>\n",
            "<content>Step 1: Sequence Length = 60, Predict Size = 10 학습에 사용할 1개의 시퀀스 데이터 형태 60개의 Sequence 데이터 10개의 Predict Size 데이터</content>\n",
            "\n",
            "<main>CONCLUSIONS AND LIMITATIONS</main>\n",
            "<sub>Evaluation</sub>\n",
            "<content>예측 평가 기준 시계열 모델의 손실함수로 MSE는 값이 너무 커서 RMSE를 선택 Key Point 어떤 변수를 Target으로 예측하는 것이 좋을까? Close(종가) vs 1d_ROC(일일 등락율) 뉴스 데이터는 예측에 유의미한 영향을 주는가? stock_only_df vs total df 어떤 모델이 예측에 가장 효과적인가? LSTM vs. GRU vs. transformer</content>\n",
            "\n",
            "<sub>Model Result</sub>\n",
            "<content>LSTM (loss = RMSE) GRU (loss = RMSE) Transformer (loss = RMSE)</content>\n",
            "\n",
            "<sub>Compare each parameter</sub>\n",
            "<content>세 모델 중 lstm의 loss 평균값이 가장 작고 transformer의 loss평균값이 가장 큼 gru 모델에서는 주식 데이터로만 사용한 경우, lstm과 transformer 모델에서는 뉴스 데이터를 포함한 데이터셋을 사용한 경우 loss평균값이 더 작음. => loss값은 Sequence_size, Batch_size는 값이 작을수록 유의미한 결과를 나타내고, Model_size에서는 LSTM, Transformer 모두 64에서 유의미했지만 GRU에서는 128에서 유의미한 결과를 보임</content>\n",
            "\n",
            "<sub>Best parameter for each model</sub>\n",
            "<content>validation_loss를 기준으로 모델별로 최적의 파라미터 조합을 선정하여 50번 반복 => 평균값으로 경향성과 오차율을 파악 후 최종 모델 선정 [LSTM] stock_only 데이터셋, seq 30, batch 1, model 64 [GRU] total 데이터셋, seq 30, batch 1, model 128 [Transformer] total 데이터셋, seq 30, batch 1, model 64 평균 오차율 : 1.85% 평균 오차율 : 1.66% 평균 오차율 : 1.62%</content>\n",
            "\n",
            "<sub>Limitations</sub>\n",
            "<content>금융 시장에서는 예상치 못한 사건이 발생하는는 일이 잦아 예측에 어려움이 있음 수치만으로 모델의 성능을 평가하기 어려워 일일이 예측 그래프를 확인해야 함 -> 실험 결과를 비교하는 데 시간 소요 뉴스 Topic NLP로 감성분석과 토픽모델링을 시도하였지만, 시퀀스 길이 문제와 정확도를 높이는 것에 한계가 있어 API를 사용 일반적인 예측의 경우, 5%의 오차가 좋은 평가를 받을 수 있으나 주식 시장에서 5% 오차는 큰 손실 또는 큰 이익-> 예측 성능이 5% 이하로 나온 모델이라 할지라도 검토가 필요</content>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 요약"
      ],
      "metadata": {
        "id": "wv1r7VOaCs8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_content(content):\n",
        "    if not content.strip():\n",
        "        return content\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    다음 내용을 요약해줘:\n",
        "\n",
        "    내용: {content}\n",
        "\n",
        "    위 내용을 간결하게 요약해줘.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    summary = response.content.strip()\n",
        "    return summary\n",
        "\n",
        "summarized_detailed_info = re.sub(\n",
        "    r'(<content>)(.*?)(</content>)',\n",
        "    lambda m: f\"{m.group(1)}{summarize_content(m.group(2))}{m.group(3)}\",\n",
        "    detailed_info,\n",
        "    flags=re.DOTALL\n",
        ")\n",
        "\n",
        "print(summarized_detailed_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w58Tuw5v-ofs",
        "outputId": "e9d576a6-6852-4885-8aeb-60e004e17a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<main>INTRODUCTION</main>\n",
            "<sub>Background of topic selection</sub>\n",
            "<content>뉴스의 감성분석과 토픽 모델링을 사용하여 주가를 예측하는 프로젝트를 진행하기 위해 뉴스 데이터를 활용하고자 함.</content>\n",
            "\n",
            "<sub>Brief Project Introduction</sub>\n",
            "<content>뉴스 데이터를 활용한 주가 예측 모델의 최적화와 효율적인 파라미터 선정을 위해 LSTM, GRU, Transformer 모델을 실험하고 파라미터를 튜닝하며, 뉴스 감성분석과 토픽 모델링의 주가 예측 유용성을 검토한다. Colab을 개발 환경으로 사용한다.</content>\n",
            "\n",
            "<sub>Data collection</sub>\n",
            "<content>FinanceDataReader 라이브러리로 NETFLIX, 핀터레스트, 메타플렛폼스, 스포티파이의 2018년 1월 2일부터 2023년 12월 29일까지의 주가 데이터를 수집했다.</content>\n",
            "\n",
            "<main>DATA PREPROCESSING</main>\n",
            "<sub>Make derived variable</sub>\n",
            "<content>Roc와 MA는 파생변수로 이전 시점을 포함하여 rolling되는 변수이므로 첫 시점은 NaN 상태이다. 따라서 2018년 데이터를 추가로 수집하여 파생 변수 생성에 활용할 필요가 있다.</content>\n",
            "\n",
            "<sub>Add indecators</sub>\n",
            "<content>금융 시계열 데이터에 대한 기술 분석을 도와주는 TA(Technical Analysis) Library이다.</content>\n",
            "\n",
            "<sub>Peer Analysis</sub>\n",
            "<content>한국투자증권에 따르면, PINS, FB, SPOT과 같은 기업들은 Netflix와 유사한 사업을 영위하고 있으며 해당 기간 종가를 feature로 추가하고 있다.</content>\n",
            "\n",
            "<main>MODELING</main>\n",
            "<sub>Time Series</sub>\n",
            "<content>시간에 따른 데이터 패턴을 분석하여 미래 값을 예측하기 위해 Sequence Length와 Predict Size를 고려하며, 이 두 값의 합은 Window Size로 고정된다.</content>\n",
            "\n",
            "<sub>Modeling</sub>\n",
            "<content>학습에 사용할 1개의 시퀀스 데이터는 60개의 입력 데이터와 10개의 예측 데이터로 구성됩니다.</content>\n",
            "\n",
            "<main>CONCLUSIONS AND LIMITATIONS</main>\n",
            "<sub>Evaluation</sub>\n",
            "<content>시계열 모델의 손실함수로 MSE 대신 RMSE를 선택하는 이유와 Target 변수로 종가(Close)를 선택하는 것이 좋다. 뉴스 데이터가 예측에 영향을 줄 수 있고, total df가 stock_only_df보다 예측에 효과적일 수 있음. LSTM, GRU, transformer 중 어떤 모델이 가장 효과적인지는 알 수 없다.</content>\n",
            "\n",
            "<sub>Model Result</sub>\n",
            "<content>LSTM, GRU, Transformer의 손실 함수는 RMSE이다.</content>\n",
            "\n",
            "<sub>Compare each parameter</sub>\n",
            "<content>세 모델 중 LSTM의 평균 loss가 가장 작고, Transformer의 평균 loss가 가장 크며, GRU 모델은 주식 데이터만 사용할 때보다 뉴스 데이터를 포함한 데이터셋을 사용할 때 더 작은 loss를 보임. Sequence size와 Batch size가 작을수록 유의미한 결과를 나타내며, Model size는 LSTM과 Transformer는 64에서, GRU는 128에서 유의미한 결과를 보임.</content>\n",
            "\n",
            "<sub>Best parameter for each model</sub>\n",
            "<content>validation_loss를 기준으로 모델별 최적 파라미터 조합을 선정하여 50번 반복 후 평균값으로 경향성과 오차율을 파악하여 최종 모델을 선정했다. LSTM 모델은 stock_only 데이터셋, seq 30, batch 1, model 64을 사용하여 1.85%의 평균 오차율을 보였고, GRU 모델은 total 데이터셋, seq 30, batch 1, model 128을 사용하여 1.66%의 평균 오차율을 보였으며, Transformer 모델은 total 데이터셋, seq 30, batch 1, model 64를 사용하여 1.62%의 평균 오차율을 보였다.</content>\n",
            "\n",
            "<sub>Limitations</sub>\n",
            "<content>금융 시장은 예상치 못한 사건이 잦아 예측이 어려우며 모델 성능을 평가하기 어려워 실험 결과 비교에 시간이 소요된다. 주식 시장에서는 5% 오차도 큰 손실이나 이익을 가져올 수 있으므로 예측 성능이 5% 이하인 모델도 검토가 필요하다.</content>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Method2"
      ],
      "metadata": {
        "id": "YYjd90AY0GXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ],
      "metadata": {
        "id": "vBoBQMag2Z5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")\n",
        "\n",
        "def extract_information(text):\n",
        "    prompt = f\"\"\"\n",
        "    Classify the provided text into the following categories without paraphrasing, summarizing, altering, or omitting any part of the original text:\n",
        "    - <subject>: Title of the document or project.\n",
        "    - <team>: List of team members.\n",
        "    - <index>: Main topics or sections in the document.\n",
        "    - <main>: Main topic headings.\n",
        "    - <sub>: Subtopics under each main topic.\n",
        "    - <content>: Detailed content related to each subtopic.\n",
        "\n",
        "    Ensure that all text is classified without omitting any content. Each classified section should be enclosed in the appropriate tags. Maintain the order of the text as it appears. If a text doesn't clearly fit into a category, classify it into the closest relevant category. Do not paraphrase, summarize, alter, or omit any part of the original text in any way. Ensure that no part of the text is omitted in the classification and tagging process, even if it seems unnecessary.\n",
        "\n",
        "    First, classify and tag parts of the text that fit into the categories of <subject>, <team>, <index>, <main>, and <sub>. Then, wrap all remaining text as <content>.\n",
        "\n",
        "    Example format:\n",
        "    <subject>Netflix Stock Price Prediction with News Topic & Sentiment</subject>\n",
        "    <team>12기 송규헌, 12기 권도영, 12기 이태경, 13기 김서윤, 13기 한진솔</team>\n",
        "    <index>INTRODUCTION, DATA PREPROCESSING, MODELING, CONCLUSIONS AND LIMITATION</index>\n",
        "    <main>웹툰 추천 시스템 구현</main>\n",
        "    <sub>기존 사용자</sub>\n",
        "    <content>사용자가 이전에 봤던 웹툰: 올가미(17회), 외모지상주의(16회), 이상한 변호사 우영우(7회). 타입 스토리: 장르 판타지, 드라마 등. 웹툰 추천 결과: rmse: 0.10001713</content>\n",
        "    <sub>신규 사용자</sub>\n",
        "    <content>좋아하는 웹툰 3개 입력: 이별 후 사내 결혼, 순정말고 순종, 다시 쓰는 연애사. 타입 스토리: 장르 로맨스 등. 웹툰 추천 결과: rmse: 0.10001718</content>\n",
        "    <main>결론 및 향후 과제</main>\n",
        "    <sub>프로젝트 요약 및 의의</sub>\n",
        "    <content>CF, NCF, SVD, ALS 등 여러 가지 추천 알고리즘 모델을 구축하고 비교 분석한 결과를 토대로 최적의 모델을 선택함으로써, 사용자에게 더 나은 추천을 제공할 수 있음. Explicit feedback 없이 Implicit feedback을 사용하여 ALS 모델을 구축하고, 이를 통해 최상의 결과를 도출해냄. 추천 시스템을 통해 사용자의 선호에 맞는 웹툰을 제공함으로써 사용자들의 만족도를 향상시키고 플랫폼 이용률을 증가시킬 수 있음</content>\n",
        "    <sub>한계점</sub>\n",
        "    <content>이미 정제된 데이터를 사용한 점. 데이터 내 사용자와 아이템 간의 interaction 여부만 포함되어 있고, rating 값이 없어서 사용자의 선호도를 정확하게 파악하기 어려웠음. 추천 시스템은 사용자의 이전 행동을 기반으로 작동하기 때문에, interaction 데이터가 없는 신규 사용자에 대한 추천에는 적합하지 않을 수 있음 (Cold Start Problem)</content>\n",
        "    <sub>향후과제</sub>\n",
        "    <content>explicit feedback(명시적데이터) 수집을 통한 모델 성능 향상</content>\n",
        "\n",
        "    Text: {text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    extracted_info = response.content.strip()\n",
        "    return extracted_info\n",
        "\n",
        "extracted_info = extract_information(text)\n",
        "print(extracted_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMmMn3wL6HCH",
        "outputId": "163f63f9-ced2-4613-d3f0-e46fe80a29a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<subject>Netflix Stock Price Prediction with News Topic & Sentiment</subject>\n",
            "<team>12기 송규헌, 12기 권도영, 12기 이태경, 13기 김서윤, 13기 한진솔</team>\n",
            "<index>INTRODUCTION, DATA PREPROCESSING, MODELING, CONCLUSIONS AND LIMITATION</index>\n",
            "<main>INTRODUCTION</main>\n",
            "<sub>Background of topic selection</sub>\n",
            "<content>뉴스가 주가 변동에 미치는 영향 탐구\n",
            "주가 예측에 뉴스를 활용할 수 있는지 탐구해보고자 함\n",
            "주가를 예측하는 데 사용하는 데이터로 뉴스의 감성분석 및 토픽 모델링 결과를 사용하고자 함\n",
            "뉴스 기사에서는 주로 한 기업에 대해 보도하고 있어 예측 대상은 한 개의 주식 종목으로 정함\n",
            "뉴스 감성 분석/토픽 모델링 결과를 활용하여 주가를 예측하는 프로젝트는 많지 않아, 직접 뉴스 데이터를 활용하고자 함</content>\n",
            "<sub>Brief Project Introduction</sub>\n",
            "<content>[목표]\n",
            "뉴스 데이터를 활용한 주가 예측 모델의 최적화 및 효율적인 파라미터 선정\n",
            "[주요 내용]\n",
            "장기적인 추세 예측에 효과적인 모델 판별: LSTM vs. GRU vs. Transformer\n",
            "실험을 통한 모델 파라미터 튜닝\n",
            "뉴스 감성분석과 토픽 모델링의 주가 예측 유용성 검토\n",
            "각 실험 결과 평가에 필요한 지표 선정\n",
            "[개발 환경]\n",
            "Colab</content>\n",
            "<sub>Data collection</sub>\n",
            "<content>FinanceDataReader 라이브러리를 활용하여 NETFLIX 기업의 2018년 1월 2일 ~ 2023년 12월 29일\n",
            "주가 데이터 수집\n",
            "동일한 기간에 대한 핀터레스트, 메타플렛폼스, 스포티파이의 주가 데이터 수집</content>\n",
            "<main>DATA PREPROCESSING</main>\n",
            "<sub>Make derived variable</sub>\n",
            "<content>Roc(Range of Change) 변화율 : 1d_RoC, 5d_RoC \n",
            "MA(Moving Average) 이동 평균 : 5MA, 120MA\n",
            "이전 시점을 포함하여 rolling하는 변수이므로 생성된 파생변수의 첫 시점은 NaN 상태.\n",
            "=> 2018년 데이터를 추가로 수집해 파생 변수 생성에 활용</content>\n",
            "<sub>Add indecators</sub>\n",
            "<content>TA(Technical Analysis) Library\n",
            ": 금융 시계열 데이터 세트(시가, 마감, 고가, 저가, 거래량)에 대한 기술 분석 라이브러리.</content>\n",
            "<sub>Peer Analysis : Add similar stock price</sub>\n",
            "<content>PINS(핀터레스트), FB(메타플렛폼스), SPOT(스포티파이)\n",
            "Netflix와 유사한 사업을 영위하고 있는 기업의 해당 기간 종가를 feature로 추가.\n",
            "자료출처 : 한국투자증권</content>\n",
            "<sub>Peer Analysis : Fill missing value</sub>\n",
            "<content>df.bffil()\n",
            "PINS의 경우 2019년 04월 22일에 최초 상장\n",
            "=> 상장 이전의 결측값은 최초 상장일의 종가(24.99$)를 기준으로 대체</content>\n",
            "<sub>Remove multicollinearity</sub>\n",
            "<content>Open, Close, High, Low, 5MA,\n",
            "120MA, bol_high, bol_low\n",
            "주가의 변화와 관련된 기본 지표이므로 유지\n",
            "VMAP, BHB, BLB, KCH, KCL, KCM,\n",
            "DCH, DCL, DCM, SMA, EMA, WMA\n",
            "주가 변화 관련 기본 지표들과 1에 가까운 상관관계를 가지는 보조 지표 Drop\n",
            "FB, PINS, SPOT\n",
            "Peer stock은 유지</content>\n",
            "<sub>Make derived variable for News Topics & Sentiment</sub>\n",
            "<content>Topics\n",
            "Label Encoding\n",
            "Sentiments\n",
            "5MM, 60MM, 120MM\n",
            "OneHot Encoding\n",
            "5MA, 60MA, 120MA\n",
            "5MM, 60MM, 120MM\n",
            "당일의 뉴스가 당일의 주가에 영향을 미칠 가능성은 적기 때문에 과거 뉴스의 영향력을 측정하기 위해 Moving Averae, Moving Mode로 추가</content>\n",
            "<main>MODELING</main>\n",
            "<sub>Time Series</sub>\n",
            "<content>시간에 따른 데이터 패턴을 분석하여 미래 값을 예측\n",
            "Sequence Length (w): \n",
            "한 번에 모델에 입력되는 연속된 데이터의 수 (input data)  \n",
            "Predict Size (k):\n",
            "모델이 예측할 미래 데이터의 길이 (output data)\n",
            " => 고정된WindowSize: w+k</content>\n",
            "<sub>Modeling</sub>\n",
            "<content>Step 1: Sequence Length = 60,   Predict Size  = 10\n",
            "학습에 사용할 1개의 시퀀스 데이터 형태\n",
            "60개의 Sequence 데이터\n",
            "10개의 Predict Size 데이터</content>\n",
            "<sub>Model Comparison</sub>\n",
            "<sub>Model Comparison 1 : 무엇을 target으로?</sub>\n",
            "<sub>Model Comparison 2 : 뉴스 포함? 미포함?</sub>\n",
            "<sub>Model Comparison 3 : LSTM vs. GRU vs. Transformer</sub>\n",
            "<main>CONCLUSIONS AND LIMITATION</main>\n",
            "<sub>Evaluation</sub>\n",
            "<content>예측 평가 기준 \n",
            "시계열 모델의 손실함수로 MSE는 값이 너무 커서 RMSE를 선택 \n",
            "Key Point\n",
            "어떤 변수를 Target으로 예측하는 것이 좋을까?\n",
            "Close(종가) vs 1d_ROC(일일 등락율)\n",
            "뉴스 데이터는 예측에 유의미한 영향을 주는가?\n",
            "stock_only_df vs total df\n",
            "어떤 모델이 예측에 가장 효과적인가?\n",
            "LSTM vs. GRU vs. transformer</content>\n",
            "<sub>Select target variable</sub>\n",
            "<content>Close vs. 1d_ROC\n",
            "Dataset : stockOnly_df\n",
            "model : LSTM, seq_size : 60, batch_size : 8, model_size : 64\n",
            "Close\n",
            "1d_ROC\n",
            "\n",
            "Close vs. 1d_ROC\n",
            "Dataset : total_df\n",
            "model : LSTM, seq_size : 60, batch_size : 8, model_size : 64\n",
            "Close\n",
            "1d_ROC\n",
            "=> Close를 직접 예측할때보다 ROC로 예측하는것이 추세 반영이 잘 되는 것을 확인\n",
            "시계열 예측에서 정상성을 확보해야 예측모델을 더욱 신뢰할 수 있기에 ROC로 Target 확정</content>\n",
            "<sub>Model Result</sub>\n",
            "<content>LSTM (loss = RMSE)</content>\n",
            "<content>GRU (loss = RMSE)</content>\n",
            "<content>Transformer (loss = RMSE)</content>\n",
            "<sub>Compare each parameter</sub>\n",
            "<content>세 모델 중 lstm의 loss 평균값이 가장 작고 transformer의 loss평균값이 가장 큼\n",
            "gru 모델에서는 주식 데이터로만 사용한 경우, lstm과 transformer 모델에서는 뉴스 데이터를 포함한 데이터셋을 사용한 경우 loss평균값이 더 작음.</content>\n",
            "<content>=> loss값은 Sequence_size, Batch_size는 값이 작을수록 유의미한 결과를 나타내고, Model_size에서는 LSTM, Transformer 모두 64에서 유의미했지만 GRU에서는 128에서 유의미한 결과를 보임</content>\n",
            "<sub>Best parameter for each model</sub>\n",
            "<content>validation_loss를 기준으로 모델별로 최적의 파라미터 조합을 선정하여 50번 반복 \n",
            "=> 평균값으로 경향성과 오차율을 파악 후 최종 모델 선정\n",
            "[LSTM] stock_only 데이터셋, seq 30, batch 1, model 64 \n",
            "[GRU] total 데이터셋, seq 30, batch 1, model 128 \n",
            "[Transformer] total 데이터셋, seq 30, batch 1, model 64 \n",
            "평균 오차율 : 1.85%\n",
            "평균 오차율 : 1.66%\n",
            "평균 오차율 : 1.62%</content>\n",
            "<sub>Limitations</sub>\n",
            "<content>금융 시장에서는 예상치 못한 사건이 발생하는는  일이 잦아 예측에 어려움이 있음 \n",
            "수치만으로 모델의 성능을 평가하기 어려워 일일이 예측 그래프를 확인해야 함 -> 실험 결과를 비교하는 데 시간 소요 \n",
            "뉴스 Topic NLP로 감성분석과 토픽모델링을 시도하였지만, 시퀀스 길이 문제와 정확도를 높이는 것에 한계가 있어 API를 사용 \n",
            "일반적인 예측의 경우, 5%의 오차가 좋은 평가를 받을 수 있으나 주식 시장에서 5% 오차는 큰 손실 또는 큰 이익-> 예측 성능이 5% 이하로 나온 모델이라 할지라도 검토가 필요</content>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9xCFXKq8-UXD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}