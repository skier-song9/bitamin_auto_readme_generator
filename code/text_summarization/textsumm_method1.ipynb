{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H-8f48Oxetyi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "import os\n",
        "import re\n",
        "from langchain_openai import ChatOpenAI\n",
        "import transformers\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "opxC6SYMisdl"
      },
      "outputs": [],
      "source": [
        "file_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\object_detection\\\\output\\\\ocr_samples_txt\\\\trading_text.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "XXCSOd6ldbz4"
      },
      "outputs": [],
      "source": [
        "api_key_filepath = \"C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\비타민NLP_240701\\\\text_summarization\\\\openai_api_key.json\"\n",
        "with open(api_key_filepath, 'r') as f:\n",
        "    api_key = json.load(f)\n",
        "api_key = api_key['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "bcVtzndaqbXY"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_3 = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_4 = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX6DQDnrjrmI"
      },
      "source": [
        "### STEP1: 첫 5페이지의 텍스트만 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "FfSysbcBi4xR"
      },
      "outputs": [],
      "source": [
        "end_marker = text.find('<p.6>')\n",
        "\n",
        "if end_marker != -1:\n",
        "    text5 = text[:end_marker]\n",
        "else:\n",
        "    text5 = text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yweVqcFOjzVb"
      },
      "source": [
        "### STEP2: 마크다운 형식으로 팀원, 주제, main topic 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_cdttzzt_nX",
        "outputId": "76d701e8-8447-4dcc-cae8-9a70407d34d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<subject>을홀 '시' 계을 |측 과 학 할용 한 피미 여 강호 습을 시 템 트 레 이딩 익 수 극도 화 BTAMIN PROJECT</subject>\n",
            "<team>송규헌, 송휘종, 서영우, 이태경, 정유진, 정준우</team>\n",
            "<index>주제, NorkFlow, 종목 선정, 시스템 트레이딩, 데이터수집및 전처리, 시계열 모델, 강화학습 모델, 모델 평가및 선정, 시스템 트레이딩, 결과</index>\n"
          ]
        }
      ],
      "source": [
        "def extract_info(text5):\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following information from the provided text:\n",
        "    1. Name (excluding the team name, include all people listed)\n",
        "    2. Title (include the entire title as it appears in the text)\n",
        "    3. Main Topics (only main topics from the Table of Contents, excluding subtopics)\n",
        "\n",
        "    The Main Topics should be extracted from the section that follows headers such as 'TABLE OF CONTENTS', '목차 소개', or any similar variation.\n",
        "    Ensure that team member's name are not split into multiple lines. Name should be connected by commas if there are more than one.\n",
        "    Ensure that title is not cut off and is extracted in their entirety.\n",
        "    Exclude any subtopics or secondary information while extracting main topics.\n",
        "    Stop at the next page marker '<p.'.\n",
        "\n",
        "    Do not include any additional notes or explanations in the output.\n",
        "\n",
        "    Text: {text5}\n",
        "\n",
        "\n",
        "    Format the extracted information as follows:\n",
        "    <subject>title</subject>\n",
        "    <team>team members</team>\n",
        "    <index>main topics</index>\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_4.invoke(prompt)\n",
        "    extracted_info = response.content.strip()\n",
        "    return extracted_info\n",
        "\n",
        "extracted_info = extract_info(text5)\n",
        "print(extracted_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "De0eBjj0BmuA"
      },
      "outputs": [],
      "source": [
        "main_topics_match = re.search(r'<index>(.*?)</index>', extracted_info, re.DOTALL)\n",
        "if main_topics_match:\n",
        "    main_topics = main_topics_match.group(1).strip()\n",
        "else:\n",
        "    main_topics = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giYOigJxtK85"
      },
      "source": [
        "### STEP3: 전체 텍스트에서 대주제, 소주제, 세부내용 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "_ifc44eL-oka"
      },
      "outputs": [],
      "source": [
        "def extract_detrails(text, main_topics):\n",
        "    prompt = f\"\"\"\n",
        "    Extract detailed information for each main topic and its subtopics from the provided text.\n",
        "    Do not change the language of the original input text. \n",
        "    Focus on the key details and results relevant to the project, with particular emphasis on the final model outcomes. \n",
        "    Ensure that the final model results are described with a high level of accuracy and detail, including specific metrics, performance evaluations, and any significant findings.\n",
        "    Include the **first page** where each subtopic begins in the \"pages used\" list. For example, if a subtopic spans from page 7 to page 10, only include page 7 in the result.\n",
        "    Ensure that detailed contents are extracted comprehensively from all relevant pages, leaving no key information behind. \n",
        "    Focus on capturing detailed content across all pages, ensuring that nothing is missed, especially in relation to the provided main topics.\n",
        "\n",
        "    Format the extracted information as follows:\n",
        "    <main>main topic</main>\n",
        "    <sub>subtopic</sub> <content>detailed contents</content> <page>3</page>\n",
        "    <sub>subtopic</sub> <content>detailed contents</content> <page>5</page>\n",
        "\n",
        "    <main>main topic</main>\n",
        "    <sub>subtopic</sub> <content>detailed contents</content> <page>10</page>\n",
        "    <sub>subtopic</sub> <content>detailed contents</content> <page>11</page>\n",
        "\n",
        "    Use the main topics provided below:\n",
        "    {main_topics}\n",
        "    \n",
        "    The text is divided into pages using the format <p.number>. For example, page 2 is marked as <p.2>. \n",
        "    Make sure to extract text from all pages except for the first five pages to ensure no information is missed.\n",
        "    If any part of the text seems to be related to the main topics but is not included in the main topics list(main_topics), include it as a subtopic under the appropriate main topic.\n",
        "    Do not include any additional notes or explanations in the output.\n",
        "\n",
        "    Text: {text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_4.invoke(prompt)\n",
        "    extracted_details = response.content.strip()\n",
        "    return extracted_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuGr0wnJ-oiP",
        "outputId": "a9f05a4a-6f6c-4e7e-e04c-21a950a75af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<main>주제</main>\n",
            "<sub>시계열 예측과 트레이딩 수익 극대화 강화학습을 활용한 시스템</sub> \n",
            "<content>기존 금융 딥러닝의 실패 요인들로는 노이즈가 심하고, 데이터 수가 부족하며, 과적합이 심한 점이 있다. 예를 들어, 마이크로소프트의 상장일인 1986년 3월 13일~현재까지의 주식 일봉 데이터 개수는 9491개 뿐이다. 이를 해결하기 위해 Time Series Forecasting을 통해 다음 일주일 동안 가장 큰 종가 변화율을 보일 종목을 예측하고, Reinforcement Learning을 통해 분 단위 거래를 학습하여 단타매매를 실행한다.</content> \n",
            "<page>3</page>\n",
            "\n",
            "<main>WorkFlow</main>\n",
            "<sub>Day Candle Stoch Datd</sub> \n",
            "<content>LETF, 6 Stock Code, 크레온 AP, Airute Candle EtoC</content> \n",
            "<page>4</page>\n",
            "\n",
            "<main>종목 선정</main>\n",
            "<sub>국내 주식 Reader로 데이터 수집</sub> \n",
            "<content>IFinance Data를 통해 2017-01-01부터 현재까지의 데이터를 수집하고, 수집 항목으로는 종가와 종목 KOSP가 포함된다.</content> \n",
            "<page>5</page>\n",
            "\n",
            "<main>데이터수집및 전처리</main>\n",
            "<sub>Transform</sub> \n",
            "<content>데이터 변환 과정이 포함된다.</content> \n",
            "<page>6</page>\n",
            "<sub>계산 RSI Sharpe Ratio</sub> \n",
            "<content>RSI (Relative Index Strength)와 Sharpe Ratio를 계산한다. RSI는 상대강도지수로, Sharpe Ratio는 투자 포트폴리오의 기대 수익률과 무위험 수익률의 차이를 표준편차로 나눠 투자의 총 위험을 나타낸다.</content> \n",
            "<page>7</page>\n",
            "\n",
            "<main>시계열 모델</main>\n",
            "<sub>Long Short-Term Memory (LSTM) 기반의 모델</sub> \n",
            "<content>RNN의 장기 의존성 문제를 해결하여 오랜 기간의 데이터를 반영할 수 있는 모델이다.</content> \n",
            "<page>8</page>\n",
            "<sub>Dlinear LSTF-linear</sub> \n",
            "<content>Transformer 구조로 선형 예측을 수행하며, 기존 Transformer에서의 시간적 정보 손실을 방지한다.</content> \n",
            "<page>9</page>\n",
            "\n",
            "<main>모델 평가및 선정</main>\n",
            "<sub>평가 기법</sub> \n",
            "<content>MAE (평균 절대 오차)와 MSE (평균 제곱 오차)를 사용한다. LSTM에 대한 MAE는 37084, MSE는 150189이며, LSTF에 대한 MAE는 00811, MSE는 00348이다.</content> \n",
            "<page>10</page>\n",
            "\n",
            "<main>시스템 트레이딩</main>\n",
            "<sub>데이터 수집</sub> \n",
            "<content>대신증권을 이용하여 분봉 데이터를 최대 20만개 수집한다. 최근 0개의 데이터를 사용하여 학습을 진행하며, 장 마감 후 당일 분봉 데이터로 추가 학습을 수행한다.</content> \n",
            "<page>11</page>\n",
            "<sub>데이터 전처리</sub> \n",
            "<content>종가와 거래량의 이동 평균, 스토케스틱 오실레이터, 상대강도지수, 볼린저밴드, 이동평균수렴확산, AROON, ATR 등의 다양한 지표를 사용하여 데이터를 전처리한다.</content> \n",
            "<page>12</page>\n",
            "<sub>전처리 후 데이터 예시</sub> \n",
            "<content>전처리 후의 데이터 예시를 제공한다.</content> \n",
            "<page>13</page>\n",
            "<sub>강화학습 모델</sub> \n",
            "<content>강화학습은 머신러닝의 한 종류로, 주식투자에서 매수, 매도, 관망 등을 판단하는 문제에 적용된다. 신경망은 에이전트가 수행하는 행동의 결과로 발생하는 수익 또는 손실의 보상과 학습 데이터를 통해 학습한다.</content> \n",
            "<page>14</page>\n",
            "<sub>A2C (Advantage Actor Critic) 알고리즘</sub> \n",
            "<content>Actor는 정책 경사 모델을 사용하여 현재 상태에서의 행동을 결정하며, Critic은 Q-러닝 모델을 사용하여 취해진 행동의 가치를 평가한다. Advantage를 통해 보상과 정책에 따른 가치 함수의 차이를 이용해 정책을 업데이트한다.</content> \n",
            "<page>15</page>\n",
            "\n",
            "<main>결과</main>\n",
            "<sub>트레이딩</sub> \n",
            "<content>학습 모델은 A2C, CNN 120dim, 15000개 분봉데이터, 300epochs를 사용한다. 한국투자증권 AP를 통해 1분 단위로 buy, sell, hold를 예측하며, Confidence에 따라 주문 수량을 결정하고, API로 주문을 수행한다.</content> \n",
            "<page>16</page>\n",
            "<sub>backtrading</sub> \n",
            "<content>포트폴리오 가치는 삼부토건, 동일제강, HD현대일렉트르, 신풍, 진원생명과학 등으로 구성된다.</content> \n",
            "<page>17</page>\n",
            "\n",
            "<main>시스템 트레이딩</main>\n",
            "<sub>한계점</sub> \n",
            "<content>훈련 과정에서의 과적합이 발생할 수 있다.</content> \n",
            "<page>19</page>\n"
          ]
        }
      ],
      "source": [
        "detailed_info = extract_detrails(text, main_topics)\n",
        "print(detailed_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv1r7VOaCs8O"
      },
      "source": [
        "### STEP4: 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w58Tuw5v-ofs",
        "outputId": "e9d576a6-6852-4885-8aeb-60e004e17a0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<subject>주제</subject>\n",
            "<team>이하나, 김하니</team>\n",
            "<index>시계열 예측과 트레이딩 수익 극대화 강화학습을 활용한 시스템>\n",
            "\n",
            "<main>시계열 예측과 트레이딩 수익 극대화 강화학습을 활용한 시스템</main>\n",
            "<sub>시계열 예측과 트레이딩 수익 극대화 강화학습을 활용한 시스템</sub> <content>기존 금융 딥러닝의 실패 요인으로는 노이즈, 데이터 부족, 과적합이 있으며, 이를 해결하기 위해 Time Series Forecasting과 Reinforcement Learning을 활용하여 단타매매를 실행한다.</content> <page>3</page>\n",
            "\n",
            "<main>WorkFlow</main>\n",
            "<sub>Day Candle Stoch Datd</sub> <content>LETF, 6 Stock Code, 크레온 AP, Airute Candle EtoC</content> <page>4</page>\n",
            "\n",
            "<main>종목 선정</main>\n",
            "<sub>국내 주식 Reader로 데이터 수집</sub> <content>IFinance Data를 통해 2017-01-01부터 현재까지의 종가와 종목 KOSP 데이터를 수집한다.</content> <page>5</page>\n",
            "\n",
            "<main>데이터수집및 전처리</main>\n",
            "<sub>Transform</sub> <content>데이터 변환 과정을 포함한다.</content> <page>6</page>\n",
            "<sub>계산 RSI Sharpe Ratio</sub> <content>RSI와 Sharpe Ratio를 계산하여 투자 포트폴리오의 위험을 평가한다.</content> <page>7</page>\n",
            "\n",
            "<main>시계열 모델</main>\n",
            "<sub>Long Short-Term Memory (LSTM) 기반의 모델</sub> <content>RNN의 장기 의존성 문제를 해결하여 오랜 기간의 데이터를 반영할 수 있는 모델이다.</content> <page>8</page>\n",
            "<sub>Dlinear LSTF-linear</sub> <content>Transformer 구조로 선형 예측을 수행하며, 시간적 정보 손실을 방지한다.</content> <page>9</page>\n",
            "\n",
            "<main>모델 평가및 선정</main>\n",
            "<sub>평가 기법</sub> <content>MAE와 MSE를 사용하며, LSTM의 MAE는 37084, MSE는 150189이고, LSTF의 MAE는 00811, MSE는 00348이다.</content> <page>10</page>\n",
            "\n",
            "<main>시스템 트레이딩</main>\n",
            "<sub>데이터 수집</sub> <content>대신증권을 이용하여 최대 20만개의 분봉 데이터를 수집하며, 학습에 사용한다.</content> <page>11</page>\n",
            "<sub>데이터 전처리</sub> <content>종가와 거래량의 이동 평균 등 다양한 지표를 사용하여 데이터를 전처리한다.</content> <page>12</page>\n",
            "<sub>전처리 후 데이터 예시</sub> <content>전처리 후의 데이터 예시를 제공한다.</content> <page>13</page>\n",
            "<sub>강화학습 모델</sub> <content>강화학습을 통해 주식 투자에서의 매수, 매도, 관망 등을 판단한다.</content> <page>14</page>\n",
            "<sub>A2C (Advantage Actor Critic) 알고리즘</sub> <content>Actor는 정책 경사 모델을 사용하여 행동을 결정하고, Critic은 Q-러닝 모델을 사용하여 행동의 가치를 평가한다.</content> <page>15</page>\n",
            "\n",
            "<main>결과</main>\n",
            "<sub>트레이딩</sub> <content>학습 모델은 A2C, CNN 120dim, 15000개 분봉데이터, 300epochs를 사용하며, 1분 단위로 매매를 예측하고 API로 주문을 수행한다.</content> <page>16</page>\n",
            "<sub>backtrading</sub> <content>포트폴리오는 삼부토건, 동일제강, HD현대일렉트르, 신풍, 진원생명과학 등으로 구성된다.</content> <page>17</page>\n",
            "\n",
            "<main>시스템 트레이딩</main>\n",
            "<sub>한계점</sub> <content>훈련 과정에서 과적합이 발생할 수 있다.</content> <page>19</page>\n"
          ]
        }
      ],
      "source": [
        "def summarize_content(content):\n",
        "    if not content.strip():\n",
        "        return content\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Do not change the language of the original input text. \n",
        "    Summarize in the same language as the input text.\n",
        "    The purpose of the summaries is to create a comprehensive README for a GitHub project.\n",
        "    If multiple subtopics within the same main topic have the same name, combine their content into a single entry with all relevant details, and merge the page numbers into a single list. \n",
        "    Ensure that no duplicate subtopics appear within the same main topic. \n",
        "    All content related to the same subtopic should be combined into one entry, regardless of how many times it appears.\n",
        "    Ensure that subtopics from different main topics are not merged.\n",
        "\n",
        "    Ensure that each summary accurately captures the key points and essential information of its respective partition.\n",
        "    Summarize in a way that is concise and informative, omitting repetitive mentions of the main topic after the first mention in each partition.\n",
        "    Focus on the key details and results relevant to the project, with particular emphasis on the final model outcomes. \n",
        "    Ensure that the final model results are described with a high level of accuracy and detail, including specific metrics, performance evaluations, and any significant findings.\n",
        "    For each subtopic, aim to provide a summary that is 1 to 2 sentences long. \n",
        "    Focus on delivering a concise yet comprehensive overview, capturing the main points and essential details without unnecessary elaboration.\n",
        "    When summarizing, only include the **first page** where each subtopic begins in the \"pages used\" list. For example, if a subtopic spans from page 7 to page 10, only include page 7 in the summary.\n",
        "    Use the example format as a reference only—do not include it in the final output. Ensure that <sub> and <content> tags are separated by a single space, not a line break, and that the output follows this format exactly.\n",
        "    Ensure the summary always begins with <subject>, <team>, and <index> in that order, before listing the main topics and content. \n",
        "\n",
        "    Here is an example of how the tagged text should look:\n",
        "\n",
        "    <subject>Korea</subject>\n",
        "    <team>이하나, 김하니</team>\n",
        "    <index>Seasons in Korea>\n",
        "    \n",
        "    <main>Seasons in Korea</main>\n",
        "    <sub>봄</sub> <content>한국의 봄은 3월에서 5월까지 지속되며, 온화한 기온과 함께 벚꽃이 만개하는 시기입니다. 이 시기에는 다양한 봄꽃 축제가 열리며, 사람들이 야외 활동을 즐기기에 좋은 날씨입니다.</content> <page>1</page>\n",
        "    <sub>여름</sub> <content>한국의 여름은 대개 매우 덥고 습하며, 기온이 종종 30°C를 넘습니다. 이 시기는 장마철이기도 해서 특히 7월에 많은 비가 내립니다. 더위에도 불구하고, 이 시기는 휴가철로 인기가 많아 많은 사람들이 해변과 리조트로 향합니다.</content> <page>3</page>\n",
        "    <sub>가을</sub> <content>가을은 한국에서 가장 아름다운 계절 중 하나로 꼽힙니다. 9월에서 11월 사이, 날씨는 시원하고 하늘은 맑으며, 단풍이 절정을 이루어 산과 들이 붉고 노란 색으로 물듭니다. 이 시기는 또한 수확의 계절로, 각종 축제가 열립니다.</content> <page>5</page>\n",
        "    <sub>겨울</sub> <content>한국의 겨울은 12월에 시작되며, 대부분의 지역에서 기온이 영하로 떨어집니다. 겨울철은 건조하며, 특히 산악 지역에서는 가끔씩 눈이 내립니다. 이 시기에는 스키나 얼음 낚시와 같은 겨울 스포츠가 인기를 끌고 있습니다.</content> <page>7</page>\n",
        "\n",
        "    <main>Animals in Korea</main>\n",
        "    <sub>봄</sub> <content>봄철에는 한국의 자연이 다시 깨어나면서 다양한 동물들이 활동을 시작합니다. 산과 숲에서는 새들이 짝을 찾기 위해 지저귀고, 개구리와 같은 양서류들이 물가에서 활동을 재개합니다. 또한, 겨울잠에서 깨어난 동물들이 활발히 먹이를 찾는 모습을 볼 수 있습니다.</content> <page>9</page>\n",
        "    <sub>여름</sub> <content>여름철에는 다양한 야생 동물들이 활발히 활동합니다. 특히, 한국의 산과 숲에서 멧돼지, 사슴, 다양한 종류의 새들이 많이 보입니다. 여름은 또한 번식기가 겹쳐 동물들이 더욱 활발히 움직이는 시기입니다.</content> <page>10</page>\n",
        "    <sub>가을</sub> <content>가을에는 동물들이 겨울을 준비하는 모습을 볼 수 있습니다. 많은 동물들이 겨울잠을 준비하기 위해 지방을 축적하고, 새들은 따뜻한 지역으로 이동하기 시작합니다. 이 시기는 또한 수확기가 겹쳐, 농작물에 접근하는 동물들이 많아집니다.</content> <page>11</page>\n",
        "    <sub>겨울</sub> <content>겨울철에는 많은 동물들이 추위를 피하기 위해 겨울잠에 들어갑니다. 겨울잠을 자지 않는 동물들은 추위를 이기기 위해 두꺼운 털을 기르거나 활동을 줄입니다. 특히, 한국의 산악지대에서는 고라니와 같은 동물들이 눈 속에서 식량을 찾아다니는 모습을 볼 수 있습니다.</content> <page>12</page>\n",
        "\n",
        "    \n",
        "    Text: {content}\n",
        "\n",
        "    Please summarize the above content concisely.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_4.invoke(prompt)\n",
        "    summary = response.content.strip()\n",
        "    return summary\n",
        "\n",
        "\n",
        "summarized_text = summarize_content(detailed_info)\n",
        "print(summarized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\trading_text.txt'.\n"
          ]
        }
      ],
      "source": [
        "# 텍스트 파일로 저장\n",
        "# 경로 수정\n",
        "\n",
        "output_name = file_path.split('\\\\')[-1].split('_text')[0]\n",
        "\n",
        "# 새로운 파일 경로 설정\n",
        "output_file_path = f'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\text_summarization\\\\output\\\\method1\\\\{output_name}_text.txt'\n",
        "\n",
        "# summarized_text 변수를 텍스트 파일로 저장하는 코드\n",
        "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(summarized_text)\n",
        "\n",
        "print(f\"Summarized text has been saved to '{output_file_path}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
