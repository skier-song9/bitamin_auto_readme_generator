{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H-8f48Oxetyi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "import os\n",
        "import re\n",
        "from langchain_openai import ChatOpenAI\n",
        "import transformers\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "opxC6SYMisdl"
      },
      "outputs": [],
      "source": [
        "file_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\object_detection\\\\output\\\\ocr_samples_txt\\\\netflix_text.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XXCSOd6ldbz4"
      },
      "outputs": [],
      "source": [
        "api_key_filepath = \"C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\비타민NLP_240701\\\\text_summarization\\\\openai_api_key.json\"\n",
        "with open(api_key_filepath, 'r') as f:\n",
        "    api_key = json.load(f)\n",
        "api_key = api_key['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bcVtzndaqbXY"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_3 = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_4 = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX6DQDnrjrmI"
      },
      "source": [
        "### STEP1: 첫 5페이지의 텍스트에서 팀원, 주제, main topic 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_5pages(text, marker='<p.6>'):\n",
        "    end_marker = text.find('<p.6>')\n",
        "    if end_marker != -1:\n",
        "        text5 = text[:end_marker]\n",
        "    else:\n",
        "        text5 = text\n",
        "    return text5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_info(text):\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following information from the provided text:\n",
        "    1. Name (excluding the team name, include all people listed)\n",
        "    2. Title (include the entire title as it appears in the text)\n",
        "    3. Main Topics (only main topics from the Table of Contents, excluding subtopics)\n",
        "\n",
        "    The Main Topics should be extracted from the section that follows headers such as 'TABLE OF CONTENTS', '목차 소개', or any similar variation.\n",
        "    Ensure that team member's name are not split into multiple lines. Name should be connected by commas if there are more than one.\n",
        "    Ensure that title is not cut off and is extracted in their entirety.\n",
        "    Exclude any subtopics or secondary information while extracting main topics.\n",
        "    Stop at the next page marker '<p.'.\n",
        "\n",
        "    Do not include any additional notes or explanations in the output.\n",
        "\n",
        "    Text: {text}\n",
        "\n",
        "\n",
        "    Format the extracted information as follows:\n",
        "    <subject>title</subject>\n",
        "    <team>team members</team>\n",
        "    <index>main topics</index>\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_4.invoke(prompt)\n",
        "    extracted_info = response.content.strip()\n",
        "    return extracted_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "De0eBjj0BmuA"
      },
      "outputs": [],
      "source": [
        "def extract_main_topics(extracted_info):\n",
        "\n",
        "    main_topics_match = re.search(r'<index>(.*?)</index>', extracted_info, re.DOTALL)\n",
        "    if main_topics_match:\n",
        "        main_topics = main_topics_match.group(1).strip()\n",
        "        main_topics_list = [topic.strip() for topic in main_topics.split(',')]\n",
        "    else:\n",
        "        main_topics_list = []\n",
        "\n",
        "    return main_topics_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giYOigJxtK85"
      },
      "source": [
        "### STEP2: 전체 텍스트에서 대주제, 소주제, 세부내용 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_ifc44eL-oka"
      },
      "outputs": [],
      "source": [
        "def extract_details(text, main_topics):\n",
        "    prompt = f\"\"\"\n",
        "    Extract detailed information for each main topic and its subtopics from the provided text.\n",
        "    Do not change the language of the original input text. \n",
        "    Focus on the key details and results relevant to the project, with particular emphasis on the final model outcomes. \n",
        "    Ensure that the final model results are described with a high level of accuracy and detail, including specific metrics, performance evaluations, and any significant findings.\n",
        "    Include the **first page** where each subtopic begins in the \"pages used\" list. For example, if a subtopic spans from page 7 to page 10, only include page 7 in the result.\n",
        "    Ensure that detailed contents are extracted comprehensively from all relevant pages, leaving no key information behind. \n",
        "    Focus on capturing detailed content across all pages, ensuring that nothing is missed, especially in relation to the provided main topics.\n",
        "\n",
        "    Format the extracted information as follows:\n",
        "    <main>main topic</main>\n",
        "    <sub>subtopic</sub> <content>detailed contents</content> <page>3</page>\n",
        "    <sub>subtopic</sub> <content>detailed contents</content> <page>5</page>\n",
        "\n",
        "    <main>main topic</main>\n",
        "    <sub>subtopic</sub> <content>detailed contents</content> <page>10</page>\n",
        "    <sub>subtopic</sub> <content>detailed contents</content> <page>11</page>\n",
        "\n",
        "    Use the main topics provided below:\n",
        "    {main_topics}\n",
        "    \n",
        "    The text is divided into pages using the format <p.number>. For example, page 2 is marked as <p.2>. \n",
        "    Make sure to extract text from all pages except for the first five pages to ensure no information is missed.\n",
        "    If any part of the text seems to be related to the main topics but is not included in the main topics list(main_topics), include it as a subtopic under the appropriate main topic.\n",
        "    Do not include any additional notes or explanations in the output.\n",
        "\n",
        "    Text: {text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_4.invoke(prompt)\n",
        "    extracted_details = response.content.strip()\n",
        "    return extracted_details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv1r7VOaCs8O"
      },
      "source": [
        "### STEP3: 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w58Tuw5v-ofs",
        "outputId": "e9d576a6-6852-4885-8aeb-60e004e17a0c"
      },
      "outputs": [],
      "source": [
        "def summarize(content):\n",
        "    if not content.strip():\n",
        "        return content\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Do not change the language of the original input text. \n",
        "    Summarize in the same language as the input text.\n",
        "    The purpose of the summaries is to create a comprehensive README for a GitHub project.\n",
        "    If multiple subtopics within the same main topic have the same name, combine their content into a single entry with all relevant details, and merge the page numbers into a single list. \n",
        "    Ensure that no duplicate subtopics appear within the same main topic. \n",
        "    All content related to the same subtopic should be combined into one entry, regardless of how many times it appears.\n",
        "    Ensure that subtopics from different main topics are not merged.\n",
        "\n",
        "    Ensure that each summary accurately captures the key points and essential information of its respective partition.\n",
        "    Summarize in a way that is concise and informative, omitting repetitive mentions of the main topic after the first mention in each partition.\n",
        "    Focus on the key details and results relevant to the project, with particular emphasis on the final model outcomes. \n",
        "    Ensure that the final model results are described with a high level of accuracy and detail, including specific metrics, performance evaluations, and any significant findings.\n",
        "    For each subtopic, aim to provide a summary that is 1 to 2 sentences long. \n",
        "    Focus on delivering a concise yet comprehensive overview, capturing the main points and essential details without unnecessary elaboration.\n",
        "    When summarizing, only include the **first page** where each subtopic begins in the \"pages used\" list. For example, if a subtopic spans from page 7 to page 10, only include page 7 in the summary.\n",
        "    Use the example format as a reference only—do not include it in the final output. Ensure that <sub> and <content> tags are separated by a single space, not a line break, and that the output follows this format exactly.\n",
        "    Ensure the summary always begins with <subject>, <team>, and <index> in that order, before listing the main topics and content. \n",
        "\n",
        "    Here is an example of how the tagged text should look:\n",
        "\n",
        "    <subject>Korea</subject>\n",
        "    <team>이하나, 김하니</team>\n",
        "    <index>Seasons in Korea>\n",
        "    \n",
        "    <main>Seasons in Korea</main>\n",
        "    <sub>봄</sub> <content>한국의 봄은 3월에서 5월까지 지속되며, 온화한 기온과 함께 벚꽃이 만개하는 시기입니다. 이 시기에는 다양한 봄꽃 축제가 열리며, 사람들이 야외 활동을 즐기기에 좋은 날씨입니다.</content> <page>1</page>\n",
        "    <sub>여름</sub> <content>한국의 여름은 대개 매우 덥고 습하며, 기온이 종종 30°C를 넘습니다. 이 시기는 장마철이기도 해서 특히 7월에 많은 비가 내립니다. 더위에도 불구하고, 이 시기는 휴가철로 인기가 많아 많은 사람들이 해변과 리조트로 향합니다.</content> <page>3</page>\n",
        "    <sub>가을</sub> <content>가을은 한국에서 가장 아름다운 계절 중 하나로 꼽힙니다. 9월에서 11월 사이, 날씨는 시원하고 하늘은 맑으며, 단풍이 절정을 이루어 산과 들이 붉고 노란 색으로 물듭니다. 이 시기는 또한 수확의 계절로, 각종 축제가 열립니다.</content> <page>5</page>\n",
        "    <sub>겨울</sub> <content>한국의 겨울은 12월에 시작되며, 대부분의 지역에서 기온이 영하로 떨어집니다. 겨울철은 건조하며, 특히 산악 지역에서는 가끔씩 눈이 내립니다. 이 시기에는 스키나 얼음 낚시와 같은 겨울 스포츠가 인기를 끌고 있습니다.</content> <page>7</page>\n",
        "\n",
        "    <main>Animals in Korea</main>\n",
        "    <sub>봄</sub> <content>봄철에는 한국의 자연이 다시 깨어나면서 다양한 동물들이 활동을 시작합니다. 산과 숲에서는 새들이 짝을 찾기 위해 지저귀고, 개구리와 같은 양서류들이 물가에서 활동을 재개합니다. 또한, 겨울잠에서 깨어난 동물들이 활발히 먹이를 찾는 모습을 볼 수 있습니다.</content> <page>9</page>\n",
        "    <sub>여름</sub> <content>여름철에는 다양한 야생 동물들이 활발히 활동합니다. 특히, 한국의 산과 숲에서 멧돼지, 사슴, 다양한 종류의 새들이 많이 보입니다. 여름은 또한 번식기가 겹쳐 동물들이 더욱 활발히 움직이는 시기입니다.</content> <page>10</page>\n",
        "    <sub>가을</sub> <content>가을에는 동물들이 겨울을 준비하는 모습을 볼 수 있습니다. 많은 동물들이 겨울잠을 준비하기 위해 지방을 축적하고, 새들은 따뜻한 지역으로 이동하기 시작합니다. 이 시기는 또한 수확기가 겹쳐, 농작물에 접근하는 동물들이 많아집니다.</content> <page>11</page>\n",
        "    <sub>겨울</sub> <content>겨울철에는 많은 동물들이 추위를 피하기 위해 겨울잠에 들어갑니다. 겨울잠을 자지 않는 동물들은 추위를 이기기 위해 두꺼운 털을 기르거나 활동을 줄입니다. 특히, 한국의 산악지대에서는 고라니와 같은 동물들이 눈 속에서 식량을 찾아다니는 모습을 볼 수 있습니다.</content> <page>12</page>\n",
        "\n",
        "    \n",
        "    Text: {content}\n",
        "\n",
        "    Please summarize the above content concisely.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm_4.invoke(prompt)\n",
        "    summary = response.content.strip()\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\arima_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\asiancup_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\barbot_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\blind_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\braintumor_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\cartoon_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\disease_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\energy_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\hangang_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\insideout_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\interior_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\kospi_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\lier-detector_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\netflix_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\restaurant_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\trading_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\var_text.txt'.\n",
            "Summarized text has been saved to 'C:\\Users\\PC\\Desktop\\DoYoung\\DS\\github\\bitamin_auto_readme_generator\\data\\text_summarization\\output\\method1\\webtoon_text.txt'.\n"
          ]
        }
      ],
      "source": [
        "input_directory = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\object_detection\\\\output\\\\ocr_samples_txt'\n",
        "\n",
        "output_directory = 'C:\\\\Users\\\\PC\\\\Desktop\\\\DoYoung\\\\DS\\\\github\\\\bitamin_auto_readme_generator\\\\data\\\\text_summarization\\\\output\\\\method1'\n",
        "\n",
        "file_list = os.listdir(input_directory)\n",
        "\n",
        "for file_name in file_list:\n",
        "    if file_name.endswith('_text.txt'):  \n",
        "        file_path = os.path.join(input_directory, file_name)\n",
        "        \n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        text5 = extract_5pages(text, marker='<p.6>')\n",
        "        extracted_info = extract_info(text5)  \n",
        "        main_topics_list = extract_main_topics(extracted_info)\n",
        "        details = extract_details(text, main_topics_list)\n",
        "        final_text = summarize(details)\n",
        "\n",
        "        base_name = file_name.split('_text')[0]\n",
        "\n",
        "        output_file_path = os.path.join(output_directory, f'{base_name}_text.txt')\n",
        "\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.write(final_text)\n",
        "\n",
        "        print(f\"Summarized text has been saved to '{output_file_path}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
