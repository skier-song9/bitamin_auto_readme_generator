import openai
import json
import requests
from openai import OpenAI

import pandas as pd
import glob,os
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np
import pandas as pd
import torch
import re
import argparse
import stat
import math
import time

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import SpectralClustering,KMeans
from sklearn.metrics import silhouette_samples, silhouette_score, calinski_harabasz_score
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel, CanineTokenizer, CanineModel

import warnings
warnings.filterwarnings("ignore")

ROOT = 'bitamin_auto_readme_generator'
RANDOM_STATE = 142
PAGE_PATTERN = '<p.\d*>'
OPENAI_API_FILEPATH = "../assets/openai_api_key.json"

def split_by_pages(filepath, encoding='utf-8'):
    """
    filepath : ocr ê²°ê³¼ txt íŒŒì¼ ê²½ë¡œ ì „ë‹¬

    return : pageë³„ë¡œ êµ¬ë¶„ëœ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    """
    with open(filepath,'r',encoding=encoding) as f:
        data = f.readlines()
    text = []
    page_text = []
    for d in data:
        if re.compile(PAGE_PATTERN).match(d):
            if len(page_text)>0:
                text.append(' '.join(page_text))
            page_text = []
        page_text.append(d)
    text.append(''.join(page_text))
    return text

def erase_tag(text, tag):
    """
    text : split_by_pageë¡œ ì–»ì€ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” í…ìŠ¤íŠ¸
    tag : ì§€ìš°ê³  ì‹¶ì€ <tag>
    """
    tag_pattern = re.compile(f'<{tag}>|</{tag}>')
    if isinstance(text, list):
        text_ = [tag_pattern.sub('',x) for x in text]
        return text_
    else:
        text_ = tag_pattern.sub('',text)
        return text_

def extract_text_between_tag(text, tag):
    """
    text : split_by_pageë¡œ ì–»ì€ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    tag : <tag> ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
    """
    # Create a regex pattern for the specified tag
    pattern = f'<{tag}>(.*?)</{tag}>'
    # Use re.findall to extract all occurrences between the specified tags
    matches = re.findall(pattern, text, re.DOTALL)
    return matches
    
def extract_index_page(textlist, table_of_contents, threshold = 0.35):
    '''
    textlist : ì²« 5ê°œ ë¬¸ì¥ë§Œ ì „ë‹¬í•œë‹¤.
    table_of_contents : ì¶”ì¶œí•œ ëª©ì°¨ë¥¼ ì „ë‹¬

    return : textlistì—ì„œ ëª©ì°¨ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ì˜ indexë¥¼ ë°˜í™˜
    '''
    toc = ', '.join(table_of_contents)
    results = np.array([])
    for sentence in erase_tag(textlist,"p.\d*"):
        sentence = sentence.replace('\n','')
        vectorizer = TfidfVectorizer()
        vectors = vectorizer.fit_transform([toc,sentence])
        cosine_sim = cosine_similarity(vectors[0:1], vectors[1:2])
        results = np.append(results, cosine_sim)
    
    max_index = results.argmax()
    max_cs = results[max_index]
    if max_cs <= threshold:
        return results, 0
    return results, max_index

class GPT():
    __classname__ = "OpenAI"
    api_key = ''
    client = None    
    def __init__(self, api_filepath):
        with open(api_filepath,'r') as f:
            ak = json.load(f)
        self.api_key = ak['OPENAI_API_KEY']
        self.client = OpenAI(api_key=self.api_key)
        
    def get_chat_completion(self, msg, model='gpt-4o-mini', temperature = 0):
        response = self.client.chat.completions.create(
            model = model,
            messages = msg,
            temperature = temperature
        )
        return response.choices[0].message.content

    def get_embedding(self, sentence, model="text-embedding-3-small"):
       '''
       - pricing : text-embedding-3-small = $0.02/1M tokens
           í…ìŠ¤íŠ¸ê°€ ë§ì€ pdfëŠ” ëŒ€ëµ 5,000 tokens -> pdf 200ê°œì— 0.02 ë‹¬ëŸ¬(25~30ì›).
       text : í•œ ë¬¸ì¥
       return : í•œ ë¬¸ì¥ì— ëŒ€í•œ embedding (output dimension = 1536)
       '''
       return self.client.embeddings.create(input = sentence, model=model).data[0].embedding


class TextSummerizer():
    model_dict = dict()
    def __init__(self, gpt_client):
        ### Initiate Embedding Models
        # í•˜ë‚˜ì˜ sentenceë¥¼ í†µì§¸ë¡œ embeddingí•˜ëŠ” ëª¨ë¸ë“¤
        se_model = SentenceTransformer('sentence-transformers/LaBSE') # BERT ê¸°ë°˜ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸
        se_model.__classname__ = "LaBSE"
        gist_model = SentenceTransformer("avsolatorio/GIST-Embedding-v0", 
                                    revision=None)
        gist_model.__classname__ = "GIST-Embedding-v0" 
        self.gpt = gpt_client
        # tokenizerë¡œ í† í°í™” í›„ embeddingí•˜ëŠ” ëª¨ë¸ë“¤
        convbert_tokenizer = AutoTokenizer.from_pretrained("YituTech/conv-bert-base")
        convbert_model = AutoModel.from_pretrained("YituTech/conv-bert-base")
        convbert_model.__classname__ = "ConvBERT" 
        canine_tokenizer = CanineTokenizer.from_pretrained('google/canine-c')
        canine_model = CanineModel.from_pretrained('google/canine-c')
        canine_model.__classname__ = "Canine-C"
        self.model_dict = {
            'sentence':[se_model, gist_model, self.gpt],
            'token':[(convbert_tokenizer,convbert_model),(canine_tokenizer,canine_model)]
        }

    def pca_best_component(self, x):
        '''
        ìµœì ì˜ PCA componentê°’ì„ ì°¾ëŠ” í•¨ìˆ˜
        '''
        pca_optimize = PCA()
        pca_optimize.fit(x)
        # ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ ê³„ì‚°
        cumulative_variance = np.cumsum(pca_optimize.explained_variance_ratio_)
        # 99% ì´ìƒ ì„¤ëª…ë ¥ì„ ê°–ëŠ” ì£¼ì„±ë¶„ ê°œìˆ˜ ê³„ì‚°
        n_components = np.argmax(cumulative_variance >= 0.99) + 1
        return n_components

    def find_optimal_k(self, max_cluster, x):     
        '''
        K-means, Spectral clusteringì—ì„œ ìµœì ì˜ Kê°’ì„ ì°¾ëŠ” í•¨ìˆ˜
        '''
        # clusterì˜ ìµœì†Ÿê°’ì€ 3ìœ¼ë¡œ ê³ ì •í•˜ê³  clusterlistë¥¼ ë§Œë“ ë‹¤.
        min_cluster = 4
        cluster_lists = [x for x in range(min_cluster,max_cluster)]
        
        # ì…ë ¥ê°’ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ê°¯ìˆ˜ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì„œ, ê° ê°¯ìˆ˜ë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ì ìš©í•˜ê³  ì‹¤ë£¨ì—£ ê°œìˆ˜ë¥¼ êµ¬í•¨
        n_cols = len(cluster_lists)
        
        results = []
        # ë¦¬ìŠ¤íŠ¸ì— ê¸°ì¬ëœ í´ëŸ¬ìŠ¤í„°ë§ ê°¯ìˆ˜ë“¤ì„ ì°¨ë¡€ë¡œ iteration ìˆ˜í–‰í•˜ë©´ì„œ ì‹¤ë£¨ì—£ ê°œìˆ˜ ì‹œê°í™”
        for ind, n_cluster in enumerate(cluster_lists):
            
            # KMeans í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰í•˜ê³ , ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ì™€ ê°œë³„ ë°ì´í„°ì˜ ì‹¤ë£¨ì—£ ê°’ ê³„ì‚°. 
            clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0)
            cluster_labels = clusterer.fit_predict(x)
            
            sil_avg = silhouette_score(x, cluster_labels)
            sil_values = silhouette_samples(x, cluster_labels)
            results.append([sil_avg, np.var([x if x>=0 else 0 for x in sil_values])])
    
        ### ê²°ê³¼ë¥¼ í† ëŒ€ë¡œ ìµœì ì˜ K ê°’ì„ ë„ì¶œ
        # 1. ì‹¤ë£¨ì—£ ê³„ìˆ˜ì˜ í‰ê· ê³¼ ë¶„ì‚°ì— ëŒ€í•´ MinMax ì •ê·œí™”
        results_df = pd.DataFrame(data=results,columns=['avg','var']) 
        mms = MinMaxScaler()
        results_df[['avg_norm', 'var_norm']] = mms.fit_transform(results_df[['avg', 'var']])
        # 2. í‘œì¤€í™”ëœ varì„ 1ì—ì„œ ë¹¼ì„œ "ë¶„ì‚°ì€ ì‘ì€ ê°’ì´ ì¢‹ìŒ"ì„ ë°˜ì˜
        results_df['var_norm'] = 1 - results_df['var_norm']
        # 3. ê°€ì¤‘ í‰ê·  ê³„ì‚° (í‰ê· ì— ëŒ€í•œ ê°€ì¤‘ì¹˜: 0.65, ë¶„ì‚°ì— ëŒ€í•œ ê°€ì¤‘ì¹˜: 0.35 -> ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê·¼ê±°ëŠ” ê²½í—˜ì  íŒë‹¨.)
        results_df['score'] = results_df['avg_norm'] * 0.65 + results_df['var_norm'] * 0.35    
        sorted_results = results_df.sort_values('score',ascending=False)
        optimal_k = sorted_results.index[0]+min_cluster
        return optimal_k

    def clustering(self, text, n_clusters, random_state=RANDOM_STATE):
        '''
        text : pdf ë¬¸ì„œë¥¼ pageë³„ë¡œ splití•˜ê³ , <p>íƒœê·¸ì™€ \n ë¬¸ìì—´ì„ ì—†ì•¤ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬.(=textlist)
        n_clusters : í´ëŸ¬ìŠ¤í„°ì˜ ê°œìˆ˜ë¥¼ ì§€ì • -> extract_text_between_tag(text,'index')ë¡œ ì¶”ì¶œí•œ ì¸ë±ìŠ¤ì˜ ê°œìˆ˜ + 2
    
        return
            - total_metrics : ëª¨ë¸ë³„, í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ë³„ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ SS, CHIë¡œ í‰ê°€ì§€í‘œë¡œ ì¸¡ì •í•œ ê²°ê³¼ì— ëŒ€í•œ ë°ì´í„°í”„ë ˆì„
            - total_dict : model_nameì„ keyê°’ìœ¼ë¡œ text, cluster label, embedding ì»¬ëŸ¼ìœ¼ë¡œ í•˜ëŠ” ë°ì´í„°í”„ë ˆì„ì„ valueë¡œ ê°–ëŠ” ë”•ì…”ë„ˆë¦¬
        '''
        total_metrics = pd.DataFrame(columns=['model','kmeans_ss','spectral_ss','kmeans_chi','spectral_chi'])
        total_dict = {}
        max_cluster = len(text)-1
        # sentence ë‹¨ìœ„ë¡œ ì„ë² ë”©í•˜ëŠ” ëª¨ë¸ì˜ ê²½ìš°
        print("--Sentence Embedding Methods--")
        for model in self.model_dict['sentence']:
            model_name = model.__classname__
            if model_name == 'Encoding':
                text_embeddings = [model.encode(t) for t in text]
            elif model_name in set(['LaBSE','GIST-Embedding-v0']):
                text_embeddings = model.encode(text, convert_to_tensor=True)
                text_embeddings = text_embeddings.cpu().detach()
            elif model_name == 'OpenAI':
                text_embeddings = []
                for t in text:
                    embedding = model.get_embedding(t)
                    text_embeddings.append(embedding)
                text_embeddings = np.array(text_embeddings)
            else:
                text_embeddings = model.encode(text)
            ### ì°¨ì› ì¶•ì†Œ : PCA + t-SNE
            # PCA n_component êµ¬í•˜ê¸°
            optimal_component = self.pca_best_component(text_embeddings)
            # PCA
            text_embeddings = PCA(n_components=optimal_component, random_state=random_state).fit_transform(text_embeddings)
            # t-SNE (3ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒìœ¼ë¡œ ê³ ì •)
            # perplexityëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„° ê°œìˆ˜ì˜ 3ë¶„ì˜ 1 ì´í•˜
            general_perplexity = int(text_embeddings.shape[0]/3)
            # ìµœì†Ÿê°’ì€ 5, ìµœëŒ“ê°’ì€ 50ìœ¼ë¡œ ì œí•œ
            if general_perplexity<5:
                general_perplexity = 5
            elif general_perplexity>50:
                general_perplexity=50
            text_embeddings = TSNE(n_components=3, perplexity=general_perplexity,
                                   random_state=random_state).fit_transform(text_embeddings)
            # ìµœì ì˜ kê°’ ì°¾ê¸° [x] >> pdfì˜ ì¸ë±ìŠ¤ ê°œìˆ˜ì— ë§ì¶° n_clustersë¥¼ ì„¤ì •
            # optimal_k = find_optimal_k(max_cluster=max_cluster,x=text_embeddings)
            # kmeansì™€ spectral clustering ì§„í–‰
            kmeans = KMeans(n_clusters=n_clusters, 
                    init='k-means++', # centroidë“¤ì„ ì„œë¡œ ìµœëŒ€í•œ ë©€ë¦¬ ë°°ì¹˜í•˜ëŠ” initialisation ë°©ì‹
                    max_iter = 500,
                    random_state = random_state)
            spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors',
                                 random_state=random_state)
            kmeans.fit(text_embeddings)
            spectral.fit(text_embeddings)
            # í‰ê°€ì§€í‘œ ê³„ì‚°
            new_row = pd.DataFrame(data=[[
                model_name, # model name
                silhouette_score(text_embeddings, kmeans.labels_), # kmeans ss
                silhouette_score(text_embeddings, spectral.labels_), # spectral ss
                calinski_harabasz_score(text_embeddings, kmeans.labels_), # kmeans chi
                calinski_harabasz_score(text_embeddings, spectral.labels_) # spectral chi
            ]], columns = total_metrics.columns)
    
            # Save results
            total_metrics = pd.concat([total_metrics, new_row],axis=0,ignore_index=True)
            temp_df = pd.DataFrame(data=[
                text, kmeans.labels_, spectral.labels_
            ]).transpose()
            temp_df.columns = ['text','kmeans','spectral']
            total_dict[model_name] = pd.concat([
                temp_df,pd.DataFrame(text_embeddings)
            ],axis=1)
    
            # garbage collection > ë³„ ì˜ë¯¸ ì—†ëŠ” ë“¯
            # gc.collect()
    
        # token ë‹¨ìœ„ë¡œ ì„ë² ë”©í•˜ëŠ” ëª¨ë¸ì˜ ê²½ìš°
        print("--Token Embedding Methods--")
        for tokenizer, model in self.model_dict['token']:
            model_name = model.__classname__
            if model_name == 'Canine-C':
                token = tokenizer(erase_tag(text,'p.\d*'), padding='longest', truncation=True, return_tensors='pt')
            else: # ConvBERT
                token = tokenizer(erase_tag(text,'p.\d*'), padding='longest',return_tensors='pt')
            text_embeddings = model(**token).last_hidden_state.detach()
            flattened = text_embeddings.view(text_embeddings.shape[0],-1)
            ### ì°¨ì› ì¶•ì†Œ : PCA + t-SNE
            # PCA n_component êµ¬í•˜ê¸°
            optimal_component = self.pca_best_component(flattened)
            # PCA
            text_embeddings = PCA(n_components=optimal_component, random_state=random_state).fit_transform(flattened)
            # t-SNE (3ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒìœ¼ë¡œ ê³ ì •)
            # perplexityëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„° ê°œìˆ˜ì˜ 3ë¶„ì˜ 1 ì´í•˜
            general_perplexity = int(text_embeddings.shape[0]/3)
            # ìµœì†Ÿê°’ì€ 5, ìµœëŒ“ê°’ì€ 50ìœ¼ë¡œ ì œí•œ
            if general_perplexity<5:
                general_perplexity = 5
            elif general_perplexity>50:
                general_perplexity=50
            text_embeddings = TSNE(n_components=3, perplexity=general_perplexity,
                                   random_state=random_state).fit_transform(text_embeddings)
    
            # ìµœì ì˜ kê°’ ì°¾ê¸° [x] >> pdfì˜ ì¸ë±ìŠ¤ ê°œìˆ˜ì— ë§ì¶° n_clustersë¥¼ ì„¤ì •
            # optimal_k = find_optimal_k(max_cluster=max_cluster,x=text_embeddings)
            # kmeansì™€ spectral clustering ì§„í–‰
            kmeans = KMeans(n_clusters=n_clusters, 
                    init='k-means++', # centroidë“¤ì„ ì„œë¡œ ìµœëŒ€í•œ ë©€ë¦¬ ë°°ì¹˜í•˜ëŠ” initialisation ë°©ì‹
                    max_iter = 500,
                    random_state = random_state)
            spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors',
                                 random_state=random_state)
            kmeans.fit(text_embeddings)
            spectral.fit(text_embeddings)
            # í‰ê°€ì§€í‘œ ê³„ì‚°
            new_row = pd.DataFrame(data=[[
                model_name, # model name
                silhouette_score(text_embeddings, kmeans.labels_), # kmeans ss
                silhouette_score(text_embeddings, spectral.labels_), # spectral ss
                calinski_harabasz_score(text_embeddings, kmeans.labels_), # kmeans chi
                calinski_harabasz_score(text_embeddings, spectral.labels_) # spectral chi
            ]], columns = total_metrics.columns)
    
            # Save results
            total_metrics = pd.concat([total_metrics, new_row],axis=0,ignore_index=True)
            temp_df = pd.DataFrame(data=[
                text, kmeans.labels_, spectral.labels_
            ]).transpose()
            temp_df.columns = ['text','kmeans','spectral']
            total_dict[model_name] = pd.concat([
                temp_df,pd.DataFrame(text_embeddings)
            ],axis=1)
    
            # garbage collection
            # gc.collect()
    
        return total_metrics, total_dict

    def best_model_n_algo(self, metrics_df):
        """
        metrics_df : clusteringí•¨ìˆ˜ ê²°ê³¼ë¡œ ì–»ì€ total_metrics
    
        return : total_metricsì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ modelê³¼ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì„ ë°˜í™˜ 
        -> total_dictì—ì„œ í•´ë‹¹ ëª¨ë¸ ì´ë¦„ê³¼ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ.
        """
        metrics_df['kmeans']=metrics_df['kmeans_ss']+metrics_df['kmeans_chi']
        metrics_df['spectral']=metrics_df['spectral_ss']+metrics_df['spectral_chi']
        
        max_value = metrics_df[['spectral','kmeans']].max().max()
        # ìµœëŒ€ê°’ì„ ê°€ì§„ í–‰ê³¼ ì—´ ì°¾ê¸°
        max_value_row_col = metrics_df[['spectral', 'kmeans']].apply(lambda x: x == max_value).stack()
        max_value_row_col = max_value_row_col[max_value_row_col].index[0]
        model_name = metrics_df.loc[max_value_row_col[0],'model']
        clustering_algo = max_value_row_col[1]
        return model_name, clustering_algo

    def renew_cluster_byorder(self, total_dict, best):
        '''
        total_dict : clustering í•¨ìˆ˜ ë°˜í™˜ê°’ì¸ total_dict
        best = best_model_n_algo í•¨ìˆ˜ ë°˜í™˜ê°’ì¸ (best_model, best_algo) íŠœí”Œ
            - best_model : best_model_n_algo ê²°ê³¼ë¡œ ì–»ì€ model_name
            - best_algo : best_model_n_algo ê²°ê³¼ë¡œ ì–»ì€ clustering_algo
    
        return df : ìˆœì„œê°€ ì—‰ì¼œìˆëŠ” cluster labelì„ ì¬ì •ë ¬í•œ ê²°ê³¼ë¥¼ ë°˜í™˜
        '''
        best_model, best_algo = best
        df = total_dict[best_model][['text',best_algo]]
        cnum = len(df[best_algo].unique())
        checklist = []
        for c in df[best_algo]:
            if c not in checklist:
                checklist.append(c)
            if len(checklist) == cnum:
                break
        renew_c = list(range(cnum))
        df['new_cluster'] = df[best_algo].apply(lambda x:renew_c[checklist.index(x)])
        return df


if __name__ == '__main__':
    # ë§¤ê°œë³€ìˆ˜ ë°›ê¸°
    parser = argparse.ArgumentParser()
    parser.add_argument('--input') # ocr_samples_txt ì „ë‹¬í•˜ê¸°
    parser.add_argument('--output') # cluster_n_summary ì „ë‹¬í•˜ê¸°
    parser.add_argument('--temperature')
    # ì¸ì íŒŒì‹±
    args = parser.parse_args()

    root_absdir = os.getcwd().split(ROOT)[0]+ROOT
    input_dir = os.path.join(root_absdir,'data','object_detection','output',args.input)
    output_dir = os.path.join(root_absdir,'data','text_summarization','output',args.output)

    # start logic
    gpt = GPT(api_filepath=OPENAI_API_FILEPATH)
    text_summarization = TextSummerizer(gpt_client=gpt)
    
    for file in os.listdir(input_dir):
        print(file,"work started.")
        start_time = time.time()
        filepath = os.path.join(input_dir, file)
        # load text
        textlist = split_by_pages(filepath=filepath, encoding='utf-8')
        # ì²« 5í˜ì´ì§€, ë§ˆì§€ë§‰ 3í˜ì´ì§€ë¡œë¶€í„° "ì£¼ì œ, íŒ€ì›, ëª©ì°¨" ì¶”ì¶œí•˜ê¸°
        msg = [
            {"role": "system", 
             "content": """Refer to the document provided between the triple quotes. This document is a text conversion of a PDF file and is structured into passages separated by '\n'. Each passage corresponds to a different textbox in the PDF. Note that repeated passages are considered redundant and should be ignored in your response.

Perform the following tasks:
1. **Identify the main subject of the document**: Select the passage that best represents the overall theme or topic of the document. If multiple passages are selected, combine them together.
2. **Extract names of individuals**: Identify and list all names mentioned in the document, separated by commas.
3. **Extract the table of contents (index)**: Identify and list the contents or sections of the document, separated by commas.

Respond in the following format:
<subject>Result of Task 1</subject>
<team>Result of Task 2</team>
<index>Result of Task 3</index>

If any result is unclear or not found, indicate it as None.
             """
            },
            {"role": "user", 
             "content": f'"""{" ".join(textlist[:5]) + " ".join(textlist[-3:])}"""'
            }
        ]
        sub_team_index = gpt.get_chat_completion(msg, model='gpt-4o-mini')
        # ëª©ì°¨ ê°œìˆ˜ ì¶”ì¶œ -> ëª©ì°¨ ê°œìˆ˜ë¥¼ cluster ê°œìˆ˜ë¡œ ì„¤ì •
        table_of_contents = extract_text_between_tag(sub_team_index,'index')[0].split(',')
        nclusters = len(table_of_contents) 
        # ëª©ì°¨ ì´í›„ì˜ í˜ì´ì§€ë§Œ í´ëŸ¬ìŠ¤í„°ë§í•˜ê¸°
        _, index_page = extract_index_page(textlist[:5], table_of_contents, threshold=0.35)
        textlist = textlist[index_page+1:] # ëª©ì°¨ í˜ì´ì§€ ì´í›„ë¶€í„° í´ëŸ¬ìŠ¤í„°ë§
        # í…ìŠ¤íŠ¸ì—ì„œ page tagì™€ \n ë¬¸ìì—´ì„ ì œê±°
        textlist_preprocessed = [text.replace('\n',' ') for text in erase_tag(textlist,'p.\d*')]
        # clustering
        print("Clustering Started.")
        c_metrics, c_dfs = text_summarization.clustering(text=textlist_preprocessed,
                                     n_clusters=nclusters, random_state=RANDOM_STATE)
        print("âœ…Clustering Finished.")
        # ì„±ëŠ¥ì´ ì œì¼ ì¢‹ì€ ì„ë² ë”© ëª¨ë¸ê³¼ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
        best_model, best_algo = text_summarization.best_model_n_algo(c_metrics)
        # del c_metrics # ë”ì´ìƒ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜
        # cluster label ì¬ì •ë ¬
        c_renew = text_summarization.renew_cluster_byorder(total_dict=c_dfs,
                                                           best=(best_model,best_algo))
        # del c_dfs # ë”ì´ìƒ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜

        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ í…ìŠ¤íŠ¸ ë¬¶ê¸°
        c_dict = dict(zip(list(range(nclusters)),['' for _ in range(nclusters)]))
        for idx in c_renew.index:
            new_cluster = c_renew.iloc[idx]['new_cluster']
            c_dict[new_cluster] += c_renew.iloc[idx]['text']

        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ í…ìŠ¤íŠ¸ ìš”ì•½
        i = 0
        summarizations = []
        for key in range(nclusters): # ëª©ì°¨ì— í•´ë‹¹í•˜ëŠ” clusterë§Œ ìš”ì•½í•¨.
            table_of_content = table_of_contents[i]
            i += 1
            ctext = c_dict[key]
            msg = [
                {"role": "system", 
                 "content": f"""Your task is to 'Summarize' the given text which is in triple quote with a focus on the key points related to '{table_of_content}'. Extract and summarize the content for each relevant subtitle. Answer in Korean and follow the format provided below.

<main>{table_of_content}</main>
<subtitle>[Extracted Subtitle]</subtitle>
<content>Summary of the content under this subtitle</content>

Instructions:
1. Identify and extract subtitles from the text that relate to '{table_of_content}'.
2. Summarize the content under each subtitle within one or two sentences, focusing on the most important details and main ideas.
3. Provide your summary in Korean, maintaining clarity and coherence.
4. Use the specified format for organizing your response.
                """
                },
                {"role": "user", 
                 "content": f'"""{ctext}"""'
                }
            ]
            summarizations.append(gpt.get_chat_completion(msg, model='gpt-4o-mini', temperature=int(args.temperature)))
        
        output_filepath = os.path.join(output_dir, file)
        with open(output_filepath,'w', encoding='utf-8') as f:
            f.write(sub_team_index)
            for s in summarizations:
                f.write('\n')
                f.write(s)
        print(f"ğŸ“¢ {file} finished in {time.time()-start_time:0.1f} seconds")
    # end for
    

    
    
    
    