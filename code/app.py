from flask import Flask
from flask import request
from flask import render_template #í…œí”Œë¦¿ íŒŒì¼(.html) ì„œë¹„ìŠ¤ìš©
# from flask import make_response #ì‘ë‹µ ê°ì²´(Response) ìƒì„±ìš©
# from flask import flash, redirect #ì‘ë‹µë©”ì‹œì§€ ë° ë¦¬ë‹¤ì´ë ‰íŠ¸ìš©
from flask import jsonify #JSONí˜•íƒœì˜ ë¬¸ìì—´ë¡œ ì‘ë‹µì‹œ
from flask_cors import CORS
import os, re, regex, time
import threading
import pandas as pd
from paddleocr import PaddleOCR

# import custom modules
from object_detection import detection, pdf2img
from image_classification import image_classifier
from text_summarization import textsumm_method3

# ì›¹ ì•± ìƒì„±
app = Flask(__name__,
            template_folder=os.path.join(os.getcwd(),'webapp'),
            static_folder=os.path.join(os.getcwd(),'webapp'))
# appì€ WSGIì–´í”Œë¦¬ì¼€ì´ì…˜(Web Server Gateway Interface)ë¥¼ ë”°ë¥´ëŠ” ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ì´ë‹¤.

#CORSì—ëŸ¬ ì²˜ë¦¬
CORS(app)

#ë¸Œë¼ìš°ì €ë¡œ ë°”ë¡œ JSON ì‘ë‹µì‹œ í•œê¸€ ì²˜ë¦¬(16ì§„ìˆ˜ë¡œ URLì¸ì½”ë”©ë˜ëŠ” ê±° ë§‰ê¸°)
#Responseê°ì²´ë¡œ ì‘ë‹µì‹œëŠ” ìƒëµ(ë‚´ë¶€ì ìœ¼ë¡œ utf8ì„ ì‚¬ìš©)
app.config['JSON_AS_ASCII']=False
#ìµœëŒ€ íŒŒì¼ ì—…ë¡œë“œ ìš©ëŸ‰ 50MBë¡œ ì„¤ì •
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024

# serverì˜ ROOT URL
APP_ROOT = os.getcwd() # bitamin_auto_readme_generator/code/

# í•„ìš” ë³€ìˆ˜ë“¤
ASSETS = os.path.join(APP_ROOT,'assets')
WORKSPACE = os.path.join(APP_ROOT,'webapp','workspace')
WORKFILEORIGIN = '' # .pdf í™•ì¥ìê¹Œì§€ í¬í•¨í•˜ëŠ” ì›ë³¸ íŒŒì¼ëª…
WORKFILECLEAN = '' # í™•ì¥ì/ê³µë°±/íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•œ íŒŒì¼ëª… + timestamp
WORKFILEHASH = '' # workfileoriginì„ hashingí•œ ê°’.
WORKIMAGEDIR = '' # images_WORKFILECLEAN : íŒŒì¼ì„ ì²˜ë¦¬í•´ì„œ ë‚˜ì˜¨ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  í´ë” ê²½ë¡œ

# # image detector class ì¤€ë¹„
img_det = detection.Image_Detector(os.path.join(ASSETS, 'best.pt'))
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
ocr = PaddleOCR(lang='korean', )
# # image classifier class ì¤€ë¹„
img_clf = image_classifier.Image_Classifier(os.path.join(ASSETS, 'VGG19clf_acc94_0728.pth'))
# # text summarizer class ì¤€ë¹„
txt_summ = textsumm_method3.TextSummarizer(api_key_path=os.path.join(ASSETS, 'openai_api_key.json'))

# í•„ìš” í•¨ìˆ˜ë“¤
def clean_filename(text):
    # 1. í™•ì¥ì ì œê±° (ë§ˆì§€ë§‰ ì (.) ì´í›„ì˜ ëª¨ë“  ë¬¸ì ì œê±°)
    text = re.sub(r'\.[^.]*$', '', text)
    # 2. ê³µë°± ëŒ€ì²´ (ëª¨ë“  ê³µë°± ì–¸ë”ë°”ë¡œ ëŒ€ì²´)
    text = re.sub(r'\s+', '_', text)
    # 3. íŠ¹ìˆ˜ë¬¸ì ì œê±° (ëª¨ë“  ë‚˜ë¼ì˜ ì–¸ì–´ëŠ” ì œì™¸)
    text = regex.sub(r'[^\p{L}\p{N}0-9]', '', text)
    return text

def process_object_detection():
    '''
    {params}
    pdf_filepath : ì²˜ë¦¬í•  pdfíŒŒì¼ì˜ ê²½ë¡œ > ë°”ë¡œ open(pdf_filepath,'r') í•´ì„œ ì‚¬ìš©í•˜ë©´ ë¨.

    {ì‘ì—…}
    1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pdf íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•œ "í…ìŠ¤íŠ¸ íŒŒì¼ì˜ ê²½ë¡œ" >> WORKSPACE ë³€ìˆ˜ë¡œ ì§€ì •ëœ ë””ë ‰í† ë¦¬ì— ì €ì¥í•˜ë©´ ë¨.
        >> í…ìŠ¤íŠ¸ íŒŒì¼ ì´ë¦„ì€ WORKFILECLEANìœ¼ë¡œ ì„¤ì •í•˜ë©´ ë¨.
        => ì „ì²´ ê²½ë¡œ = os.path.join(WORKSPACE,'WORKFILECLEAN'+'.txt')

    2. ì´ë¯¸ì§€ ì¶”ì¶œ
        pdf íŒŒì¼ì—ì„œ ì „ì²´ ì´ë¯¸ì§€ë“¤ì„ ì¶”ì¶œí•˜ì—¬ WORKIMAGEDIR ê²½ë¡œì— ì €ì¥í•˜ë©´ ë¨.
        >> WORKIMAGEDIRì´ í´ë” ì „ì²´ ê²½ë¡œë‹ˆê¹Œ os.path.join(WORKIMAGEDIR,ì´ë¯¸ì§€íŒŒì¼ì´ë¦„) ìœ¼ë¡œ ë°”ë¡œ ì €ì¥í•˜ë©´ ë¨.
    3. ì¢…ë£Œ.
    '''
    global WORKSPACE
    global WORKFILEORIGIN
    global WORKFILECLEAN
    global WORKIMAGEDIR
    global img_det
    global ocr
    pdf_filepath = os.path.join(WORKSPACE,WORKFILEORIGIN)
    # í˜ì´ì§€ë³„ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ >> page imageê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ì´ë¦„ ë°˜í™˜ > WORKFILECLEAN
    pdf2img_dirname = pdf2img.convert_and_rename_pdf(input_filepath=pdf_filepath,
                                   workfileorigin=WORKFILEORIGIN,
                                   workfileclean=WORKFILECLEAN)
    # image detector ì‹¤í–‰ >> textbox imageë“¤ì´ ì €ì¥ëœ ê²½ë¡œ ë°˜í™˜ /data/object_detection/output/WORKFILECLEAN/
    textbox_dirpath = img_det.predict(pdf_name=pdf2img_dirname,  save_image_dir=WORKIMAGEDIR, save=True)

    # OCR
    textbox_filenames = os.listdir(textbox_dirpath)
    textbox_filenames.sort()
    page_texts = {}
    for filename in textbox_filenames:
        parts = filename.split('_')
        if len(parts)>=4:
            page_number = parts[2]
        else:
            continue
        textbox_imagepath = os.path.join(textbox_dirpath, filename)
        result = ocr.ocr(textbox_imagepath, cls=False)[0]
        text = ''
        for r in result:
            text += ' '
            text += r[1][0]
        if page_number not in page_texts:
            page_texts[page_number]=[]
        page_texts[page_number].append(text)
    txt_filepath = os.path.join(WORKSPACE, WORKFILECLEAN+'.txt')
    if os.path.exists(txt_filepath):
        os.remove(txt_filepath)
    with open(txt_filepath, 'w', encoding='utf-8') as f:
        for page_number in sorted(page_texts.keys()):
            f.write(f"<p.{page_number}>\n")
            for text in page_texts[page_number]:
                f.write(text+'\n')

def process_image_classification():
    '''
    {ì‘ì—…}
    1. WORKIMAGEDIR ë‚´ì— ìˆëŠ” ì´ë¯¸ì§€ë“¤ì„ ë¶„ë¥˜í•˜ì—¬ ê²°ê³¼ë¥¼ dataFrameìœ¼ë¡œ ì„ì‹œ ì €ì¥
    2. dataFrameì—ì„œ classê°€ 'none'ìœ¼ë¡œ ë¶„ë¥˜ëœ ì´ë¯¸ì§€ë“¤ì€ WORKIMAGEDIRì—ì„œ ì œê±°
    3. ì¢…ë£Œ.
    '''
    global WORKIMAGEDIR
    global img_clf
    print('start image classification')
    # ì¶”ë¡ 
    img_clf_result_df, _ = img_clf.predict(WORKIMAGEDIR, save=False) # ê²°ê³¼ dfë¥¼ csvíŒŒì¼ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ.
    for _, row in img_clf_result_df.iterrows():
        if row['class'] == 'none':
            img_filepath = os.path.join(WORKIMAGEDIR, row['filename'])
            if os.path.exists(img_filepath):
                os.remove(img_filepath)
            else:
                raise OSError(f"{row['filename']} File doesn't exists")

def process_text_summarization():
    '''
    {ì‘ì—…}
    1. txt_filepath ê²½ë¡œì— object_detectionì˜ 1ë²ˆ ì‘ì—… ê²°ê³¼ë¬¼ì´ ìˆìŒ. ê·¸ê±° ì½ì–´ì˜¤ê¸°
    2. text summarization ì‘ì—… ìˆ˜í–‰
    3. txt to markdown ì‘ì—… ìˆ˜í–‰
    4. os.path.join(WORKSPACE,'WORKFILECLEAN'+'.md') ê²½ë¡œë¡œ markdown íŒŒì¼ ì €ì¥.
    5. ì¢…ë£Œ.
    '''
    global WORKSPACE
    global WORKFILECLEAN
    print('start text summarization')
    txt_filepath = os.path.join(WORKSPACE, WORKFILECLEAN+'.txt')
    if not os.path.exists(txt_filepath):
        raise OSError(f"{txt_filepath} File doesn't exists")
    with open(txt_filepath, 'r', encoding='utf-8') as f:
        text = f.read()
    text5 = txt_summ.extract_5pages(text)
    extracted_info, main_topics_list = txt_summ.extract_info(text5)
    cleaned_text = txt_summ.remove_page(text)
    divided_text = txt_summ.divide_text(extracted_info, main_topics_list, cleaned_text)
    summarized_text = txt_summ.summarize(divided_text)
    tagged_text = txt_summ.tag_text(summarized_text)

    print('readme formatting')
    # # readme formatting
    main_re = re.compile(r'<main>(.*?)</main>', re.DOTALL)
    sub_re = re.compile(r'<sub>(.*?)</sub>', re.DOTALL)
    content_re = re.compile(r'<content>(.*?)</content>', re.DOTALL)
    page_re = re.compile(r'<page>(.*?)</page>', re.DOTALL)

    # Find all occurrences of each tag
    subject = tagged_text.split("<subject>")[1].split("</subject>")[0].strip()
    team = tagged_text.split("<team>")[1].split("</team>")[0].strip().split(", ")
    index = tagged_text.split("<index>")[1].split("</index>")[0].strip().split(", ")
    mains = main_re.findall(tagged_text)

    # Convert to Markdown format
    readme_md = f"""
    # {subject}
    (í”„ë¡œì íŠ¸ ì§„í–‰ê¸°ê°„ì„ ì…ë ¥í•˜ì„¸ìš”. ####.##.## ~ ####.##.##)
    ### Team
    {', '.join(team)}

    ## Table of Contents
    """
    for i, section in enumerate(index, 1):
        readme_md += f"- [{section}](#section_{i})\n"
    readme_md += "<br>\n"
    for idx, main in enumerate(mains):
        main_text = main.strip()
        readme_md += f"<a name='section_{idx + 1}'></a>\n\n## {main_text}\n\n"
        section_content = tagged_text.split(f"<main>{main_text}</main>")[1].split("<main>")[0]

        subs = sub_re.findall(section_content)
        contents = content_re.findall(section_content)
        pages = page_re.findall(section_content)

        # Add the subsections and content
        for sub, content_text, page in zip(subs, contents, pages):
            readme_md += f"#### {sub.strip()}\n\n"
            readme_md += f"- {content_text.strip()}\n\n"
            # readme_md += f"*Page: {page.strip()}*\n\n"

    readme_filepath = os.path.join(WORKSPACE, WORKFILECLEAN+'.md')
    if os.path.exists(readme_filepath):
        os.remove(readme_filepath)
    with open(readme_filepath, 'w', encoding='utf-8') as f:
        f.write(readme_md)

@app.route('/')
def index():
    return render_template('editormd.html')

@app.route('/loadimages.do', methods=['GET'])
def loadImageGallery():
    images_list = os.listdir(WORKIMAGEDIR)
    return images_list

# upload ì²˜ë¦¬
@app.route('/upload.do', methods=['POST']) # ë‹¨ì¼ íŒŒì¼ ì—…ë¡œë“œ
def fileUpload():
    global WORKFILEORIGIN
    global WORKFILECLEAN
    global WORKIMAGEDIR
    global WORKFILEHASH
    try:
        # íŒŒì¼ì€ request.files['íŒŒë¼ë¯¸í„°ëª…']ìœ¼ë¡œ ë°›ê¸°
        files = request.files['file']
        # print(files)
        if files.filename.split('.')[-1] != 'pdf':
            return jsonify({'code':400,'msg':'Not PDF file!'})
        # ì„œë²„ì— íŒŒì¼ ì €ì¥ : save('ì„œë²„ì— ì €ì¥í•  íŒŒì¼ì˜ ì „ì²´ê²½ë¡œ')
        files.save(os.path.join(WORKSPACE,files.filename))  # >> upload ë””ë ‰í† ë¦¬ ë°‘ì— íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥í•¨.
        # í•´ë‹¹ íŒŒì¼ëª…ìœ¼ë¡œ image í´ë” ìƒì„±
        # íŒŒì¼ëª…ì—ì„œ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        WORKFILEORIGIN = files.filename  # ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ pdf íŒŒì¼ ì´ë¦„ ì›ë³¸ (í™•ì¥ì í¬í•¨)
        timestamp = str(int(time.time()))
        WORKFILECLEAN = clean_filename(WORKFILEORIGIN)+f"_{timestamp}"  # WORKFILEORIGINì—ì„œ ê³µë°±/íŠ¹ìˆ˜ë¬¸ì/í™•ì¥ìë¥¼ ì œê±°í•œ clean filename (í´ë” ìƒì„± ì‹œ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´)
        WORKIMAGEDIR = os.path.join(WORKSPACE, 'images_'+WORKFILECLEAN)  # pdfì— ëŒ€í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ì´ë¯¸ì§€ í´ë” ê²½ë¡œ
        WORKFILEHASH = str(hash(WORKFILEORIGIN))[1:]+f"_{timestamp}"
        try:
            if os.path.exists(WORKIMAGEDIR):
                pass
                #ğŸ“¢ğŸ“¢ğŸ“¢ğŸ“¢ğŸ“¢ğŸ“¢ ë§ˆì§€ë§‰ì— ì£¼ì„ í’€ì–´ì•¼ í•¨.
                # for image in os.listdir(WORKIMAGEDIR): # ì´ë¯¸ì§€ í´ë”ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ í•´ë‹¹ ê²½ë¡œì˜ ì´ë¯¸ì§€ë“¤ì„ ëª¨ë‘ ì‚­ì œ
                #     image_path = os.path.join(WORKIMAGEDIR,image)
                #     if os.path.isfile(image_path):
                #         os.remove(image_path)
            else:
                os.mkdir(WORKIMAGEDIR) # ì´ë¯¸ì§€ í´ë” ìƒì„±

        except Exception as e:
            print("Error when making image dir",e)

        ###############################################################
        # object detection í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì „ì²´ í…ìŠ¤íŠ¸ì™€ ì „ì²´ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•œë‹¤.
        process_object_detection()
        # Multi-thread : ì•„ë˜ 2ê°€ì§€ í”„ë¡œì„¸ìŠ¤ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ì§„í–‰í•œë‹¤.
        try:
            # # image classificationì„ ì§„í–‰í•œ í›„ WORKFILEIMAGEDIRì— ì €ì¥í•œë‹¤.
            thread1 = threading.Thread(target=process_image_classification)
            # # text summarizationì„ ì§„í–‰í•œ í›„, markdown í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ WORKSPACEì— WORKFILECLEAN.md íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥í•œë‹¤.
            thread2 = threading.Thread(target=process_text_summarization)
            thread1.start()
            thread2.start()
            thread1.join()
            thread2.join()
        except Exception as e:
            print("Threading ì¤‘ ì—ëŸ¬:", e, ',',WORKFILECLEAN, WORKIMAGEDIR)

        return jsonify({'code': 200, 'msg': [WORKFILEORIGIN,WORKFILECLEAN]})
    except Exception as e:
        print('íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬:',e)
        # print(WORKFILEORIGIN, WORKIMAGEDIR)
        return jsonify({'code':400,'msg': 'íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨'})

@app.route('/loadfile.do',methods=['GET'])
def loadFile():
    md_filepath = os.path.join(WORKSPACE,WORKFILECLEAN+'.md')
    if os.path.exists(md_filepath):
        try:
            with open(md_filepath,'r',encoding='utf-8') as f:
                data = f.readlines()
            return jsonify({'code': 200, 'msg': data})
        except Exception as e:
            print(e)
            return jsonify({'code':400,'msg':"Server Error: Cannot read markdown file"})
    else:
        return jsonify({'code': 400, 'msg':"Server Error: File doesn't exists!"})


