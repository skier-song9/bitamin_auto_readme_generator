<subject>도배 하자 질의 응답 처리 한솔데코 시즌 2 Al 경진대회 알고리즘 언어 - LLM MLOps OA - Cosine Similarity</subject>
<team>구준회, 김지원, 박서진, 신진섭, 엄성원</team>
<index>프로젝트 배경, EDA, Text Augmentation, Modeling & Inference, 보완할 점 & 추후계획</index>

<main>프로젝트 배경</main>
<sub>ChatGPT</sub> <content>자연어를 처리하여 언어모델에 학습시키고 특정 TASK에 적합한 TEXT를 생성하는 원리를 이해하는 것이 목표임.</content> <page>1</page>
<main>EDA</main>
<sub>데이터 구조</sub> <content>주최 측의 Train Data는 id, 질문 2개, 질문의 카테고리, 그리고 답변 5개의 column으로 구성되어 있으며, 질문과 답변 Set으로 토큰화하여 입력함.</content> <page>3</page>
<main>Text Augmentation</main>
<sub>Crawling & Prompt Engineering</sub> <content>실용적인 Train 데이터를 위해 다양한 종류의 질문을 생성하고, 사람들이 자주 묻는 질문을 바탕으로 전문성 있는 질문 소스를 탐색함. ChatGPT를 이용해 질문에 대한 200글자 내의 전문적인 답변 5가지를 생성함.</content> <page>5</page>
<main>Modeling & Inference</main>
<sub>Fine Tuning</sub> <content>사전 학습된 모델을 소규모의 특정 데이터 세트에 대해 추가 학습시켜 특정 작업에서 성능을 향상시키는 과정임.</content> <page>7</page>
<sub>LORA PEFT</sub> <content>적은 매개변수 학습으로 새로운 문제를 효과적으로 해결하는 fine-tuning 기법으로, 대부분의 매개변수 가중치는 원래대로 유지하며 일부만 미세조정하여 훈련 비용과 리소스를 절약하면서 특정 작업의 성능을 높임.</content> <page>9</page>
<sub>Query 가중치에 대한 LoRA 적용</sub> <content>LoRA 방식을 통해 query 가중치에 대한 적용을 설명함.</content> <page>11</page>
<main>보완할 점 & 추후계획</main>
<sub>Issues</sub> <content>한정된 GPU 자원으로 인해 Kaggle Notebook과 Colab T4 RAM 16GB를 사용 중이며, 파라미터 수가 큰 모델은 한국어 데이터로 사전훈련된 모델이 아니더라도 높은 성능을 기대할 수 있음.</content> <page>13</page>