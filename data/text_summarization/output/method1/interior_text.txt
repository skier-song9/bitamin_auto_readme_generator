<subject>ChatGPT Language Model Project</subject>
<team>팀명 미제공</team>
<index>프로젝트 배경, EDA, Text Augmentation, Modeling & Inference, 보완할 점 & 추후계획</index>

<main>프로젝트 배경</main>
<sub>Project Background</sub> <content>목표는 자연어를 처리하여 언어모델에 학습시키고, 특정 TASK에 적합한 TEXT를 생성하는 원리를 이해하는 것입니다.</content> <page>3</page>

<main>EDA</main>
<sub>Overall Progress</sub> <content>NLP 도메인 이해, Transformer 연구 및 공모전 목표에 대한 언어모델 개발과 추론에 대한 진행 상황입니다.</content> <page>4</page>
<sub>Data Structure</sub> <content>Train Data는 질문 2개와 답변 5개로 구성되며, 질문-답변 Set으로 토큰화하여 입력합니다.</content> <page>5</page>

<main>Text Augmentation</main>
<sub>Text Augmentation Techniques</sub> <content>키워드 기반의 다양한 질문 생성 및 실용적 질문 소스를 탐색하여 Train 데이터를 증강하는 방법입니다.</content> <page>8</page>
<sub>Final Train Dataset</sub> <content>기존 6440 QA Set에서 증강 후 8319 QA Sets로 증가하였습니다.</content> <page>9</page>

<main>Modeling & Inference</main>
<sub>Modeling Techniques</sub> <content>Fine Tuning을 통해 사전 학습된 모델을 특정 데이터 세트에 맞춰 추가 학습하여 성능을 향상시키는 과정입니다.</content> <page>11</page>
<sub>PEFT and LoRA</sub> <content>PEFT는 적은 매개변수 학습으로 문제 해결을 위한 기법이며, LoRA는 일부 매개변수만 미세조정하여 훈련 비용을 절약합니다.</content> <page>12</page>
<sub>Model Performance</sub> <content>Base model은 동어 반복 생성만 가능하지만, Fine-tuned model은 주어진 질문에 대한 구체적인 답변을 생성할 수 있습니다.</content> <page>13</page>

<main>보완할 점 & 추후계획</main>
<sub>Issues Faced</sub> <content>한정된 GPU 자원과 한국어 데이터로 사전훈련된 모델의 필요성 등이 문제로 지적되었습니다.</content> <page>10</page>