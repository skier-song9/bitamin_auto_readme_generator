<subject>프로젝트 배경</subject>  
<team>팀명 미제공</team>  
<index>프로젝트 배경>  

<main>프로젝트 배경</main>  
<sub>Project Background</sub> <content>ChatGPT 언어모델을 활용하여 자연어처리의 원리를 이해하고, 특정 TASK에 적합한 TEXT 생성을 목표로 한다.</content> <page>3</page>  

<main>EDA</main>  
<sub>Overall Progress</sub> <content>NLP 도메인 이해와 Transformer 관련 논문 연구를 통해 언어모델 개발 및 추론 목표에 대한 전반적인 진행 상황을 정리한다.</content> <page>4</page>  
<sub>Train Data</sub> <content>Train Data는 질문 2개와 답변 5개로 구성되며, 질문 및 답변 Set으로 토큰화하여 input으로 사용한다.</content> <page>5</page>  

<main>Text Augmentation</main>  
<sub>Crawling & Prompt Engineering</sub> <content>실용적 Train 데이터를 위한 질문 소스를 탐색하고, ChatGPT를 통해 전문적인 답변을 생성한다.</content> <page>8</page>  
<sub>최종 train dataset</sub> <content>기존 6440 QA rOWS Sets에서 증강 후 8319 QA Sets로 증가하였다.</content> <page>9</page>  

<main>Modeling & Inference</main>  
<sub>Issues</sub> <content>Kaggle Notebook과 Colab T4의 제한된 GPU 자원 사용으로 어려움이 있으며, 한국어 데이터로 사전훈련된 모델의 성능 기대에 대한 논의가 있다.</content> <page>10</page>  
<sub>Fine Tuning</sub> <content>사전 학습된 모델을 특정 데이터 세트에 맞춰 추가 학습하여 성능을 향상시키는 Fine Tuning 과정을 설명한다.</content> <page>11</page>  
<sub>LORA PEFT</sub> <content>적은 매개변수 학습으로 효율적인 문제 해결을 위한 fine-tuning 기법인 LoRA PEFT를 활용하여 훈련 비용과 리소스를 절약하면서 성능을 향상시킨다.</content> <page>12</page>  
<sub>Inference</sub> <content>Base model과 Fine-tuned model의 성능 차이를 비교하며, 주어진 질문에 대한 구체적인 답변 생성을 위한 instruction 부여 방법을 설명한다.</content> <page>13</page>  

<main>보완할 점 & 추후계획</main>  
<sub>Gradio</sub> <content>EOD</content> <page>15</page>