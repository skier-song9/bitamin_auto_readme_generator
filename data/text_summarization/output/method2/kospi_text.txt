<subject>비타민 12 & 13기 학기 프로젝트 피쳐 중요도 통한 분석을 지수 예측 KOSP2O</subject>
<team>비타민 12기, 비타민 13기</team>
<index>LEVEL 2 모델 소개 XGB LSTM GRU</index>
<index>LEVEL 3 피쳐 중요도 분석 Attention SHAP</index>
<index>LEVEL 4 결과 비교 및 결론</index>

<main>데이터 전처리</main>
<sub>200을 기준으로 데이터프레임 병합 KOSPI</sub> <content>ex 다우존스의 5월 15일 종가는 우리나라 시간으로 5월 16일 날짜를 하루 앞당겨서 사용 날짜 하루씩 조정 Dow [ ['날짜']= to datet Me[ I dow ['날짜']} Od dow'[ edit_날파']- dow['날짜'] + mede Ita OdTi days- - dow Head 날짜를 하루 앞당긴 변수 다우존스 원/달러 환율 WTI 원 [ 유가격 브렌트유 가격 국제 |금 가격 Vix 지수 - df fillnamethod='ffill' inplace=True로 결측치 처리 -~</content> <page>5</page>

<main>트리 기반의 앙상불 학습 알고리즘</main>
<sub>XGBoost</sub> <content>기본 학습기를 의사결정나무 Decision Tree로 하여 그래디언트 부스팅과 같이 그래디언트를 이용해 이전 모형의 약점을 보완하는 방식으로 학습하는 모델 XGBoost 본 프로젝트에서는 이진분류 모델을 구현했으므로 손실함수로 'binary: logistic 사용함 병렬처리가 가능해 빠른 속도로 뛰어난 예측 성능 보임</content> <page>7</page>

<main>슬라이딩 윈도우 방법론</main>
<content>데이터의 시간적 순서를 유지하며 동시에 과거 데이터를 기반으로 미래를 예측하는 효과적 방법! 누적된 과거 데이터를 윈도우로 정의 윈도우를 일정 간격으로 이동시키면서 다음 시점의 값을 예측 > 그림처럼 1명의 고객으로 7개의 분석용 데이터를 얻을 수 있음 A라는 동일한 사람이 A1 A2 A3으로 시간 차를 두고 복제됨으로써 기존에 1000개의 정보가 있었다면 슬라이딩 윈도우 기법을 통해 고객 정보를 확보! 7000건의 시간 종속성을 유지하며 데이터의 패턴 반영해 모델이 더 많은 정보를 기반으로 학습 예측 가능 윈도우 크기만큼 데이터의 피쳐가 증가함</content> <page>8</page>

<main>XGBoost 구축 종가 데이터</main>
<sub>하이퍼 파라미터 튜닝 결과</sub> <content>learning rate=0.05 depth=5 n_estimators=200 window Size=100 max 0.5518의 보임 accuracy를</content> <page>9</page>

<main>XGBoost 구축 변화율 데이터</main>
<sub>하이퍼 튜닝 결과 파라미터</sub> <content>learning rate=0.05 depth=5 n_estimators=100 max 0.6098의 보임 accuracy를 변화율 데이터를 이용한 모델이 0.05 정도의 성능 개선을 보임</content> <page>10</page>

<main>긴 시간동안 RNN의 메모리를 유지</main>
<sub>LSTM</sub> <content>LSTM은 Cell state와 Gate 메커니즘을 통해 긴 시퀀스 내의 중요한 정보를 오래 유지하고 불필요한 정보는 잊어버려 장기 의존성 문제를 효과적으로 해결함 Input Output Forget Gate Input Output Forget Gate를 사용하여 시계열 데이터의 중요한 패턴과 추세를 학습하고 보존함 시계열 데이터에 효과적인 딥러닝 프레임워크</content> <page>11</page>

<main>LSTM 구축</main>
<content>두 개의 LSTM 레이어를 사용 각 레이어 후에 드롭아웃을 추가하여 과적합을 방지 콜백을 사용하여 가장 좋은 가중치를 복원 EarlyStopping</content> <page>12</page>

<main>GRU</main>
<content>LSTM과 유사하게 게이트 메커니즘을 통해 중요한 정보를 유지하고 불필요한 정보를 제거함 GRU는 셀 상태 없이 두 개의 Gate만 사용하여 시퀀스 데이터의 장기 의존성 문제를 효과적으로 해결 Reset Gate와 Update Gate Reset Gate와 Update Gate를 사용하여 시계열 데이터의 중요한 패턴과 추세를 학습하고 보존 시계열 데이터에 효과적인 딥러닝 프레임워크</content> <page>13</page>

<main>GRU 구축</main>
<content>두 개의 GRU 레이어를 사용 각 레이어 후에 드롭아웃을 추가하여 과적합을 방지 콜백을 사용하여 가장 좋은 가중치를 복원 EarlyStopping</content> <page>14</page>

<main>Attention</main>
<sub>중요한 부분에 더 집중Attention 하자!</sub> <content>Attention 변수 중요도 가중치 보통 자연어 처리와 컴퓨터 비전 분야에서 사용되는 기술 가중치는 각 단어의 중요도기여도를 의미한다 시계열 데이터에 적용하면 특정 시간 단계에서 중요한 정보에 집중 각 쿼리 벡터와 모든 키 벡터 사이의 유사도를 계산 내적 등 여러 개의 입력 벡터를 받고 그 벡터를 세 종류 벡터로 변환 Attention 가중치 계산된 유사도 점수를 Softmax 함수에 통과시켜 가중치 생성 쿼리Query 주어진 문장에서 현재 주목하는 단어를 나타내는 벡터 키Key 각 단어의 특성을 나타내는 벡터 밸류Value 각 단어의 실제 정보를 나타내는 벡터 가중합Weighted Sum 각 밸류 벡터에 해당 가중치를 곱한 후 이를 모두 더해 최종 출력 벡터를 만듬 각 단어의 가중치를 계산하고 반영해 어떤 것에 집중하자 더 높은 가중치를 부여하는 것은 그만큼 더 주의Attention를 기울인다는 것</content> <page>16</page>

<main>SHAP</main>
<sub>게임이론에 기반한 머신러닝 모델의 출력을 설명하기 위한 접근법</sub> <content>피쳐 중요도 시각화 해석 상위 20개의 피쳐가 표시되었다 왼: 종가 오: 변화율 피처의 중요도: Y축의 순서대로 피처가 모델 예측에 미치는 중요도가 높다 SHAP 값의 크기와 방향: X축에서 SHAP 값의 절대값이 클수록 해당 피처가 모델 예측에 더 큰 영향을 미친다 SHAP 값이 양수이면 해당 피처 값이 클수록 예측 확률이 증가함을 의미하고 음수이면 예측 확률이 감소함을 의미 피쳐 값의 효과: 빨간 색은 피쳐값이 높음 파란색은 피쳐값이 낮음을 의미 빨간 점이 오른쪽에 많이 분포한다면 해당 피쳐 값이 클수록 예측 확률이 증가함을 의미한다 -> SHAP를 통해서 중요도 상위 피쳐를 찾아낼 수 있었다!</content> <page>19</page>
<sub>피쳐 간 상호작용 확인</sub> <content>피쳐 간 상호작용의 정도는 미미하였다! 왼: 종가 오: 변화율</content> <page>21</page>
<sub>SHAP 결과 상위 피쳐만 사용하여 모델링 종가</sub> <content>SHAP 변수 중요도 분석 결과 상위 피쳐만 사용하여 모델링 결과 1300개의 피쳐 window Size=100 중 상위 몇 백개만 사용하여 모델링 1 원래 모델 성능 0.5518 2 300개의 피쳐만 사용한 결과 성능 0.52 3 500개의 피쳐만 사용한 결과 성능: 0.5396 변수 중요도 분석을 통해 중요 변수를 뽑아낸 결과 -> SHAP 훨씬 적은 피쳐만으로도 성능을 비슷하게 유지할 수 있었다! *상위 20개의 피쳐에 가중치를 부여하여 모델링도 해보았으나 모델의 성능에 차이가 없음도 발견할 수 있었다</content> <page>22</page>
<sub>SHAP 결과 상위 피쳐만 사용하여 모델링 변화율</sub> <content>SHAP 변수 중요도 분석 결과 상위 피쳐만 사용하여 모델링 결과 130개의 피쳐 window Size=10 중 상위 몇 개만 사용하여 모델링 1 원래 모델 성능 0.6098 2 5개의 피쳐만 사용한 결과 성능 0.5925 3 10개의 피쳐만 사용한 결과 성능 0.5954 SHAP 변수 -> 중요도 분석을 통해 중요 변수를 뽑아낸 결과 훨씬 적은 피쳐만으로도 성능을 비슷하게 유지할 수 있었다!</content> <page>23</page>

<main>결과 비교</main>
<sub>종가와 변화율 사용에 따른 모델 성능 비교</sub> <content>XGBoost가 가장 우수한 성능 보여줌 변화율 데이터에서 성능 향상을 보여줌 변화율 데이터를 사용하여 성능이 향상된 것은 변화율 데이터가 종가 데이터보다 더 유의미한 패턴과 트렌드를 제공함으로써 모델의 예측 정확도를 높이고 노이즈를 줄이며 일반화 능력을 향상시키는 데 효과적임을 확인함</content> <page>26</page>
<sub>변수중요도 적용유무에 따른 모델 성능 비교</sub> <content>XGBoost가 가장 우수한 성능 보여줌 LSTM이나 GRU에 비해 XGBoost가 더 높은 성능을 보였다 attention을 이용한 Feature 적용한 결과: Importance LSTM GRU 성능 증가/ XGB 성능 약간 감소 LSTM GRU 의 경우 변수중요도를 적용하지 않은 데이터세트보다 변수 중요도를 적용한 데이터 세트에서 약간의 성능 향상을 보임 XGBoost의 경우 어텐션 결과를 적용한 결과 약간의 성능 하강을 보임 이는 LSTM과 GRU가 RNN계열의 모델로 어텐션 매커니즘을 적용함으로써 시퀀스 데이터 내의 중요 정보를 잘 파악할 수 있게 하는 반면 XGB는 트리기반의 모델로 피쳐 간 순차적 관계를 고려하는 어텐션 매커니즘 이 오히려 노이즈를 늘릴 수 있기 때문이라고 추정할 수 있다 트리기반 모델은 순차적 피쳐 관계 고려 X</content> <page>27</page>
<sub>변수 중요도 적용 하는 방법에 따른 모델 성능 비교</sub> <content>XGBoost와 LSTM이 각각 우수한 성능 보여줌 변수 중요도 계수를 적용한 데이터세트에서는 XGBoost가 변수 중요도가 양수인 계수만 적용한 데이터세트에서는 LSTM이 우수한 성능을 보여줌 변수 중요도 계수 적용한 경우 성능이 가장 우수 각 변수의 중요도를 곱한 데이터에서 모델의 성능이 가장 우수함 변수 중요도 양수값만 적용한 경우 약 0.01의 성능이 감소함 음수가 나온 칼럼들이 원래 데이터의 절반정도임을 고려하였을 때 의미있는 결과로 볼 수 있음</content> <page>28</page>

<main>결론</main>
<sub>종가 VS 변화율 데이터</sub> <content>Attention 매커니즘 활용 변수 중요도 분석 변화율 데이터가 데이터의 잡음을 줄이고 추세성을 반영함으로써 종가 데이터보다 정확한 성능을 보인다 -> 주가 예측에서는 변화율 데이터를 사용하는 것이 좋다 매커니즘을 활용해 변수 중요도 가중치를 얻을 수 있다 LSTM GRU의 예측 성능이 증가했다 Attention 변수 중요도 정보로 시계열 분석에서의 RNN 계열 모델 정확도를 올릴 수 있다 변수 중요도 분석 의의 변수 분석을 통해 모델의 중요도 성능의 개선을 꾀할 수 있다 변수 중요도 분석을 통해 변수 선택 Feature Selection을 적용하여 적은 피쳐만으로도 모델의 성능을 유지할 수 있다 주가 예측에서 데이터의 피쳐가 많을 경우의 처리가 용이하다</content> <page>29</page>