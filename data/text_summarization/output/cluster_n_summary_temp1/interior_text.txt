<subject>도배 하자 질의 응답 처리 및 LLM 개발에 관한 프로젝트</subject>
<team>구준회, 김지원, 박서진, 신진섭, 엄성원</team>
<index>프로젝트 배경, EDA, Text Augmentation, Modeling & Inference, 보완할 점 & 추후계획</index>
<main>프로젝트 배경</main>
<sub>[목표]</sub>
<content>자연어를 처리하여 언어모델에 학습시키고, 특정 작업에 적합한 텍스트를 생성하는 원리를 이해하는 것이 목표이다.</content>
<main> EDA</main>
<sub>Overall Progress</sub>
<content>EDA의 전반적인 진행 상태를 다루며, 단계별로 NLP 도메인에 대한 이해와 Transformer 모델의 개념을 습득하고, 질의응답 관련 논문을 학습하며, 학습 목표와 언어 모델 개발에 대해 설명한다.</content>
<sub>Step Step N</sub>
<content>각 단계에서 NLP 도메인에 대한 접근과 Transformer 모델의 적용을 통해 자신이 진행해야 할 과제와 목표를 명확히 하고, 언어 모델의 개발 및 추론 방법을 탐색한다.</content>
<sub>Step 3 이해</sub>
<content>EDA의 세 번째 단계에서는 언어 모델의 기능과 질의 응답 영역에서의 활용 가능성을 집중적으로 분석하고 개발 방향을 설정한다.</content>
<main> Text Augmentation</main>
<sub>Text Augmentation 개요</sub>
<content>텍스트 증강은 질문과 답변 데이터를 활용하여 더욱 다양하고 유용한 트레이닝 데이터를 생성하는 과정이다. 이를 통해 모델의 성능 향상이 기대된다.</content>

<sub>토큰화 및 입력 처리</sub>
<content>질문과 답변 세트를 토큰화하여 입력 데이터로 변환하는 방법을 설명하고 있다. 이는 모델이 텍스트를 효과적으로 이해하고 처리할 수 있게 한다.</content>

<sub>질문 생성 방법</sub>
<content>전문성을 갖춘 질문 소스를 탐색하고 자주 묻는 질문을 바탕으로 다양한 질문을 생성하는 방법을 제시한다. 이는 실용적인 키워드 탐색을 강조한다.</content>

<sub>ChatGPT 활용</sub>
<content>ChatGPT를 이용하여 특정 질문에 대한 전문적인 답변을 생성하는 예시를 제공한다. 예를 들어, 도배업계의 전문가에게 일반인을 위한 200글자 내의 답변을 요청하는 방식이다.</content>
<main> Modeling & Inference</main>
<sub>Modeling Fine Tuning</sub>
<content>Fine Tuning은 사전 학습된 모델을 소규모의 특정 데이터 세트에 대해 추가로 학습시켜 특정 작업이나 도메인에서 기능을 개선하고 성능을 향상시키는 과정이다.</content>

<sub>Modeling LORA PEFT</sub>
<content>LoRA PEFT는 적은 매개변수 학습으로 빠르게 새로운 문제를 해결하는 기법으로, 대부분의 매개변수를 유지하면서 일부만 미세 조정해 훈련 비용과 컴퓨팅 리소스를 절약하면서 성능을 향상시킨다.</content>

<sub>Inference</sub>
<content>Fine-tuned 모델은 주어진 질문에 대한 구체적인 답변을 생성할 수 있도록 instruction을 부여받아 학습된 정보를 바탕으로 자세한 답변을 생성한다.</content>
<main> 보완할 점 & 추후계획</main>
<sub>한정된 GPU 자원</sub>
<content>현재 사용 중인 Kaggle Notebook의 Colab T4는 RAM이 16GB로 제한되어 있어 GPU 자원이 부족한 문제를 해결해야 한다.</content>

<sub>모델 성능 향상</sub>
<content>파라미터 수가 큰 모델을 활용하여 한국어 데이터로 사전훈련된 모델이 아니더라도 성능을 개선할 수 있는 방안을 모색할 계획이다.</content>