<subject>도배 하자 질의 응답 처리 및 LLM 개발에 관한 프로젝트</subject>
<team>구준회, 김지원, 박서진, 신진섭, 엄성원</team>
<index>프로젝트 배경, EDA, Text Augmentation, Modeling & Inference, 보완할 점 & 추후계획</index>
<main>프로젝트 배경</main>
<subtitle>[목표]</subtitle>
<content>자연어를 처리하고 언어모델에 학습시켜, 특정 작업에 적합한 텍스트를 생성하는 방법론을 이해하는 것이 프로젝트의 주요 목표이다.</content>
<main> EDA</main>
<subtitle>[Overall Progress]</subtitle>
<content>전반적인 진행 상황으로, NLP 도메인에 대한 이해와 함께 Transformer 모델 및 질의 응답에 관한 논문을 학습하고 있으며, 공모전 목표를 이해하고 언어 모델 개발 및 추론을 진행 중입니다.</content>
<main> Text Augmentation</main>
<subtitle>Text Augmentation 방법</subtitle>
<content>Text Augmentation은 다양한 질문을 생성하고, 사람들의 자주 묻는 질문을 탐색하는 과정이다. 이를 통해 전문적인 답변을 만들기 위해 ChatGPT를 이용하여 주어진 질문에 대한 정확하고 신뢰성 있는 응답을 생성할 수 있다.</content>

<subtitle>기술적 접근</subtitle>
<content>트레인 데이터는 질문과 답변 쌍의 세트로 구성되며, 이를 토큰화하여 입력값으로 사용한다. 웹사이트와 응용 프로그램을 통해 키워드를 기반으로 자료를 탐색하고, 문서 및 레퍼런스를 활용하여 실용적인 질문을 도출한다.</content>
<main> Modeling & Inference</main>
<subtitle>Modeling Fine Tuning</subtitle>
<content>모델링 파인 튜닝은 사전 학습된 모델을 소규모 특정 데이터로 추가 학습하여 특정 작업이나 도메인에서 기능을 개선하고 성능을 향상시키는 과정이다.</content>

<subtitle>Modeling LORA PEFT</subtitle>
<content>LoRA PEFT는 적은 매개변수만 조정하여도 새로운 문제를 효과적으로 해결할 수 있도록 하는 파인 튜닝 기법으로, 대부분의 매개변수 가중치는 원래 유지하면서 일부만 미세 조정하여 훈련 비용과 컴퓨팅 리소스를 절약한다.</content>

<subtitle>Inference</subtitle>
<content>모델은 학습된 정보를 바탕으로 구체적인 답변을 생성하며, 입력에 지침을 부여함으로써 주어진 질문에 대한 답변을 제공한다. 파인 튜닝된 모델은 무의미한 동어 반복 대신 질문에 대한 해당 답변을 생성하도록 설계되었다.</content>
<main> 보완할 점 & 추후계획</main>
<subtitle>[한정된 GPU 자원 문제]</subtitle>
<content>현재 Kaggle Notebook과 Colab T4의 16GB RAM을 사용할 때 GPU 자원의 한정이 문제점으로 지적된다.</content>

<subtitle>[파라미터 수가 큰 모델의 성능]</subtitle>
<content>한국어 데이터로 사전훈련되지 않은 모델이라도 파라미터 수가 큰 경우 높은 성능을 기대할 수 있어, 이를 고려한 모델 개발이 필요하다.</content>