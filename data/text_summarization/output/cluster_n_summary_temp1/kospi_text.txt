<subject>비타민 12 & 13기 학기 프로젝트의 피쳐 중요도를 통한 KOSPI 지수 예측 분석</subject>
<team>None</team>
<index>비타민 학기 프로젝트 최종 발표 목차, LEVEL 2 모델 소개 XGB LSTM GRU, EVEL 3 피쳐 중요도 분석 Attention SHAP, LEVEL 4 결과 비교 및 결론</index>
<main>비타민 학기 프로젝트 최종 발표 목차</main>
<sub>피쳐 중요도 분석을 통한 KOSPI 지수 예측</sub>
<content>KOSPI 지수 예측을 위해 피쳐 중요도 분석을 실시하였으며, XGBoost 및 LSTM, GRU 모델을 활용하여 성능을 비교하였다.</content>

<sub>데이터 고개</sub>
<content>모델 구축을 위해 종가 데이터와 변화율 데이터를 수집하였고, 각 모델에 대한 하이퍼 파라미터 튜닝 결과를 제시하였다.</content>

<sub>모델 고개</sub>
<content>XGB, LSTM, GRU 모델을 사용하여 KOSPI 지수를 예측하고, 하이퍼 파라미터의 변화가 모델 성능에 미치는 영향을 분석하였다.</content>

<sub>하이퍼 파라미터 튜닝 결과</sub>
<content>하이퍼 파라미터 튜닝 결과, XGBoost 모델의 경우 특정 파라미터 설정에서 각각 0.5518과 0.6098의 정확도를 달성하였다.</content>

<sub>변화율 데이터를 이용한 모델 성능 개선</sub>
<content>변화율 데이터를 사용하는 모델이 약 0.005 정도의 성능 개선을 보임을 확인하였다.</content>

<sub>피쳐 간 상호작용 확인</sub>
<content>SHAP 분석을 통해 피쳐 간의 상호작용 정도를 검토하였으며, 상호작용이 미미하다는 결과를 도출하였다.</content>

<sub>결론</sub>
<content>피쳐 중요도 분석을 기반으로 한 KOSPI 지수 예측 모델의 성능을 비교하며, Attention 메커니즘을 활용한 피쳐 중요도 평가 결과를 제시하였다.</content>
<main> LEVEL 2 모델 소개 XGB LSTM GRU</main>
<sub>[LSTM 모델 구축]</sub>
<content>LSTM 모델은 두 개의 레이어로 구성되며, 각 레이어 뒤에 드롭아웃을 추가하여 과적합을 방지하고 EarlyStopping을 통해 최적의 가중치를 복원하는 콜백 기능을 사용한다.</content>

<sub>[GRU 모델 구축]</sub>
<content>GRU 모델 역시 두 개의 레이어로 구성되며, 각 레이어 뒤에 드롭아웃을 추가하여 과적합을 방지하고 EarlyStopping을 통해 최적의 가중치를 복원하는 콜백 기능이 적용된다.</content>

<sub>[모델 성능 비교]</sub>
<content>종가와 변화율을 사용한 모델 성능 비교에서 XGBoost가 가장 우수한 성능을 보였으며, 변화율 데이터를 사용함으로써 모델의 예측 정확도가 향상되고 노이즈가 줄어드는 효과를 확인하였다.</content>
<main> EVEL 3 피쳐 중요도 분석 Attention SHAP</main>
<sub>[데이터 전처리]</sub>
<content>기본적인 데이터 전처리 과정을 통해 결측치를 처리하고, 슬라이딩 윈도우 방법론을 적용하여 시간적 순서를 유지하며 고객 데이터를 확보하는 기법에 대해 설명하였다.</content>

<sub>[Attention 모델 구조]</sub>
<content>Attention 모델을 정의하는 과정에서 입력 형태와 다양한 매개변수를 설정하며 Transformer 블록을 포함한 구조를 구성하였다.</content>

<sub>[SHAP 변수 중요도 분석]</sub>
<content>SHAP를 통해 변수의 중요도를 분석하고, 상위 피쳐만을 사용하여 모델링한 결과, 원래 모델 성능과 유사함에도 피쳐 수를 대폭 줄일 수 있었다는 점을 강조하였다.</content>

<sub>[결과 비교]</sub>
<content>XGBoost와 LSTM 모델 성능의 비교를 통해, 변수 중요도 계수를 적용한 데이터셋에서 XGBoost가 우수한 성능을 보였으며, 성능 감소의 원인을 음수 계수의 존재로 분석하였다.</content>
<main> LEVEL 4 결과 비교 및 결론</main>
<sub>[결과 비교]</sub>
<content>XGBoost가 가장 우수한 성능을 보였으며, LSTM과 GRU에 비해 높은 성능 차이를 나타냈다. Attention을 적용한 LSTM과 GRU의 성능은 약간 향상되었으나, XGBoost의 경우 성능이 다소 감소하였다. 이는 LSTM과 GRU가 중요한 시계열 정보를 잘 포착하는 반면, XGBoost는 어텐션 메커니즘이 오히려 노이즈를 증가시킬 수 있음을 시사한다.</content>

<sub>[결론]</sub>
<content>변화율 데이터를 활용한 예측이 종가 데이터보다 더 정확한 성능을 제공하며, Attention 매커니즘을 통해 변수 중요도를 분석함으로써 LSTM과 GRU의 예측 성능이 향상되었다. 변수 중요도 분석은 모델 성능 개선에 기여하며, Feature Selection을 통해 적은 피쳐로도 성능을 유지할 수 있는 가능성을 보여준다.</content>